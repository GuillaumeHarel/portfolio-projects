{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center> Web scraping multiple job platforms with Selenium & BeautifulSoup and aggregating the job offers in an interactive Bokeh dashboard </center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='Images/Capture_bokeh_2.png' width=\"1000\" height=\"1000\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><u>Context and objectives</u></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is focused on a personal project which aims at centralizing in a single app my job searches on several online job boards. To date, in the framework of this project, \"only\" the APEC and Indeed job websites have been scraped and the job search queries have been restricted to data-related positions in France. \n",
    "<br>\n",
    "\n",
    "This project can be divided into three tasks with distinct purposes. \n",
    "<br>\n",
    "The first chapter of the notebook tackles in details the web scraping exercise with the Python libraries Selenium and BeautifulSoup while the second one is specifically dedicated to the construction of the personal job dashboard using the Bokeh library. Finally, the last task will consist in an exploratory data analysis of all the information collected in order to have an overall picture of the data-related job market in France and enlighten the most valuable and sought-after soft and hard skills.\n",
    "<br>\n",
    "<br>\n",
    "<u>*Key skills:*</u>\n",
    "<br>\n",
    "**Unstructured data**, **Web scraping**, **Text mining**, **Dataviz**, **Text similarity metrics**\n",
    "<br>\n",
    "<u>*Key libraries:*</u>\n",
    "<br>\n",
    "**Selenium**, **BeautifulSoup**, **Bokeh**, **Pandas**, **Re**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<h2><center><u>Table of contents</u></center></h2>\n",
    "\n",
    "[<h3>I) Web scraping the job boards</h3>](#1)\n",
    "[<h4>1) APEC job board </h4>](#1.1)\n",
    "[<h5>&emsp;a) Step by step approach </h5>](#1.1.a) \n",
    "[<h5>&emsp;b) All in one functions </h5>](#1.1.b)  \n",
    "[<h4>2) Indeed job board </h4>](#1.2)\n",
    "[<h5>&emsp;a) Step by step approach </h5>](#1.2.a)\n",
    "[<h5>&emsp;b) All in one functions </h5>](#1.2.b)\n",
    "<br>\n",
    "[<h3>II) Using Bokeh to build a custom aggregated job board</h3>](#2)\n",
    "[<h4>1) Preprocessing the raw data </h4>](#2.1)\n",
    "[<h5>&emsp;a) Data cleaning </h5>](#2.1.a)\n",
    "[<h5>&emsp;b) Feature engineering </h5>](#2.1.b)\n",
    "[<h5>&emsp;c) All in one function </h5>](#2.1.c)\n",
    "<br>\n",
    "[<h4>2) Custom aggregated job board using Bokeh </h4>](#2.2)\n",
    "[<h5>&emsp;a) Bokeh App architecture </h5>](#2.2.a)\n",
    "[<h5>&emsp;a) Bokeh App </h5>](#2.2.b)\n",
    "<br>\n",
    "[<h3>III) Exploratory data analysis (on going...)</h3>](#3)\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>I) Web scraping the job boards</h2>\n",
    "<a id=\"1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For dynamic web scraping I used the powerful library combination of Selenium and BeautifoulSoup. BeautifoulSoup would be self-sufficient for static scraping but does not handle Javascript components and could not deal with the few necessary event-action for the targeted website (e.g. closing a popup, navigating to next page, scroll down the page to display its full content ...)\n",
    "<br>\n",
    "Selenium allows web browser interactions/navigation directly from Python via a chosen web driver and can also fetch the HTML page source for the current DOM of a website. Afterwards, BeautifulSoup can perform the parsing job of the document using a custom parser (here \"lxml\").\n",
    "<br>\n",
    "<br>\n",
    "For furter information on how to install and use these libraries, please read the available documentations [[1]](#ref1), [[2]](#ref2).\n",
    "Other usefull contents browsed during my journey through web scraping : [[3]](#ref3), [[4]](#ref4), [[5]](#ref4), [[6]](#ref6), [[7]](#ref7).\n",
    "<br>\n",
    "<br>\n",
    "<u>*Prior to scrape a website, its general terms of use should be checked.*</u>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1) APEC job board</h3>\n",
    "<a id=\"1.1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>a) Step by step approach</h4>\n",
    "<a id=\"1.1.a\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the APEC job board, the process of retrieving the target information from unstructured data will be presented step by step. Feel free to directly move to the next section if you are already familiar with Selenium and BeautifulSoup. \n",
    "<br>\n",
    "Please not that APEC is a French job board and that some information will be displayed in french.\n",
    "<br>\n",
    "<br>\n",
    "First, let's import all the necessary libaries for the Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "import glob\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import string\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from six.moves import urllib\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy.sparse import vstack\n",
    "import seaborn as sns\n",
    "\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "\n",
    "import bokeh\n",
    "\n",
    "from IPython.display import display, Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version:  3.7.3 (default, Apr 24 2019, 15:29:51) [MSC v.1915 64 bit (AMD64)]\n",
      "Numpy version:  1.16.4\n",
      "Scipy version:  1.3.0\n",
      "Pandas version:  0.24.2\n",
      "Scikit-learn version:  0.21.2\n",
      "Seaborn version:  0.9.0\n",
      "Selenium version:  3.141.0\n",
      "BeautifulSoup version:  4.7.1\n",
      "Bokeh version:  1.3.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Python version: \", sys.version)\n",
    "print(\"Numpy version: \", np.__version__)\n",
    "print(\"Scipy version: \", scipy.__version__)\n",
    "print(\"Pandas version: \", pd.__version__)\n",
    "print(\"Scikit-learn version: \", sklearn.__version__)\n",
    "print(\"Seaborn version: \", sns.__version__)\n",
    "print(\"Selenium version: \", selenium.__version__)\n",
    "print(\"BeautifulSoup version: \", bs4.__version__)\n",
    "print(\"Bokeh version: \", bokeh.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define and initiate our Selenium web driver. Headless option is removed here in order to see what we are actually doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--ignore-certificate-errors')\n",
    "options.add_argument('--incognito')\n",
    "#options.add_argument('--headless')\n",
    "driver = webdriver.Chrome(options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_url = \"https://cadres.apec.fr/home/mes-offres-d-emploi.html\"\n",
    "driver.get(some_url)\n",
    "time.sleep(2)\n",
    "driver.find_element_by_xpath(\"//button[@title='Accepter']\").click() #cookie popup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='Images/Capture_apec_query.png' width=\"600\" height=\"600\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selenium allows you to find and access elements of a web page according to multiple ways (report to documentation). Here, all we need to do is access to two elements : What position we are looking for and Where. \n",
    "<br>\n",
    "To easily find the proper information to access the different elements : Right click > Inspect element (manually)\n",
    "<br> \n",
    "As mentionned earlier, Selenium replicate all the intereactions a \"real end-user\" could have with its web brower. Here we clear the two blank fields and enter our proposition. Finally we press Enter Key while being in the location field to initiate our query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "kw_elem = driver.find_element_by_id(\"keywords\")\n",
    "kw_elem.clear()\n",
    "kw_elem.send_keys('data scientist OU data analyst')\n",
    "loc_elem = driver.find_element_by_xpath(\"//input[@placeholder='Ex : Paris, Lyon ...']\")\n",
    "loc_elem.clear()\n",
    "loc_elem.send_keys('Auvergne-Rh√¥ne-Alpes', Keys.RETURN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='Images/Capture_apec_results_51.png' width=\"600\" height=\"600\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next step is to collect all the job offer links returned by our query for every page of results. \n",
    "<br>\n",
    "Just one subtility : to display the \"Next\" Button to navigate between the pages, we first need to scroll down to the page bottom. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Navigating to next page (2)\n",
      "Navigating to next page (3)\n",
      "\n",
      " All the pages (3) have been read through\n"
     ]
    }
   ],
   "source": [
    "page_nb = 1\n",
    "job_links = []\n",
    "while True:\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    job_titles = driver.find_elements_by_class_name('offre-title')\n",
    "    for i in range(len(job_titles)):\n",
    "        job_link = job_titles[i].find_element_by_css_selector('a').get_attribute('href')\n",
    "        job_links.append(job_link)\n",
    "    try:\n",
    "        next_page = driver.find_element_by_link_text('Suiv.').click()\n",
    "        page_nb += 1\n",
    "        print(\"Navigating to next page ({})\".format(page_nb))\n",
    "    except:\n",
    "        print(\"\\n All the pages ({}) have been read through\".format(page_nb))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(job_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything seems fine. We are almost done with Selenium. \n",
    "<br> \n",
    "The last action consists in fetching the source code of every webpage stored in our job_links list. Let's move on to the parsing exercise with BeautifulSoup. As adviced by the documentation, I installed and used the lxml parser for speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_offer = job_links[0]\n",
    "driver.get(some_offer)\n",
    "page_source = driver.page_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(page_source, 'lxml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BeautifoulSoup provides a lot of means and alternatives to navigate through the DOM and retrieve the desired elements. Please, read the documentation for further information on how navigating/searching the tree and manipulating the different kind of objects. Like for Selenium, left-click > Inspect element, will be our best option to identify the target information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='Images/Capture_apec_offer.png' width=\"700\" height=\"700\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'164421518W'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref = soup.find(string=re.compile('Ref\\. Apec'))\n",
    "re.findall(\"\\d+\\w*\", ref)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Data Scientist F/H'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_title = soup.find('h1', class_=\"text-uppercase ng-binding\").text\n",
    "job_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('DataGenius', 'Lyon 03 - 69')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comp_and_loc = soup.find_all('strong', class_=\"ng-binding\")\n",
    "compagny, location = comp_and_loc[1].text, comp_and_loc[2].text\n",
    "compagny, location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('CDI', '2')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_type = soup.find('span', class_=\"ng-binding ng-scope\").text\n",
    "nb_pos = next(soup.find('span', class_=\"ng-binding ng-scope\").parent.stripped_strings)\n",
    "pos_type, nb_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('18/07/2019', '18/07/2019')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pub_date = re.findall(\"(?:/*\\\\d+)+\", soup.find(string=re.compile('Publi√©e le')))[0]\n",
    "act_date = re.findall(\"(?:/*\\\\d+)+\", soup.find(string=re.compile('Actualis√©e le')))[0]\n",
    "pub_date, act_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BeautifulSoup also accepts function for tag. Here we want to retrieve all \"strong\" tag objects with no \"class\" attribute. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_strong_but_no_class(tag):\n",
    "    return (tag.name == \"strong\") and not(tag.has_attr('class'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<strong>Salaire :</strong>,\n",
       " <strong>Prise de poste :</strong>,\n",
       " <strong>Exp√©rience dans le poste :</strong>,\n",
       " <strong>Statut du poste :</strong>,\n",
       " <strong>Zone de d√©placement :</strong>,\n",
       " <strong>Secteur d‚Äôactivit√© du poste :</strong>,\n",
       " <strong>Client :</strong>,\n",
       " <strong>R&amp;D :</strong>,\n",
       " <strong>DataGenius</strong>,\n",
       " <strong>Intelligence Artificielle</strong>,\n",
       " <strong>Machine Learning et Big Data</strong>,\n",
       " <strong>Data Science et Intelligence Artificielle sont¬†au c≈ìur¬†d'une rupture soci√©tale et √©conomique majeure</strong>,\n",
       " <strong>start-up dynamique et bienveillante</strong>,\n",
       " <strong>porteuse de projets stimulants et innovants</strong>,\n",
       " <strong>Personne en charge du recrutement :</strong>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_info = soup.find_all(tag_strong_but_no_class)\n",
    "full_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' A partir de 36 k‚Ç¨ brut annuel'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_info[0].next_sibling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Tous niveaux d'exp√©rience accept√©s\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_info_l = ['salary', 'empl_date', 'req_exp', 'pos_stt', 'btrip_area', 'act_area']\n",
    "relevant_info_d = {}\n",
    "for idx, info in enumerate(relevant_info_l):\n",
    "    relevant_info_d[info] = full_info[idx].next_sibling\n",
    "    \n",
    "relevant_info_d['req_exp']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will retrieve the longer text objects for the description of the position, the required profile, the entreprise and the recruitment process. Let's first try with the position description and then build a function to retrieve all these elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptif du poste\n",
      "En rejoignant¬†DataGenius, vous jouerez un¬†r√¥le-cl√© dans le d√©veloppement technique¬†de l‚Äôentreprise, et exploiterez notre¬†formidable potentiel de croissance.\n",
      "Int√©gr√© au sein d‚Äôune √©quipe exp√©riment√©e, vous serez amen√© √† intervenir sur deux types de missions :\n",
      "Client :\n",
      "Vous participerez √† la¬†mise en place de mod√®les de machine et deep Learning¬†pour des clients, les tester et les mettre en production. Vous serez amen√© √† vous¬†adapter¬†aux probl√©matiques et au m√©tier des clients, mais aussi faire preuve d‚Äôune¬†autonomie¬†dans l‚Äôanalyse des donn√©es qui vous seront fournis et la mise en place d‚Äôun mod√®le qui r√©pond aux objectifs qui vous seront d√©finis.\n",
      "R&D :\n",
      "Vous contribuerez au d√©veloppement des diff√©rents modules de‚Äã¬†Atlas¬†(NLP, Auto-ML, exploitation de l‚ÄôOpen Data, mise en place d‚ÄôAPI, etc.) :\n",
      "Effectuer une √©tude bibliographique sur l'√©tat de l‚Äôart concernant le module choisi\n",
      "Proposer une solution ad√©quate\n",
      "D√©finir le protocole exp√©rimental¬†(les donn√©es √† utiliser, la fonction d'√©valuation, et la baseline avec laquelle comparer les r√©sultats)\n",
      "Impl√©menter une solution et la tester selon le protocole d√©fini\n",
      "Int√©grer le module √† la librairie ‚ÄãAtlas\n",
      "Au-del√† d'un Data Scientist nous recherchons une personne force de proposition capable de¬†mettre en place une solution adapt√©e aux besoins de nos clients.\n"
     ]
    }
   ],
   "source": [
    "pos_descr = soup.find(id='descriptif-du-poste')\n",
    "pos_descr_txt = \"\\n\".join(pos_descr.stripped_strings) #generator\n",
    "print(pos_descr_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_txt_from_soup(id_txt, soup):\n",
    "    \"\"\"Take a soup object and an id and return the full text corresponding to the section\"\"\"\n",
    "    elem = soup.find(id=id_txt)\n",
    "    if elem != None:\n",
    "        elem_txt = \"\\n\".join(elem.stripped_strings) #generator\n",
    "        if id_txt == 'entreprise': #noisy text\n",
    "            elem_txt = re.sub(\"\\nAutres offres de l'entreprise\", '', elem_txt)\n",
    "        elif id_txt == 'processus-de-recrutement': #noisy text\n",
    "            elem_txt = re.sub('\\n?.*POSTULER.*\\nImprimer\\nSignaler cette offre', '', \n",
    "                              elem_txt)\n",
    "        return elem_txt\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_in_l = ['descriptif-du-poste', 'profil-recherche', 'entreprise', 'processus-de-recrutement']\n",
    "id_out_l = ['pos_descr', 'profil', 'comp_info', 'recruit_proc']\n",
    "\n",
    "for id_in, id_out in zip(id_in_l, id_out_l):\n",
    "    relevant_info_d[id_out] = fetch_txt_from_soup(id_in, soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profil recherch√©\n",
      "De niveau Bac+4/5 ou √©quivalent en¬†√©cole d'ing√©nieur,¬†vous poss√©dez une sp√©cialisation en math√©matiques appliqu√©es, statistiques, machine learning ou intelligence artificielle.\n",
      "Vous affichez des capacit√©s √† manipuler et traiter des donn√©es en provenance de sources diverses.\n",
      "Vous d√©montrez du go√ªt pour la r√©solution de probl√®mes par des approches quantitatives en se formant sur des techniques avanc√©es de Machine Learning.\n",
      "Vous avez la volont√© de rejoindre une aventure entrepreneuriale avec une √©quipe ambitieuse, tourn√©e vers l‚Äôinnovation.\n",
      "Une bonne ma√Ætrise de l‚Äôanglais est indispensable.\n",
      "Vous ma√Ætrisez l'utilisation de¬† Git, Python (Scikit-learn, Numpy, Pandas, Keras, Tensorflow) et optionnellement Linux, Docker, Django, Spark.\n"
     ]
    }
   ],
   "source": [
    "print(relevant_info_d['profil'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, the recruitment responsible (if provided)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Taha Zemmouri - Pr√©sident'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elem = soup.find('i')\n",
    "try:\n",
    "    recruit_resp = elem.previous_element + elem.text\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    recruit_resp=''\n",
    "\n",
    "recruit_resp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**This is pretty much everything we need to do to scrap the APEC job board.\n",
    "<br> The only thing left is to define some nice functions to proceed the whole step by step approach and build our own job database.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>b) All in one functions</h4>\n",
    "<a id=\"1.1.b\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define 3 main functions to handle the full web scraping process and build our own databe :\n",
    "* 1st : initiate the webdrive and request the job platform on position key words and location and precise a sorting option for the result;\n",
    "* 2nd : take the driver returned by the first function and browse all the result pages to fetch the URL of every job offer;\n",
    "* 3rd : load source code of every web page provided by the second function, scrape it with BeautifulSoup and store the information in a list of dictionnaries (in anticipation of the coming dataframe). \n",
    "\n",
    "Nothing special to mention for these 3 functions in comparison to the previous step by step presentation. I just added some time.sleep() in the code because of a few erros I encountered while dealing with bigger data. This prevents server overwhelming, allows the page to fully load/wait for a specific element to appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_URL = \"https://cadres.apec.fr/home/mes-offres-d-emploi.html\"\n",
    "JOB_KW = \"data scientist OU data analyst\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_SAVING_DIR = './job_db'\n",
    "os.makedirs(DB_SAVING_DIR, exist_ok=True)\n",
    "DB_APEC_SAVING_PATH = os.path.join(DB_SAVING_DIR, 'apec_db.xlsx')\n",
    "DB_APEC_NEW_SAVING_PATH = os.path.join(DB_SAVING_DIR, 'apec_new.xlsx') #save also new offer in a seperate file for late"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_apec_platform(root_url=ROOT_URL, job_kw=JOB_KW, job_loc=None,\n",
    "                          sort_by='Pertinence'):\n",
    "    \"\"\"Initiate the webdrive and request the job platform on position key words and location and\n",
    "    precise a sorting option for the result\"\"\"\n",
    "    global driver\n",
    "    try:\n",
    "        driver.quit()\n",
    "    except:\n",
    "        pass\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--ignore-certificate-errors')\n",
    "    options.add_argument('--incognito')\n",
    "    options.add_argument('--headless')\n",
    "    options.add_argument(\"--window-size=1366, 768\")\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    driver.get(root_url)\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(2)\n",
    "    try:\n",
    "        driver.find_element_by_xpath(\"//button[@title='Accepter']\").click() #cookie popup\n",
    "        time.sleep(2)\n",
    "    except:\n",
    "        pass\n",
    "    kw_elem = driver.find_element_by_id(\"keywords\")\n",
    "    kw_elem.clear()\n",
    "    kw_elem.send_keys(job_kw)\n",
    "    if job_loc != None:\n",
    "        loc_elem = driver.find_element_by_xpath(\"//input[@placeholder='Ex : Paris, Lyon ...']\")\n",
    "        loc_elem.clear()\n",
    "        loc_elem.send_keys(job_loc, Keys.RETURN)\n",
    "    else:\n",
    "        driver.find_element_by_class_name('btn-block').click()\n",
    "    sort_by_el = driver.find_element_by_link_text(sort_by)\n",
    "    sort_by_el.click()\n",
    "    print(\"Job search request was performed on:\\n\"\n",
    "          \"Job title: {}, Job location: {}, sorted by {}\".format(job_kw, job_loc, sort_by))\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_apec_job_links(driver=driver, old_db_path=DB_APEC_SAVING_PATH, \n",
    "                           max_consec_existing_link=40):\n",
    "    \"\"\"Take the driver request and browse all the result pages to fetch the URL of every job offer\"\"\"\n",
    "    page_nb = 1\n",
    "    job_links = []\n",
    "    consec_existing_link = 0\n",
    "    update = os.path.isfile(old_db_path)\n",
    "    if update is True:\n",
    "        print('A previous database already exists. Only new job links will be fetched')\n",
    "        old_db = pd.read_excel(old_db_path)\n",
    "        old_job_links = old_db['url'].to_list()\n",
    "    while True:\n",
    "        time.sleep(1)\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(1)\n",
    "        job_titles = driver.find_elements_by_class_name('offre-title')\n",
    "        for i in range(len(job_titles)):\n",
    "            job_link = job_titles[i].find_element_by_css_selector('a').get_attribute('href')\n",
    "            job_link = re.split('&totalCount', job_link)[0]\n",
    "            if (update is False) or (job_link not in old_job_links):\n",
    "                job_links.append(job_link)\n",
    "                consec_existing_link = 0\n",
    "            else:\n",
    "                consec_existing_link += 1\n",
    "                if consec_existing_link >= max_consec_existing_link:\n",
    "                    print(\"\\nNo more new links to retrieve\")\n",
    "                    break  \n",
    "        try:\n",
    "            assert consec_existing_link < max_consec_existing_link\n",
    "            next_page = driver.find_element_by_link_text('Suiv.').click()\n",
    "            page_nb += 1\n",
    "            print(\"Navigating to next page ({})\".format(page_nb))\n",
    "        except:\n",
    "            print(\"\\nAll the pages ({}) have been read through\".format(page_nb))\n",
    "            break\n",
    "    return job_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapping_apec_offers(job_links, parser='lxml', LIMIT=None):\n",
    "    \"\"\"Load source code of every web page from the URL job_links list, scrape it with BeautifulSoup \n",
    "    and store the information in a list of dictionnaries\"\"\"\n",
    "    global driver\n",
    "    try:\n",
    "        driver.quit()\n",
    "    except:\n",
    "        pass\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--ignore-certificate-errors')\n",
    "    options.add_argument('--incognito')\n",
    "    options.add_argument('--headless')\n",
    "    options.add_argument(\"--window-size=1366, 768\")\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    job_parser_l = []\n",
    "    relevant_info_l = ['salary', 'empl_date', 'req_exp', 'pos_stt', 'btrip_area', 'act_area']\n",
    "    id_in_l = ['descriptif-du-poste', 'profil-recherche', 'entreprise', \n",
    "               'processus-de-recrutement']\n",
    "    id_out_l = ['pos_descr', 'profil', 'comp_info', 'recruit_proc']\n",
    "    for x, link in enumerate(job_links[:LIMIT]):\n",
    "        size = len(job_links)\n",
    "        if x % 100 == 0:\n",
    "            print('\\n{} offers have been parsed out of {}'.format(x, size))\n",
    "        try:\n",
    "            job_parser_d = {}\n",
    "            driver.get(link)\n",
    "            page_source = driver.page_source\n",
    "            soup = BeautifulSoup(page_source, parser)\n",
    "            ref = soup.find(string=re.compile('Ref\\. Apec'))\n",
    "            job_parser_d['ref_site'] = re.findall(\"Ref. Apec : (\\d*\\w*)\", ref)[0]\n",
    "            ref = soup.find(string=re.compile('Ref\\. Soci√©t√©'))\n",
    "            if ref != None:\n",
    "                job_parser_d['ref_comp'] = re.findall(\"Ref. Soci√©t√© : (\\d*\\w*)\", ref)[0]\n",
    "            else:\n",
    "                job_parser_d['ref_comp'] = ''\n",
    "            job_title = soup.find('h1', class_=\"text-uppercase ng-binding\").text\n",
    "            job_parser_d['job_title'] = re.sub(' F/H', '', job_title)\n",
    "            comp_and_loc = soup.find_all('strong', class_=\"ng-binding\")\n",
    "            job_parser_d['company'] = comp_and_loc[1].text\n",
    "            job_parser_d['location'] = comp_and_loc[2].text\n",
    "            pos_type = soup.find('span', class_=\"ng-binding ng-scope\")\n",
    "            job_parser_d['pos_type'] = pos_type.text\n",
    "            job_parser_d['nb_pos'] = next(pos_type.parent.stripped_strings)\n",
    "            pub_date = re.findall(\"(?:/*\\\\d+)+\", soup.find(string=re.compile('Publi√©e le')))[0]\n",
    "            job_parser_d['pub_date'] = pub_date\n",
    "            act_date = re.findall(\"(?:/*\\\\d+)+\", soup.find(string=re.compile('Actualis√©e le')))[0]\n",
    "            job_parser_d['act_date'] = act_date\n",
    "            pos_info = soup.find_all(tag_strong_but_no_class)\n",
    "            for idx, info in enumerate(relevant_info_l):\n",
    "                job_parser_d[info] = pos_info[idx].next_sibling            \n",
    "            for id_in, id_out in zip(id_in_l, id_out_l):\n",
    "                job_parser_d[id_out] = fetch_txt_from_soup(id_txt=id_in, soup=soup)\n",
    "            recruiter = soup.find('i')\n",
    "            try:\n",
    "                recruit_resp = recruiter.previous_element + recruiter.text\n",
    "            except :\n",
    "                recruit_resp = ''\n",
    "            job_parser_d['recruit_resp'] = recruit_resp\n",
    "            job_parser_d['url'] = link\n",
    "                        \n",
    "            job_parser_l.append(job_parser_d)\n",
    "        except:\n",
    "            print('\\n', '~-'*20, 'WARNING', '~-'*20,\n",
    "            '\\nAn issue was encountered with an offer. It might probably be no longer'\n",
    "                ' available at the parsing time.',\n",
    "            '\\nAs a result this offer could not be added to the database. '\n",
    "            'You should investigate job_link number {}'.format(x),\n",
    "            '\\n', '~-'*20, 'WARNING', '~-'*20)\n",
    "    print(\"\\nAll the offers have been processed\")\n",
    "    return job_parser_l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, wrap these three functions into one to perform the full web scraping process and create/update the APEC job database. Here I chose to work with a simple Excel file but a real relational database could be a better option (in particular to feed and update the database !). \n",
    "<br> Let's first define a short function to feed an excel file with new recordings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_excel_file(old_file_path, new_df):\n",
    "    \"\"\"Feed new recordings from a dataframe to an old excel file with same format\"\"\"\n",
    "    df_db = pd.read_excel(old_file_path)\n",
    "    df_full = df_db.append(new_df, ignore_index=True)\n",
    "    with pd.ExcelWriter(old_file_path, options={'strings_to_urls': False}) as writer:\n",
    "            df_full.to_excel(writer, index=False)\n",
    "    return df_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_apec_db(db_path=DB_APEC_SAVING_PATH, new_file_path=DB_APEC_NEW_SAVING_PATH):\n",
    "    \"\"\"Perform the full web scraping process and create/update the APEC database\"\"\"\n",
    "    driver = request_apec_platform(sort_by='Date')\n",
    "    time.sleep(1)\n",
    "    job_links_new = fetch_apec_job_links(driver=driver, old_db_path=db_path)\n",
    "    print('{} new offers were founded and will be parsed'.format(len(job_links_new)))\n",
    "    new_job_parser = scrapping_apec_offers(job_links=job_links_new)\n",
    "    df_apec_new = pd.DataFrame(new_job_parser)\n",
    "    update = os.path.isfile(db_path)\n",
    "    if update:\n",
    "        df_apec_full = update_excel_file(db_path, df_apec_new)\n",
    "        with pd.ExcelWriter(new_file_path, options={'strings_to_urls': False}) as writer:\n",
    "            df_apec_new.to_excel(writer, index=False)\n",
    "        print('\\nDone: the new offers have been saved and added to the APEC DB')\n",
    "        return df_apec_full\n",
    "    else:\n",
    "        with pd.ExcelWriter(db_path, options={'strings_to_urls': False}) as writer:\n",
    "            df_apec_new.to_excel(writer, index=False)\n",
    "        print('\\nDone: the APEC DB has been created and the offers have been saved')\n",
    "        return df_apec_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job search request was performed on:\n",
      "Job title: data scientist OU data analyst, Job location: None, sorted by Date\n",
      "A previous database already exists. Only new job links will be fetched\n",
      "Navigating to next page (2)\n",
      "Navigating to next page (3)\n",
      "Navigating to next page (4)\n",
      "Navigating to next page (5)\n",
      "\n",
      "No more new links to retrieve\n",
      "\n",
      "All the pages (5) have been read through\n",
      "17 new offers were founded and will be parsed\n",
      "\n",
      "0 offers have been parsed out of 17\n",
      "\n",
      "All the offers have been processed\n",
      "\n",
      "Done: the new offers have been saved and added to the APEC DB\n"
     ]
    }
   ],
   "source": [
    "df_apec_full = update_apec_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>act_area</th>\n",
       "      <th>act_date</th>\n",
       "      <th>btrip_area</th>\n",
       "      <th>comp_info</th>\n",
       "      <th>company</th>\n",
       "      <th>empl_date</th>\n",
       "      <th>job_title</th>\n",
       "      <th>location</th>\n",
       "      <th>nb_pos</th>\n",
       "      <th>pos_descr</th>\n",
       "      <th>...</th>\n",
       "      <th>pos_type</th>\n",
       "      <th>profil</th>\n",
       "      <th>pub_date</th>\n",
       "      <th>recruit_proc</th>\n",
       "      <th>recruit_resp</th>\n",
       "      <th>ref_comp</th>\n",
       "      <th>ref_site</th>\n",
       "      <th>req_exp</th>\n",
       "      <th>salary</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Conseil pour les affaires et autres conseils ...</td>\n",
       "      <td>15/05/2019</td>\n",
       "      <td>Pas de d√©placement</td>\n",
       "      <td>Entreprise\\nNous cherchons pour notre client d...</td>\n",
       "      <td>MINEO</td>\n",
       "      <td>D√®s que possible</td>\n",
       "      <td>Data Analyst/Scientist</td>\n",
       "      <td>Aix-en-Provence - 13</td>\n",
       "      <td>1</td>\n",
       "      <td>Descriptif du poste\\nAmen√© √† travailler sur de...</td>\n",
       "      <td>...</td>\n",
       "      <td>CDI</td>\n",
       "      <td>Profil recherch√©\\nDe ce fait nous recherchons ...</td>\n",
       "      <td>11/04/2019</td>\n",
       "      <td>Processus de recrutement\\nPersonne en charge d...</td>\n",
       "      <td>Gr√©goire CLEMENT - Founder</td>\n",
       "      <td>NaN</td>\n",
       "      <td>164141813W</td>\n",
       "      <td>Minimum 3 ans</td>\n",
       "      <td>A partir de 40 k‚Ç¨ brut annuel</td>\n",
       "      <td>https://cadres.apec.fr/offres-emploi-cadres/0_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Conseil en syst√®mes et logiciels informatiques</td>\n",
       "      <td>15/05/2019</td>\n",
       "      <td>D√©partementale</td>\n",
       "      <td>Entreprise\\nModis recrute 1000 Talents en 2019...</td>\n",
       "      <td>MODIS FRANCE</td>\n",
       "      <td>D√®s que possible</td>\n",
       "      <td>Consultant Data analyst / Data scientist</td>\n",
       "      <td>Toulouse - 31</td>\n",
       "      <td>1</td>\n",
       "      <td>Descriptif du poste\\nVous souhaitez c√¥toyer au...</td>\n",
       "      <td>...</td>\n",
       "      <td>CDI</td>\n",
       "      <td>Profil recherch√©\\nDe formation Bac+5/ Ing√©nieu...</td>\n",
       "      <td>15/05/2019</td>\n",
       "      <td>Processus de recrutement\\nPersonne en charge d...</td>\n",
       "      <td>Lory Lo Moro - Recruteur</td>\n",
       "      <td>MID</td>\n",
       "      <td>164236918W</td>\n",
       "      <td>Minimum 1 an</td>\n",
       "      <td>A n√©gocier</td>\n",
       "      <td>https://cadres.apec.fr/offres-emploi-cadres/0_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Autre mise √† disposition de ressources humaines</td>\n",
       "      <td>03/06/2019</td>\n",
       "      <td>D√©partementale</td>\n",
       "      <td>Entreprise\\nNous sommes un cabinet de conseil ...</td>\n",
       "      <td>ATHANOR INFORMATIQUE</td>\n",
       "      <td>D√®s que possible</td>\n",
       "      <td>DATA ANALYST / DATA SCIENTIST</td>\n",
       "      <td>Suresnes - 92</td>\n",
       "      <td>1</td>\n",
       "      <td>Descriptif du poste\\nData Analyst/ Data Scient...</td>\n",
       "      <td>...</td>\n",
       "      <td>CDI</td>\n",
       "      <td>Profil recherch√©\\nData Analyst/ Data Scientist...</td>\n",
       "      <td>03/06/2019</td>\n",
       "      <td>Processus de recrutement\\nNous recevrons les c...</td>\n",
       "      <td>THIERRY JOUDELAT - Direction</td>\n",
       "      <td>DATA</td>\n",
       "      <td>164266817W</td>\n",
       "      <td>Minimum 2 ans</td>\n",
       "      <td>A n√©gocier</td>\n",
       "      <td>https://cadres.apec.fr/offres-emploi-cadres/0_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Conseil pour les affaires et autres conseils ...</td>\n",
       "      <td>29/05/2019</td>\n",
       "      <td>Pas de d√©placement</td>\n",
       "      <td>Entreprise\\nEasy Partner fait aujourd'hui part...</td>\n",
       "      <td>EASY PARTNER</td>\n",
       "      <td>D√®s que possible</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Paris 02 - 75</td>\n",
       "      <td>1</td>\n",
       "      <td>Descriptif du poste\\nLa soci√©t√©¬†:\\nNotre clien...</td>\n",
       "      <td>...</td>\n",
       "      <td>CDI</td>\n",
       "      <td>Profil recherch√©\\nLe profil¬†:\\nVous ma√Ætrisez ...</td>\n",
       "      <td>29/05/2019</td>\n",
       "      <td>Processus de recrutement\\nPersonne en charge d...</td>\n",
       "      <td>. Masson - Consultant en recrutement</td>\n",
       "      <td>NaN</td>\n",
       "      <td>164259645W</td>\n",
       "      <td>Minimum 1 an</td>\n",
       "      <td>40 - 45 k‚Ç¨ brut annuel</td>\n",
       "      <td>https://cadres.apec.fr/offres-emploi-cadres/0_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Conseil pour les affaires et autres conseils ...</td>\n",
       "      <td>29/05/2019</td>\n",
       "      <td>R√©gionale</td>\n",
       "      <td>Entreprise\\nDu plaisir √† faire son travail, da...</td>\n",
       "      <td>SIDERLOG</td>\n",
       "      <td>D√®s que possible</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Niort - 79</td>\n",
       "      <td>1</td>\n",
       "      <td>Descriptif du poste\\nPour renforcer son √©quipe...</td>\n",
       "      <td>...</td>\n",
       "      <td>CDI</td>\n",
       "      <td>Profil recherch√©\\nDe formation d√©cisionnelle, ...</td>\n",
       "      <td>29/05/2019</td>\n",
       "      <td>Processus de recrutement\\nPersonne en charge d...</td>\n",
       "      <td>Fran√ßoise LEGER - Assistante charg√©e de recrut...</td>\n",
       "      <td>Data</td>\n",
       "      <td>164279571W</td>\n",
       "      <td>Minimum 5 ans</td>\n",
       "      <td>A n√©gocier</td>\n",
       "      <td>https://cadres.apec.fr/offres-emploi-cadres/0_...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            act_area    act_date  \\\n",
       "0   Conseil pour les affaires et autres conseils ...  15/05/2019   \n",
       "1     Conseil en syst√®mes et logiciels informatiques  15/05/2019   \n",
       "2    Autre mise √† disposition de ressources humaines  03/06/2019   \n",
       "3   Conseil pour les affaires et autres conseils ...  29/05/2019   \n",
       "4   Conseil pour les affaires et autres conseils ...  29/05/2019   \n",
       "\n",
       "            btrip_area                                          comp_info  \\\n",
       "0   Pas de d√©placement  Entreprise\\nNous cherchons pour notre client d...   \n",
       "1       D√©partementale  Entreprise\\nModis recrute 1000 Talents en 2019...   \n",
       "2       D√©partementale  Entreprise\\nNous sommes un cabinet de conseil ...   \n",
       "3   Pas de d√©placement  Entreprise\\nEasy Partner fait aujourd'hui part...   \n",
       "4            R√©gionale  Entreprise\\nDu plaisir √† faire son travail, da...   \n",
       "\n",
       "                company          empl_date  \\\n",
       "0                 MINEO   D√®s que possible   \n",
       "1          MODIS FRANCE   D√®s que possible   \n",
       "2  ATHANOR INFORMATIQUE   D√®s que possible   \n",
       "3          EASY PARTNER   D√®s que possible   \n",
       "4              SIDERLOG   D√®s que possible   \n",
       "\n",
       "                                  job_title              location nb_pos  \\\n",
       "0                    Data Analyst/Scientist  Aix-en-Provence - 13      1   \n",
       "1  Consultant Data analyst / Data scientist         Toulouse - 31      1   \n",
       "2             DATA ANALYST / DATA SCIENTIST         Suresnes - 92      1   \n",
       "3                            Data Scientist         Paris 02 - 75      1   \n",
       "4                            Data Scientist            Niort - 79      1   \n",
       "\n",
       "                                           pos_descr  ... pos_type  \\\n",
       "0  Descriptif du poste\\nAmen√© √† travailler sur de...  ...      CDI   \n",
       "1  Descriptif du poste\\nVous souhaitez c√¥toyer au...  ...      CDI   \n",
       "2  Descriptif du poste\\nData Analyst/ Data Scient...  ...      CDI   \n",
       "3  Descriptif du poste\\nLa soci√©t√©¬†:\\nNotre clien...  ...      CDI   \n",
       "4  Descriptif du poste\\nPour renforcer son √©quipe...  ...      CDI   \n",
       "\n",
       "                                              profil    pub_date  \\\n",
       "0  Profil recherch√©\\nDe ce fait nous recherchons ...  11/04/2019   \n",
       "1  Profil recherch√©\\nDe formation Bac+5/ Ing√©nieu...  15/05/2019   \n",
       "2  Profil recherch√©\\nData Analyst/ Data Scientist...  03/06/2019   \n",
       "3  Profil recherch√©\\nLe profil¬†:\\nVous ma√Ætrisez ...  29/05/2019   \n",
       "4  Profil recherch√©\\nDe formation d√©cisionnelle, ...  29/05/2019   \n",
       "\n",
       "                                        recruit_proc  \\\n",
       "0  Processus de recrutement\\nPersonne en charge d...   \n",
       "1  Processus de recrutement\\nPersonne en charge d...   \n",
       "2  Processus de recrutement\\nNous recevrons les c...   \n",
       "3  Processus de recrutement\\nPersonne en charge d...   \n",
       "4  Processus de recrutement\\nPersonne en charge d...   \n",
       "\n",
       "                                        recruit_resp ref_comp    ref_site  \\\n",
       "0                         Gr√©goire CLEMENT - Founder      NaN  164141813W   \n",
       "1                           Lory Lo Moro - Recruteur      MID  164236918W   \n",
       "2                       THIERRY JOUDELAT - Direction     DATA  164266817W   \n",
       "3               . Masson - Consultant en recrutement      NaN  164259645W   \n",
       "4  Fran√ßoise LEGER - Assistante charg√©e de recrut...     Data  164279571W   \n",
       "\n",
       "          req_exp                          salary  \\\n",
       "0   Minimum 3 ans   A partir de 40 k‚Ç¨ brut annuel   \n",
       "1    Minimum 1 an                      A n√©gocier   \n",
       "2   Minimum 2 ans                      A n√©gocier   \n",
       "3    Minimum 1 an          40 - 45 k‚Ç¨ brut annuel   \n",
       "4   Minimum 5 ans                      A n√©gocier   \n",
       "\n",
       "                                                 url  \n",
       "0  https://cadres.apec.fr/offres-emploi-cadres/0_...  \n",
       "1  https://cadres.apec.fr/offres-emploi-cadres/0_...  \n",
       "2  https://cadres.apec.fr/offres-emploi-cadres/0_...  \n",
       "3  https://cadres.apec.fr/offres-emploi-cadres/0_...  \n",
       "4  https://cadres.apec.fr/offres-emploi-cadres/0_...  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(df_apec_full.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>act_area</th>\n",
       "      <th>act_date</th>\n",
       "      <th>btrip_area</th>\n",
       "      <th>comp_info</th>\n",
       "      <th>company</th>\n",
       "      <th>empl_date</th>\n",
       "      <th>job_title</th>\n",
       "      <th>location</th>\n",
       "      <th>nb_pos</th>\n",
       "      <th>pos_descr</th>\n",
       "      <th>...</th>\n",
       "      <th>pos_type</th>\n",
       "      <th>profil</th>\n",
       "      <th>pub_date</th>\n",
       "      <th>recruit_proc</th>\n",
       "      <th>recruit_resp</th>\n",
       "      <th>ref_comp</th>\n",
       "      <th>ref_site</th>\n",
       "      <th>req_exp</th>\n",
       "      <th>salary</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1802</th>\n",
       "      <td>√âtudes de march√© et sondages</td>\n",
       "      <td>14/08/2019</td>\n",
       "      <td>Pas de d√©placement</td>\n",
       "      <td>Entreprise\\nIRI, le leader mondial dans les do...</td>\n",
       "      <td>INFORMATION RESOURCES</td>\n",
       "      <td>D√®s que possible</td>\n",
       "      <td>CHARGE(E) D‚ÄôETUDES ‚Äì Service Mod√®les  IRI ‚Äì Ch...</td>\n",
       "      <td>Chambourcy - 78</td>\n",
       "      <td>1</td>\n",
       "      <td>Descriptif du poste\\nIRI recherche un(e) Charg...</td>\n",
       "      <td>...</td>\n",
       "      <td>CDI</td>\n",
       "      <td>Profil recherch√©\\nVous √™tes dipl√¥m√©(e) d‚Äôun BA...</td>\n",
       "      <td>14/08/2019</td>\n",
       "      <td>Processus de recrutement\\nPersonne en charge d...</td>\n",
       "      <td>Solene Lavergne - Responsable RH</td>\n",
       "      <td></td>\n",
       "      <td>164480915W</td>\n",
       "      <td>Tous niveaux d'exp√©rience accept√©s</td>\n",
       "      <td>34 - 40 k‚Ç¨ brut annuel</td>\n",
       "      <td>https://cadres.apec.fr/offres-emploi-cadres/0_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1803</th>\n",
       "      <td>Conseil pour les affaires et autres conseils ...</td>\n",
       "      <td>14/08/2019</td>\n",
       "      <td>Internationale</td>\n",
       "      <td>Entreprise\\nAdoc Talent Management recrute un¬∑...</td>\n",
       "      <td>ADOC TALENT MANAGEMENT</td>\n",
       "      <td>D√®s que possible</td>\n",
       "      <td>Chef¬∑fe d'√©quipe d√©veloppement</td>\n",
       "      <td>Paris 12 - 75</td>\n",
       "      <td>1</td>\n",
       "      <td>Descriptif du poste\\nEn lien direct avec le C....</td>\n",
       "      <td>...</td>\n",
       "      <td>CDI</td>\n",
       "      <td>Profil recherch√©\\nTitulaire d‚Äôun dipl√¥me d‚Äôing...</td>\n",
       "      <td>26/07/2019</td>\n",
       "      <td>Processus de recrutement\\nPersonne en charge d...</td>\n",
       "      <td>Faustine Bizet - Consultante en Recrutement</td>\n",
       "      <td></td>\n",
       "      <td>164441597W</td>\n",
       "      <td>Minimum 3 ans</td>\n",
       "      <td>A n√©gocier</td>\n",
       "      <td>https://cadres.apec.fr/offres-emploi-cadres/0_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1804</th>\n",
       "      <td>Conseil en syst√®mes et logiciels informatiques</td>\n",
       "      <td>14/08/2019</td>\n",
       "      <td>Pas de d√©placement</td>\n",
       "      <td>Entreprise\\nAlligator, est une soci√©t√© au c≈ìur...</td>\n",
       "      <td>ALLIGATOR</td>\n",
       "      <td>D√®s que possible</td>\n",
       "      <td>Expert Azure DevOps</td>\n",
       "      <td>Valbonne - 06</td>\n",
       "      <td>1</td>\n",
       "      <td>Descriptif du poste\\nMission\\nVous √©voluerez d...</td>\n",
       "      <td>...</td>\n",
       "      <td>CDI</td>\n",
       "      <td>Profil recherch√©\\nProfil\\nDipl√¥me sup√©rieur da...</td>\n",
       "      <td>14/08/2019</td>\n",
       "      <td>Processus de recrutement\\nPersonne en charge d...</td>\n",
       "      <td>C√©line Grimaldi - Responsable RH &amp; recrutement</td>\n",
       "      <td></td>\n",
       "      <td>164480855W</td>\n",
       "      <td>Minimum 4 ans</td>\n",
       "      <td>A n√©gocier</td>\n",
       "      <td>https://cadres.apec.fr/offres-emploi-cadres/0_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1805</th>\n",
       "      <td>Conseil en syst√®mes et logiciels informatiques</td>\n",
       "      <td>14/08/2019</td>\n",
       "      <td>Pas de d√©placement</td>\n",
       "      <td>Entreprise\\nQUI SOMMES-NOUS¬†?\\nADONIS est une ...</td>\n",
       "      <td>ADONIS</td>\n",
       "      <td>D√®s que possible</td>\n",
       "      <td>Lead Technique Hadoop - Big Data</td>\n",
       "      <td>Paris 08 - 75</td>\n",
       "      <td>1</td>\n",
       "      <td>Descriptif du poste\\nDescription de la mission...</td>\n",
       "      <td>...</td>\n",
       "      <td>CDI</td>\n",
       "      <td>Profil recherch√©\\nProfil recherch√© :\\nVous pr√©...</td>\n",
       "      <td>14/08/2019</td>\n",
       "      <td>Processus de recrutement\\nProcess RH ADONIS :\\...</td>\n",
       "      <td>AGNES LACOMBE - DRH</td>\n",
       "      <td>LT_HADOOP_BIGDATA_140819</td>\n",
       "      <td>164480795W</td>\n",
       "      <td>Minimum 2 ans</td>\n",
       "      <td>40 - 50 k‚Ç¨ brut annuel</td>\n",
       "      <td>https://cadres.apec.fr/offres-emploi-cadres/0_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1806</th>\n",
       "      <td>Conseil en syst√®mes et logiciels informatiques</td>\n",
       "      <td>14/08/2019</td>\n",
       "      <td>Pas de d√©placement</td>\n",
       "      <td>Entreprise\\nQUI SOMMES-NOUS¬†?\\nADONIS est une ...</td>\n",
       "      <td>ADONIS</td>\n",
       "      <td>D√®s que possible</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Paris 08 - 75</td>\n",
       "      <td>1</td>\n",
       "      <td>Descriptif du poste\\nDans le cadre de notre d√©...</td>\n",
       "      <td>...</td>\n",
       "      <td>CDI</td>\n",
       "      <td>Profil recherch√©\\nProfil recherch√© :\\nDe forma...</td>\n",
       "      <td>14/08/2019</td>\n",
       "      <td>Processus de recrutement\\nProcess RH ADONIS :\\...</td>\n",
       "      <td>AGNES LACOMBE - DRH</td>\n",
       "      <td>DATA_SCIENT_AS_140819</td>\n",
       "      <td>164480667W</td>\n",
       "      <td>Minimum 5 ans</td>\n",
       "      <td>40 - 55 k‚Ç¨ brut annuel</td>\n",
       "      <td>https://cadres.apec.fr/offres-emploi-cadres/0_...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               act_area    act_date  \\\n",
       "1802                       √âtudes de march√© et sondages  14/08/2019   \n",
       "1803   Conseil pour les affaires et autres conseils ...  14/08/2019   \n",
       "1804     Conseil en syst√®mes et logiciels informatiques  14/08/2019   \n",
       "1805     Conseil en syst√®mes et logiciels informatiques  14/08/2019   \n",
       "1806     Conseil en syst√®mes et logiciels informatiques  14/08/2019   \n",
       "\n",
       "               btrip_area                                          comp_info  \\\n",
       "1802   Pas de d√©placement  Entreprise\\nIRI, le leader mondial dans les do...   \n",
       "1803       Internationale  Entreprise\\nAdoc Talent Management recrute un¬∑...   \n",
       "1804   Pas de d√©placement  Entreprise\\nAlligator, est une soci√©t√© au c≈ìur...   \n",
       "1805   Pas de d√©placement  Entreprise\\nQUI SOMMES-NOUS¬†?\\nADONIS est une ...   \n",
       "1806   Pas de d√©placement  Entreprise\\nQUI SOMMES-NOUS¬†?\\nADONIS est une ...   \n",
       "\n",
       "                     company          empl_date  \\\n",
       "1802   INFORMATION RESOURCES   D√®s que possible   \n",
       "1803  ADOC TALENT MANAGEMENT   D√®s que possible   \n",
       "1804               ALLIGATOR   D√®s que possible   \n",
       "1805                  ADONIS   D√®s que possible   \n",
       "1806                  ADONIS   D√®s que possible   \n",
       "\n",
       "                                              job_title         location  \\\n",
       "1802  CHARGE(E) D‚ÄôETUDES ‚Äì Service Mod√®les  IRI ‚Äì Ch...  Chambourcy - 78   \n",
       "1803                     Chef¬∑fe d'√©quipe d√©veloppement    Paris 12 - 75   \n",
       "1804                                Expert Azure DevOps    Valbonne - 06   \n",
       "1805                   Lead Technique Hadoop - Big Data    Paris 08 - 75   \n",
       "1806                                     Data Scientist    Paris 08 - 75   \n",
       "\n",
       "     nb_pos                                          pos_descr  ... pos_type  \\\n",
       "1802      1  Descriptif du poste\\nIRI recherche un(e) Charg...  ...      CDI   \n",
       "1803      1  Descriptif du poste\\nEn lien direct avec le C....  ...      CDI   \n",
       "1804      1  Descriptif du poste\\nMission\\nVous √©voluerez d...  ...      CDI   \n",
       "1805      1  Descriptif du poste\\nDescription de la mission...  ...      CDI   \n",
       "1806      1  Descriptif du poste\\nDans le cadre de notre d√©...  ...      CDI   \n",
       "\n",
       "                                                 profil    pub_date  \\\n",
       "1802  Profil recherch√©\\nVous √™tes dipl√¥m√©(e) d‚Äôun BA...  14/08/2019   \n",
       "1803  Profil recherch√©\\nTitulaire d‚Äôun dipl√¥me d‚Äôing...  26/07/2019   \n",
       "1804  Profil recherch√©\\nProfil\\nDipl√¥me sup√©rieur da...  14/08/2019   \n",
       "1805  Profil recherch√©\\nProfil recherch√© :\\nVous pr√©...  14/08/2019   \n",
       "1806  Profil recherch√©\\nProfil recherch√© :\\nDe forma...  14/08/2019   \n",
       "\n",
       "                                           recruit_proc  \\\n",
       "1802  Processus de recrutement\\nPersonne en charge d...   \n",
       "1803  Processus de recrutement\\nPersonne en charge d...   \n",
       "1804  Processus de recrutement\\nPersonne en charge d...   \n",
       "1805  Processus de recrutement\\nProcess RH ADONIS :\\...   \n",
       "1806  Processus de recrutement\\nProcess RH ADONIS :\\...   \n",
       "\n",
       "                                        recruit_resp  \\\n",
       "1802                Solene Lavergne - Responsable RH   \n",
       "1803     Faustine Bizet - Consultante en Recrutement   \n",
       "1804  C√©line Grimaldi - Responsable RH & recrutement   \n",
       "1805                             AGNES LACOMBE - DRH   \n",
       "1806                             AGNES LACOMBE - DRH   \n",
       "\n",
       "                      ref_comp    ref_site  \\\n",
       "1802                            164480915W   \n",
       "1803                            164441597W   \n",
       "1804                            164480855W   \n",
       "1805  LT_HADOOP_BIGDATA_140819  164480795W   \n",
       "1806     DATA_SCIENT_AS_140819  164480667W   \n",
       "\n",
       "                                  req_exp                   salary  \\\n",
       "1802   Tous niveaux d'exp√©rience accept√©s   34 - 40 k‚Ç¨ brut annuel   \n",
       "1803                        Minimum 3 ans               A n√©gocier   \n",
       "1804                        Minimum 4 ans               A n√©gocier   \n",
       "1805                        Minimum 2 ans   40 - 50 k‚Ç¨ brut annuel   \n",
       "1806                        Minimum 5 ans   40 - 55 k‚Ç¨ brut annuel   \n",
       "\n",
       "                                                    url  \n",
       "1802  https://cadres.apec.fr/offres-emploi-cadres/0_...  \n",
       "1803  https://cadres.apec.fr/offres-emploi-cadres/0_...  \n",
       "1804  https://cadres.apec.fr/offres-emploi-cadres/0_...  \n",
       "1805  https://cadres.apec.fr/offres-emploi-cadres/0_...  \n",
       "1806  https://cadres.apec.fr/offres-emploi-cadres/0_...  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(df_apec_full.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2) Indeed job board</h3>\n",
    "<a id=\"1.2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>a) Step by step approach</h4>\n",
    "<a id=\"1.2.a\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The step by step process for the Indeed jo board would basically alike the one for the APEC job board (even much simpler since the indeed offers contain less info and are not as well organised). Let's directly move to the all in one chapter. \n",
    "<br> \n",
    "Please not that I use the French site version for Indeed and that a few information might be in french too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>b) All in one functions</h4>\n",
    "<a id=\"1.2.b\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exactly like for the APEC job board, let's define 3 main functions to handle the full web scraping process and build our own databe :\n",
    "* 1st : initiate the webdrive and request the job platform on position key words and location and precise a sorting option for the result;\n",
    "* 2nd : take the driver returned by the first function and browse all the result pages to fetch the URL of every job offer;\n",
    "* 3rd : load source code of every web page provided by the second function, scrape it with BeautifulSoup and store the information in a list of dictionnaries (in anticipation of the coming dataframe). \n",
    "\n",
    "Here too I added some time.sleep() in the code because of a few erros I encountered while dealing with bigger data. This prevents server overwhelming, allows the page to fully load/wait for a specific element to appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_URL = \"https://www.indeed.fr/advanced_search\"\n",
    "JOB_KW = \"data\" # allow to scrap on job title on Indeed :)\n",
    "JOB_LOC = 'France'\n",
    "NB_PER_PAGE = '50'\n",
    "DB_INDEED_SAVING_PATH = os.path.join(DB_SAVING_DIR, 'indeed_db.xlsx')\n",
    "DB_INDEED_NEW_SAVING_PATH = os.path.join(DB_SAVING_DIR, 'indeed_new.xlsx') #save also new offer in a seperate file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_indeed_platform(root_url=ROOT_URL, job_kw=JOB_KW, job_loc=None, \n",
    "                            lim=NB_PER_PAGE, sort_by='pertinence'):\n",
    "    \"\"\"Initiate the webdrive and request the job platform on position key words and location and\n",
    "    precise a sorting option for the result\"\"\"\n",
    "    global driver\n",
    "    try:\n",
    "        driver.quit()\n",
    "    except:\n",
    "        pass\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--ignore-certificate-errors')\n",
    "    options.add_argument('--incognito')\n",
    "    #options.add_argument('--headless')\n",
    "    #options.add_argument(\"--window-size=1366, 768\")\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    driver.get(root_url)\n",
    "    try:   \n",
    "        driver.find_element_by_css_selector(\"[class='tos-Button tos-Button-white']\").click()\n",
    "        time.sleep(1)\n",
    "    except:\n",
    "        pass \n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    kw_elem = driver.find_element_by_id(\"as_ttl\")\n",
    "    kw_elem.clear()\n",
    "    kw_elem.send_keys(job_kw)\n",
    "    nb_per_page = Select(driver.find_element_by_id('limit'))\n",
    "    nb_per_page.select_by_visible_text(lim)\n",
    "    sort_by_el = Select(driver.find_element_by_id('sort'))\n",
    "    sort_by_el.select_by_visible_text(sort_by)\n",
    "    if job_loc != None:\n",
    "        loc_elem = driver.find_element_by_id(\"where\")\n",
    "        loc_elem.clear()\n",
    "        loc_elem.send_keys(job_loc, Keys.RETURN)\n",
    "    else:\n",
    "        driver.find_element_by_id('fj').click()\n",
    "    print(\"Job search request was performed on:\\n\"\n",
    "          \"Job title: {}, Job location: {}, sorted by {}\".format(job_kw, job_loc, sort_by))\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_indeed_job_links(driver=driver, old_db_path=DB_INDEED_SAVING_PATH, \n",
    "                           max_consec_existing_link=40):\n",
    "    \"\"\"Take the driver request and browse all the result pages to fetch the URL of every job offer\"\"\"\n",
    "    page_nb = 1\n",
    "    job_links = []\n",
    "    consec_existing_link = 0\n",
    "    update = os.path.isfile(old_db_path)\n",
    "    if update is True:\n",
    "        print('A previous database already exists. Only new job links will be fetched')\n",
    "        old_db = pd.read_excel(old_db_path)\n",
    "        old_job_links = old_db['url'].to_list()\n",
    "    while True:\n",
    "        time.sleep(random.randint(10, 30))\n",
    "        try:\n",
    "            popup = driver.find_element_by_class_name('icl-CloseButton')\n",
    "            popup.click() #close popup\n",
    "        except:\n",
    "            pass\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        job_titles = driver.find_elements_by_class_name(\"title\")\n",
    "        for i in range(len(job_titles)):\n",
    "            job_link = job_titles[i].find_element_by_css_selector('a').get_attribute('href')\n",
    "            if (update is False) or (job_link not in old_job_links):\n",
    "                job_links.append(job_link)\n",
    "                consec_existing_link = 0\n",
    "            else:\n",
    "                consec_existing_link += 1\n",
    "                if consec_existing_link >= max_consec_existing_link:\n",
    "                    print(\"\\nNo more new links to retrieve\")\n",
    "                    break  \n",
    "        try:\n",
    "            assert consec_existing_link < max_consec_existing_link\n",
    "            next_page = driver.find_element_by_link_text('Suivant ¬ª').click()\n",
    "            page_nb += 1\n",
    "            print(\"Navigating to next page ({})\".format(page_nb))\n",
    "        except:\n",
    "            print(\"\\nAll the pages ({}) have been read through\".format(page_nb))\n",
    "            break\n",
    "    return job_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indeed_delta_to_date(soup):\n",
    "    \"\"\"Change the Indeed very unconvenient delta date (e.g : \"One hour ago\", \"two weeks ago\") into a classic one\"\"\"\n",
    "    job_age = soup.find(\"div\", class_='jobsearch-JobMetadataFooter').text\n",
    "    if job_age == None:\n",
    "        return ''\n",
    "    else:\n",
    "        digits = re.findall('(\\d+)\\+?\\s(?:jours?|heures?|mois?)', job_age)[0]\n",
    "        now = datetime.datetime.now()\n",
    "        match = re.search('heure', job_age)\n",
    "        if match != None:\n",
    "            date = now - datetime.timedelta(hours=int(digits))\n",
    "        elif re.search('jour', job_age) != None:\n",
    "            date = now - datetime.timedelta(days=int(digits))\n",
    "        else:\n",
    "            date = now - datetime.timedelta(days=30)\n",
    "        return date.strftime('%d/%m/%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapping_indeed_offers(job_links, parser='lxml', LIMIT=None):\n",
    "    \"\"\"Load source code of every web page from the URL job_links list, scrape it with BeautifulSoup \n",
    "    and store the information in a list of dictionnaries\"\"\"\n",
    "    icons_l = ['icl-IconFunctional icl-IconFunctional--location icl-IconFunctional--md',\n",
    "               'icl-IconFunctional icl-IconFunctional--jobs icl-IconFunctional--md',\n",
    "               'icl-IconFunctional icl-IconFunctional--salary icl-IconFunctional--md']\n",
    "    att_out_l = ['location', 'pos_type', 'salary']\n",
    "    job_parser_l = []\n",
    "    while True:\n",
    "        global driver\n",
    "        try:\n",
    "            driver.quit()\n",
    "        except:\n",
    "            pass\n",
    "        options = webdriver.ChromeOptions()\n",
    "        options.add_argument('--ignore-certificate-errors')\n",
    "        options.add_argument('--incognito')\n",
    "        #options.add_argument('--headless')\n",
    "        #options.add_argument(\"--window-size=1366, 768\")\n",
    "        driver = webdriver.Chrome(options=options)\n",
    "        some_link = job_links[random.randint(0, len(job_links))]\n",
    "        driver.get(some_link)\n",
    "        page_source = driver.page_source\n",
    "        soup = BeautifulSoup(page_source, parser)\n",
    "        elem = soup.find(class_=icons_l[0])\n",
    "        if elem == None:\n",
    "            print('Webpage not displayed correctly, restarting the web driver')\n",
    "        else:\n",
    "            print('\\nWebpage displayed correctly, parsing will start... :)')\n",
    "            break\n",
    "   \n",
    "    for x, link in enumerate(job_links[:LIMIT]):\n",
    "        size = len(job_links)\n",
    "        if x % 100 == 0:\n",
    "            print('\\n{} offers have been parsed out of {}'.format(x, size))\n",
    "        try:\n",
    "            job_parser_d = {}\n",
    "            driver.get(link)\n",
    "            page_source = driver.page_source\n",
    "            soup = BeautifulSoup(page_source, parser)\n",
    "            job_title = soup.find('h3').text\n",
    "            job_parser_d['job_title'] = re.sub('\\s+[-(]?\\s?(?:F/H|H/F)\\)?', '', \n",
    "                                               job_title, flags=re.IGNORECASE)\n",
    "            comp = soup.find('div', class_='icl-u-lg-mr--sm icl-u-xs-mr--xs')\n",
    "            job_parser_d['company'] = comp.text\n",
    "            for icon, att_out in zip(icons_l, att_out_l):\n",
    "                elem = soup.find(class_=icon)\n",
    "                if elem!=None:\n",
    "                    job_parser_d[att_out] = elem.next_sibling.text\n",
    "                else:\n",
    "                    job_parser_d[att_out] = ''\n",
    "            pos_descr = soup.find(id='jobDescriptionText')\n",
    "            job_parser_d['pos_descr'] = pos_descr.text\n",
    "            job_parser_d['pub_date'] = indeed_delta_to_date(soup=soup)\n",
    "            job_parser_d['url'] = link                     \n",
    "            job_parser_l.append(job_parser_d)\n",
    "        except:\n",
    "            print('\\n', '~-'*20, 'WARNING', '~-'*20,\n",
    "            '\\nAn issue was encountered with an offer. It might probably be no longer'\n",
    "                ' available at the parsing time.',\n",
    "            '\\nAs a result this offer could not be added to the database. '\n",
    "            'You should investigate job_link number {}'.format(x),\n",
    "            '\\n', '~-'*20, 'WARNING', '~-'*20)\n",
    "    print(\"\\nAll the offers have been processed\")\n",
    "    return job_parser_l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, wrap these three functions into one to perform the full web scraping process and create/update the Indeed job database. Here I chose to work with a simple Excel file but a real relational database could be a better option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_indeed_db(db_path=DB_INDEED_SAVING_PATH, new_file_path=DB_INDEED_NEW_SAVING_PATH):\n",
    "    \"\"\"Perform the full web scraping process and create/update the Indeed database\"\"\"\n",
    "    job_driver = request_indeed_platform(sort_by='date', job_loc='France')\n",
    "    time.sleep(1)\n",
    "    job_links_new = fetch_indeed_job_links(driver=driver, old_db_path=db_path)\n",
    "    print('{} new offers were founded and will be parsed'.format(len(job_links_new)))\n",
    "    new_job_parser = scrapping_indeed_offers(job_links=job_links_new)\n",
    "    df_indeed_new = pd.DataFrame(new_job_parser)\n",
    "    update = os.path.isfile(db_path)\n",
    "    if update:\n",
    "        df_indeed_full=update_excel_file(db_path, df_indeed_new)\n",
    "        with pd.ExcelWriter(new_file_path, options={'strings_to_urls': False}) as writer:\n",
    "            df_indeed_new.to_excel(writer, index=False)\n",
    "        print('\\nDone: the new offers have been saved and added to the Indeed DB')\n",
    "        return df_indeed_full\n",
    "    else:\n",
    "        with pd.ExcelWriter(db_path, options={'strings_to_urls': False}) as writer:\n",
    "            df_indeed_new.to_excel(writer, index=False)\n",
    "        print('\\nDone: the Indeed DB has been created and the offers have been saved')\n",
    "        return df_indeed_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job search request was performed on:\n",
      "Job title: data, Job location: France, sorted by date\n",
      "A previous database already exists. Only new job links will be fetched\n",
      "Navigating to next page (2)\n",
      "Navigating to next page (3)\n",
      "Navigating to next page (4)\n",
      "Navigating to next page (5)\n",
      "\n",
      "No more new links to retrieve\n",
      "\n",
      "All the pages (5) have been read through\n",
      "94 new offers were founded and will be parsed\n",
      "\n",
      "Webpage displayed correctly, parsing will start... :)\n",
      "\n",
      "0 offers have been parsed out of 94\n",
      "\n",
      "All the offers have been processed\n",
      "\n",
      "Done: the new offers have been saved and added to the Indeed DB\n"
     ]
    }
   ],
   "source": [
    "df_indeed_full = update_indeed_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>job_title</th>\n",
       "      <th>location</th>\n",
       "      <th>pos_descr</th>\n",
       "      <th>pos_type</th>\n",
       "      <th>pub_date</th>\n",
       "      <th>salary</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OUTREMER TELECOM</td>\n",
       "      <td>Ing√©nieur IP Data</td>\n",
       "      <td>Fort-de-France (MQ)</td>\n",
       "      <td>Outremer Telecom, filiale du groupe Altice, so...</td>\n",
       "      <td>CDI</td>\n",
       "      <td>10/05/2019</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.indeed.fr/pagead/clk?mo=r&amp;ad=-6NYl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Digital Virgo</td>\n",
       "      <td>Ing√©nieur Data Architecte</td>\n",
       "      <td>Aix-en-Provence (13)</td>\n",
       "      <td>WE ARE DIGITAL VIRGO | SMART DATA PERFORMER\\n\\...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13/05/2019</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.indeed.fr/pagead/clk?mo=r&amp;ad=-6NYl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Novencia</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Paris (75)</td>\n",
       "      <td>Contexte\\nData is fuel ! Quelle que soit la fa...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10/05/2019</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.indeed.fr/pagead/clk?mo=r&amp;ad=-6NYl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5√®me Agence</td>\n",
       "      <td>Data Manager / D√©veloppeur SQL</td>\n",
       "      <td>Bordeaux (33)</td>\n",
       "      <td>Sous la responsabilit√© du DSI, le Data Manager...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10/05/2019</td>\n",
       "      <td>30¬†000 ‚Ç¨ - 40¬†000 ‚Ç¨ par an</td>\n",
       "      <td>https://www.indeed.fr/pagead/clk?mo=r&amp;ad=-6NYl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Total</td>\n",
       "      <td>Data scientist</td>\n",
       "      <td>Courbevoie (92)</td>\n",
       "      <td>Au sein de la Branche Exploration Production, ...</td>\n",
       "      <td>CDI</td>\n",
       "      <td>10/05/2019</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.indeed.fr/pagead/clk?mo=r&amp;ad=-6NYl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            company                       job_title              location  \\\n",
       "0  OUTREMER TELECOM               Ing√©nieur IP Data   Fort-de-France (MQ)   \n",
       "1     Digital Virgo       Ing√©nieur Data Architecte  Aix-en-Provence (13)   \n",
       "2          Novencia                    Data Analyst            Paris (75)   \n",
       "3       5√®me Agence  Data Manager / D√©veloppeur SQL         Bordeaux (33)   \n",
       "4             Total                  Data scientist       Courbevoie (92)   \n",
       "\n",
       "                                           pos_descr pos_type    pub_date  \\\n",
       "0  Outremer Telecom, filiale du groupe Altice, so...      CDI  10/05/2019   \n",
       "1  WE ARE DIGITAL VIRGO | SMART DATA PERFORMER\\n\\...      NaN  13/05/2019   \n",
       "2  Contexte\\nData is fuel ! Quelle que soit la fa...      NaN  10/05/2019   \n",
       "3  Sous la responsabilit√© du DSI, le Data Manager...      NaN  10/05/2019   \n",
       "4  Au sein de la Branche Exploration Production, ...      CDI  10/05/2019   \n",
       "\n",
       "                       salary  \\\n",
       "0                         NaN   \n",
       "1                         NaN   \n",
       "2                         NaN   \n",
       "3  30¬†000 ‚Ç¨ - 40¬†000 ‚Ç¨ par an   \n",
       "4                         NaN   \n",
       "\n",
       "                                                 url  \n",
       "0  https://www.indeed.fr/pagead/clk?mo=r&ad=-6NYl...  \n",
       "1  https://www.indeed.fr/pagead/clk?mo=r&ad=-6NYl...  \n",
       "2  https://www.indeed.fr/pagead/clk?mo=r&ad=-6NYl...  \n",
       "3  https://www.indeed.fr/pagead/clk?mo=r&ad=-6NYl...  \n",
       "4  https://www.indeed.fr/pagead/clk?mo=r&ad=-6NYl...  "
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(df_indeed_full.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>job_title</th>\n",
       "      <th>location</th>\n",
       "      <th>pos_descr</th>\n",
       "      <th>pos_type</th>\n",
       "      <th>pub_date</th>\n",
       "      <th>salary</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5073</th>\n",
       "      <td>BNP Paribas Personal Finance</td>\n",
       "      <td>RESPONSABLE DATA SCIENCE ET AI</td>\n",
       "      <td>Levallois-Perret (92)</td>\n",
       "      <td>RESPONSABLE DATA SCIENCE ET AI - H/F (NUM√âRO D...</td>\n",
       "      <td>CDI</td>\n",
       "      <td>02/08/2019</td>\n",
       "      <td></td>\n",
       "      <td>https://www.indeed.fr/pagead/clk?mo=r&amp;ad=-6NYl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5074</th>\n",
       "      <td>Teradata</td>\n",
       "      <td>Data Architect</td>\n",
       "      <td>92160 Antony</td>\n",
       "      <td>Requisition Number:\\n205157\\n\\nPosition Title:...</td>\n",
       "      <td></td>\n",
       "      <td>23/07/2019</td>\n",
       "      <td></td>\n",
       "      <td>https://www.indeed.fr/pagead/clk?mo=r&amp;ad=-6NYl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5075</th>\n",
       "      <td>Teradata</td>\n",
       "      <td>Data Developer</td>\n",
       "      <td>92160 Antony</td>\n",
       "      <td>Requisition Number:\\n205033\\n\\nPosition Title:...</td>\n",
       "      <td></td>\n",
       "      <td>18/07/2019</td>\n",
       "      <td></td>\n",
       "      <td>https://www.indeed.fr/pagead/clk?mo=r&amp;ad=-6NYl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5076</th>\n",
       "      <td>Groupe Pierre &amp; Vacances - Center Parcs</td>\n",
       "      <td>Data Business Analyst</td>\n",
       "      <td>Paris (75)</td>\n",
       "      <td>Nous recherchons un business data analyst pass...</td>\n",
       "      <td>CDI</td>\n",
       "      <td>18/07/2019</td>\n",
       "      <td></td>\n",
       "      <td>https://www.indeed.fr/pagead/clk?mo=r&amp;ad=-6NYl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5077</th>\n",
       "      <td>Orange Business Services</td>\n",
       "      <td>D√©veloppeur Big Data SPLUNK ELK</td>\n",
       "      <td>38330 Montbonnot-Saint-Martin</td>\n",
       "      <td>votre r√¥le\\nInt√©gr√©(e) au d√©partement Data, vo...</td>\n",
       "      <td>CDI</td>\n",
       "      <td>07/08/2019</td>\n",
       "      <td></td>\n",
       "      <td>https://www.indeed.fr/pagead/clk?mo=r&amp;ad=-6NYl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      company  \\\n",
       "5073             BNP Paribas Personal Finance   \n",
       "5074                                 Teradata   \n",
       "5075                                 Teradata   \n",
       "5076  Groupe Pierre & Vacances - Center Parcs   \n",
       "5077                 Orange Business Services   \n",
       "\n",
       "                            job_title                       location  \\\n",
       "5073   RESPONSABLE DATA SCIENCE ET AI          Levallois-Perret (92)   \n",
       "5074                   Data Architect                   92160 Antony   \n",
       "5075                   Data Developer                   92160 Antony   \n",
       "5076            Data Business Analyst                     Paris (75)   \n",
       "5077  D√©veloppeur Big Data SPLUNK ELK  38330 Montbonnot-Saint-Martin   \n",
       "\n",
       "                                              pos_descr pos_type    pub_date  \\\n",
       "5073  RESPONSABLE DATA SCIENCE ET AI - H/F (NUM√âRO D...      CDI  02/08/2019   \n",
       "5074  Requisition Number:\\n205157\\n\\nPosition Title:...           23/07/2019   \n",
       "5075  Requisition Number:\\n205033\\n\\nPosition Title:...           18/07/2019   \n",
       "5076  Nous recherchons un business data analyst pass...      CDI  18/07/2019   \n",
       "5077  votre r√¥le\\nInt√©gr√©(e) au d√©partement Data, vo...      CDI  07/08/2019   \n",
       "\n",
       "     salary                                                url  \n",
       "5073         https://www.indeed.fr/pagead/clk?mo=r&ad=-6NYl...  \n",
       "5074         https://www.indeed.fr/pagead/clk?mo=r&ad=-6NYl...  \n",
       "5075         https://www.indeed.fr/pagead/clk?mo=r&ad=-6NYl...  \n",
       "5076         https://www.indeed.fr/pagead/clk?mo=r&ad=-6NYl...  \n",
       "5077         https://www.indeed.fr/pagead/clk?mo=r&ad=-6NYl...  "
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(df_indeed_full.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The web scraping job is complete !**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>II) Using Bokeh to build a custom aggregated job board</h2>\n",
    "<a id=\"2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before jumping directly into Bokeh, it is necessary to perform some text mining on the raw data to identify and isolate useful features. These sub-chapter might be a bit redundant with the chapter 3 of the notebook and will be reorganised later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1) Preprocessing the raw data</h3>\n",
    "<a id=\"2.1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>a) Data cleaning</h4>\n",
    "<a id=\"2.1.a\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's import the job databases for each job boards and proceed to some data cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data - apec: 17 job offers\n",
      "Raw data - indeed: 94 job offers\n"
     ]
    }
   ],
   "source": [
    "DB_UNION_SAVING_PATH = os.path.join(DB_SAVING_DIR, 'union_database.xlsx') #union of individual databases\n",
    "\n",
    "dateparse = lambda x: pd.datetime.strptime(x, '%d/%m/%Y') #to parse date format when importing the dataframe\n",
    "df={} # initiate the dict for the job database\n",
    "update = os.path.isfile(DB_UNION_SAVING_PATH)\n",
    "if update:\n",
    "    file_name_pat='\\*_new.xlsx' #only *_new.xlsx files (preprocessing task has been previously executed on an old db)\n",
    "else:\n",
    "    file_name_pat='\\*_db.xlsx' #only *_db.xlsx files (preprocessing task has never been executed before)\n",
    "    \n",
    "for filepath in glob.glob(DB_SAVING_DIR + file_name_pat):\n",
    "    name_origin = re.search(r'\\w+(?=_(db|new)\\.xlsx)', filepath).group()\n",
    "    df[name_origin] = pd.read_excel(filepath, parse_dates=['pub_date'], date_parser=dateparse)\n",
    "    df[name_origin]['origin'] = name_origin\n",
    "    print(\"Raw data - {}: {} job offers\".format(name_origin, len(df[name_origin])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After filtering - apec: 6 job offers\n",
      "After filtering - indeed: 89 job offers\n"
     ]
    }
   ],
   "source": [
    "mask={} #initiate the dict for boolean mask filtering\n",
    "mask['apec'] = df['apec']['job_title'].map(lambda x: re.match(r'.*\\b(data|donn√©es?|bi|business intelligence)\\b.*',\n",
    "                                                              str(x), flags=re.IGNORECASE)).notnull()\n",
    "mask['indeed'] = df['indeed']['pos_type'].map(lambda x: re.match(r'.*\\b(stage|alternance|apprentissage)\\b.*',\n",
    "                                                              str(x), flags=re.IGNORECASE)).isnull()\n",
    "for key in df.keys():\n",
    "    df[key] = df[key][mask[key]]\n",
    "    print(\"After filtering - {}: {} job offers\".format(key, len(df[key])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Job description for the APEC job board is separated into several column. To have an harmonised and centralised aggregated datebase, it is necessary to reunite these features into a single one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['apec']['pos_descr'] = df['apec']['comp_info'] + '\\n' + df['apec']['pos_descr'] + '\\n' + df['apec']['profil']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>b) Feature engineering</h4>\n",
    "<a id=\"2.1.b\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>**City and Department**</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The location attribute of each dataframe can be transformed into two seperated attributes : City and Department. The regex pattern varies accros job board."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "apec_loc_regex_pat = r'\\s-\\s(?=\\d{2,})'\n",
    "indeed_loc_regex_pat = r'\\s\\(|\\)'\n",
    "\n",
    "def get_city_and_dept_apec(location):\n",
    "    \"\"\"Return city and departement from location attribute for the APEC database\"\"\"\n",
    "    regex_pat = apec_loc_regex_pat\n",
    "    try:\n",
    "        match_split = re.split(regex_pat, location)\n",
    "        city, dept = match_split[0], match_split[1]\n",
    "    except:\n",
    "        city, dept = np.NaN, np.NaN\n",
    "    return city, dept\n",
    "\n",
    "def get_city_and_dept_indeed(location):\n",
    "    \"\"\"Return city and departement from location attribute for the APEC database\"\"\"\n",
    "    regex_pat = indeed_loc_regex_pat\n",
    "    try:\n",
    "        match_split = re.split(regex_pat, location)\n",
    "        if len(match_split[0]) != len(location):\n",
    "            city, dept = match_split[0], match_split[1]\n",
    "        else:\n",
    "            unc_pat = r'(?<=\\d{5})\\s' #uncommon (old?) pattern that could be encountered\n",
    "            match_split = re.split(unc_pat, str(location))\n",
    "            dept, city = str(match_split[0])[:2], match_split[1]\n",
    "    except:\n",
    "        city, dept = np.NaN, np.NaN\n",
    "    return city, dept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['apec']['city'], df['apec']['dept'] = zip(*df['apec']['location'].map(get_city_and_dept_apec))\n",
    "df['indeed']['city'], df['indeed']['dept'] = zip(*df['indeed']['location'].map(get_city_and_dept_indeed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that everything is fine for the city and department features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paris 08    2\n",
      "Talence     1\n",
      "Lyon 07     1\n",
      "Paris 01    1\n",
      "Paris 02    1\n",
      "Name: city, dtype: int64\n",
      "Paris                 36\n",
      "Lyon                   4\n",
      "Antony                 4\n",
      "Saint-Paul-l√®s-Dax     4\n",
      "Nantes                 3\n",
      "Name: city, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df['apec']['city'].value_counts()[:5]) \n",
    "print(df['indeed']['city'].value_counts()[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75    4\n",
      "33    1\n",
      "69    1\n",
      "Name: dept, dtype: int64\n",
      "75    38\n",
      "92    13\n",
      "69     7\n",
      "40     4\n",
      "44     4\n",
      "Name: dept, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df['apec']['dept'].value_counts()[:5]) \n",
    "print(df['indeed']['dept'].value_counts()[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now, the different job board database can be aggregated into a single one : df_union !** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_union = pd.concat([df[key] for key in df.keys()], ignore_index=True, sort=False)\n",
    "len(df_union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>act_area</th>\n",
       "      <th>act_date</th>\n",
       "      <th>btrip_area</th>\n",
       "      <th>comp_info</th>\n",
       "      <th>company</th>\n",
       "      <th>empl_date</th>\n",
       "      <th>job_title</th>\n",
       "      <th>location</th>\n",
       "      <th>nb_pos</th>\n",
       "      <th>pos_descr</th>\n",
       "      <th>...</th>\n",
       "      <th>recruit_proc</th>\n",
       "      <th>recruit_resp</th>\n",
       "      <th>ref_comp</th>\n",
       "      <th>ref_site</th>\n",
       "      <th>req_exp</th>\n",
       "      <th>salary</th>\n",
       "      <th>url</th>\n",
       "      <th>origin</th>\n",
       "      <th>city</th>\n",
       "      <th>dept</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Autres organisations fonctionnant par adh√©sio...</td>\n",
       "      <td>16/08/2019</td>\n",
       "      <td>Pas de d√©placement</td>\n",
       "      <td>Entreprise\\nBIOASTER est l'unique Institut de ...</td>\n",
       "      <td>BIOASTER</td>\n",
       "      <td>D√®s que possible</td>\n",
       "      <td>R&amp;D DATA MANAGER / DATA ENGINEER</td>\n",
       "      <td>Lyon 07 - 69</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Entreprise\\nBIOASTER est l'unique Institut de ...</td>\n",
       "      <td>...</td>\n",
       "      <td>Processus de recrutement\\nPersonne en charge d...</td>\n",
       "      <td>Accompagner quotidiennement les scientifiques ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>164482537W</td>\n",
       "      <td>Minimum 2 ans</td>\n",
       "      <td>A n√©gocier</td>\n",
       "      <td>https://cadres.apec.fr/offres-emploi-cadres/0_...</td>\n",
       "      <td>apec</td>\n",
       "      <td>Lyon 07</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Formation continue d'adultes</td>\n",
       "      <td>16/08/2019</td>\n",
       "      <td>Pas de d√©placement</td>\n",
       "      <td>Entreprise\\nNotre Client est une ONG (Organisa...</td>\n",
       "      <td>MCP CONSEIL</td>\n",
       "      <td>D√®s que possible</td>\n",
       "      <td>Marketing Data Analyst</td>\n",
       "      <td>Paris 01 - 75</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Entreprise\\nNotre Client est une ONG (Organisa...</td>\n",
       "      <td>...</td>\n",
       "      <td>Processus de recrutement\\nPersonne en charge d...</td>\n",
       "      <td>Charl√®ne Ureta - Dirigeant de la soci√©t√©</td>\n",
       "      <td>NaN</td>\n",
       "      <td>164482266W</td>\n",
       "      <td>Minimum 3 ans</td>\n",
       "      <td>36 - 38 k‚Ç¨ brut annuel</td>\n",
       "      <td>https://cadres.apec.fr/offres-emploi-cadres/0_...</td>\n",
       "      <td>apec</td>\n",
       "      <td>Paris 01</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Conseil en syst√®mes et logiciels informatiques</td>\n",
       "      <td>16/08/2019</td>\n",
       "      <td>R√©gionale</td>\n",
       "      <td>Entreprise\\nALTIMA, CR√âATEUR D'EXP√âRIENCE(S) E...</td>\n",
       "      <td>Altima</td>\n",
       "      <td>D√®s que possible</td>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Paris 02 - 75</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Entreprise\\nALTIMA, CR√âATEUR D'EXP√âRIENCE(S) E...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>talentplug</td>\n",
       "      <td>164481971W</td>\n",
       "      <td>Tous niveaux d'exp√©rience accept√©s</td>\n",
       "      <td>30 - 45 k‚Ç¨ brut annuel</td>\n",
       "      <td>https://cadres.apec.fr/offres-emploi-cadres/0_...</td>\n",
       "      <td>apec</td>\n",
       "      <td>Paris 02</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Formation continue d'adultes</td>\n",
       "      <td>14/08/2019</td>\n",
       "      <td>Pas de d√©placement</td>\n",
       "      <td>Entreprise\\nTu souhaites devenir¬†d√©veloppeur s...</td>\n",
       "      <td>LA PISCINE BORDEAUX</td>\n",
       "      <td>D√®s que possible</td>\n",
       "      <td>Forme toi et deviens D√©veloppeur sp√©cialit√© DA...</td>\n",
       "      <td>Talence - 33</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Entreprise\\nTu souhaites devenir¬†d√©veloppeur s...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>164481351W</td>\n",
       "      <td>Tous niveaux d'exp√©rience accept√©s</td>\n",
       "      <td>A n√©gocier</td>\n",
       "      <td>https://cadres.apec.fr/offres-emploi-cadres/0_...</td>\n",
       "      <td>apec</td>\n",
       "      <td>Talence</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Conseil en syst√®mes et logiciels informatiques</td>\n",
       "      <td>14/08/2019</td>\n",
       "      <td>Pas de d√©placement</td>\n",
       "      <td>Entreprise\\nQUI SOMMES-NOUS¬†?\\nADONIS est une ...</td>\n",
       "      <td>ADONIS</td>\n",
       "      <td>D√®s que possible</td>\n",
       "      <td>Lead Technique Hadoop - Big Data</td>\n",
       "      <td>Paris 08 - 75</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Entreprise\\nQUI SOMMES-NOUS¬†?\\nADONIS est une ...</td>\n",
       "      <td>...</td>\n",
       "      <td>Processus de recrutement\\nProcess RH ADONIS :\\...</td>\n",
       "      <td>AGNES LACOMBE - DRH</td>\n",
       "      <td>LT_HADOOP_BIGDATA_140819</td>\n",
       "      <td>164480795W</td>\n",
       "      <td>Minimum 2 ans</td>\n",
       "      <td>40 - 50 k‚Ç¨ brut annuel</td>\n",
       "      <td>https://cadres.apec.fr/offres-emploi-cadres/0_...</td>\n",
       "      <td>apec</td>\n",
       "      <td>Paris 08</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            act_area    act_date  \\\n",
       "0   Autres organisations fonctionnant par adh√©sio...  16/08/2019   \n",
       "1                       Formation continue d'adultes  16/08/2019   \n",
       "2     Conseil en syst√®mes et logiciels informatiques  16/08/2019   \n",
       "3                       Formation continue d'adultes  14/08/2019   \n",
       "4     Conseil en syst√®mes et logiciels informatiques  14/08/2019   \n",
       "\n",
       "            btrip_area                                          comp_info  \\\n",
       "0   Pas de d√©placement  Entreprise\\nBIOASTER est l'unique Institut de ...   \n",
       "1   Pas de d√©placement  Entreprise\\nNotre Client est une ONG (Organisa...   \n",
       "2            R√©gionale  Entreprise\\nALTIMA, CR√âATEUR D'EXP√âRIENCE(S) E...   \n",
       "3   Pas de d√©placement  Entreprise\\nTu souhaites devenir¬†d√©veloppeur s...   \n",
       "4   Pas de d√©placement  Entreprise\\nQUI SOMMES-NOUS¬†?\\nADONIS est une ...   \n",
       "\n",
       "               company          empl_date  \\\n",
       "0             BIOASTER   D√®s que possible   \n",
       "1          MCP CONSEIL   D√®s que possible   \n",
       "2               Altima   D√®s que possible   \n",
       "3  LA PISCINE BORDEAUX   D√®s que possible   \n",
       "4               ADONIS   D√®s que possible   \n",
       "\n",
       "                                           job_title       location  nb_pos  \\\n",
       "0                   R&D DATA MANAGER / DATA ENGINEER   Lyon 07 - 69     1.0   \n",
       "1                             Marketing Data Analyst  Paris 01 - 75     1.0   \n",
       "2                                      Data Engineer  Paris 02 - 75     2.0   \n",
       "3  Forme toi et deviens D√©veloppeur sp√©cialit√© DA...   Talence - 33     1.0   \n",
       "4                   Lead Technique Hadoop - Big Data  Paris 08 - 75     1.0   \n",
       "\n",
       "                                           pos_descr  ...  \\\n",
       "0  Entreprise\\nBIOASTER est l'unique Institut de ...  ...   \n",
       "1  Entreprise\\nNotre Client est une ONG (Organisa...  ...   \n",
       "2  Entreprise\\nALTIMA, CR√âATEUR D'EXP√âRIENCE(S) E...  ...   \n",
       "3  Entreprise\\nTu souhaites devenir¬†d√©veloppeur s...  ...   \n",
       "4  Entreprise\\nQUI SOMMES-NOUS¬†?\\nADONIS est une ...  ...   \n",
       "\n",
       "                                        recruit_proc  \\\n",
       "0  Processus de recrutement\\nPersonne en charge d...   \n",
       "1  Processus de recrutement\\nPersonne en charge d...   \n",
       "2                                                NaN   \n",
       "3                                                NaN   \n",
       "4  Processus de recrutement\\nProcess RH ADONIS :\\...   \n",
       "\n",
       "                                        recruit_resp  \\\n",
       "0  Accompagner quotidiennement les scientifiques ...   \n",
       "1           Charl√®ne Ureta - Dirigeant de la soci√©t√©   \n",
       "2                                                NaN   \n",
       "3                                                NaN   \n",
       "4                                AGNES LACOMBE - DRH   \n",
       "\n",
       "                   ref_comp    ref_site                              req_exp  \\\n",
       "0                       NaN  164482537W                        Minimum 2 ans   \n",
       "1                       NaN  164482266W                        Minimum 3 ans   \n",
       "2                talentplug  164481971W   Tous niveaux d'exp√©rience accept√©s   \n",
       "3                       NaN  164481351W   Tous niveaux d'exp√©rience accept√©s   \n",
       "4  LT_HADOOP_BIGDATA_140819  164480795W                        Minimum 2 ans   \n",
       "\n",
       "                    salary                                                url  \\\n",
       "0               A n√©gocier  https://cadres.apec.fr/offres-emploi-cadres/0_...   \n",
       "1   36 - 38 k‚Ç¨ brut annuel  https://cadres.apec.fr/offres-emploi-cadres/0_...   \n",
       "2   30 - 45 k‚Ç¨ brut annuel  https://cadres.apec.fr/offres-emploi-cadres/0_...   \n",
       "3               A n√©gocier  https://cadres.apec.fr/offres-emploi-cadres/0_...   \n",
       "4   40 - 50 k‚Ç¨ brut annuel  https://cadres.apec.fr/offres-emploi-cadres/0_...   \n",
       "\n",
       "  origin      city dept  \n",
       "0   apec   Lyon 07   69  \n",
       "1   apec  Paris 01   75  \n",
       "2   apec  Paris 02   75  \n",
       "3   apec   Talence   33  \n",
       "4   apec  Paris 08   75  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_union.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, there are still a few preprocissing steps to go through for df_union. Let's continue text mining and feature engineering to fetch :\n",
    "* France region; \n",
    "* Average salary (salary is often provided as an interval);\n",
    "* Category of data-related position (e.g.: data engineer, analyst, scientist ...).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>**Region**</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To add the Region feature, we will use a left join with a correspondence table for department and region in France. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEPT_PATH = './dpt_reg_fr.csv'\n",
    "DEPT_DL_URL = 'https://raw.githubusercontent.com/GuillaumeHarel/portfolio-projects/master/'\\\n",
    "              '%231_Webscraping_%26_Bokeh/Auxiliary_files/Dept_Region_France.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "urllib.request.urlretrieve(DEPT_DL_URL, DEPT_PATH)\n",
    "df_dept_reg = pd.read_csv(DEPT_PATH, dtype={'regionCode': 'Int32'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>departmentCode</th>\n",
       "      <th>departmentName</th>\n",
       "      <th>regionCode</th>\n",
       "      <th>regionName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01</td>\n",
       "      <td>Ain</td>\n",
       "      <td>84</td>\n",
       "      <td>Auvergne-Rh√¥ne-Alpes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>02</td>\n",
       "      <td>Aisne</td>\n",
       "      <td>32</td>\n",
       "      <td>Hauts-de-France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>03</td>\n",
       "      <td>Allier</td>\n",
       "      <td>84</td>\n",
       "      <td>Auvergne-Rh√¥ne-Alpes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>04</td>\n",
       "      <td>Alpes-de-Haute-Provence</td>\n",
       "      <td>93</td>\n",
       "      <td>Provence-Alpes-C√¥te d'Azur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>05</td>\n",
       "      <td>Hautes-Alpes</td>\n",
       "      <td>93</td>\n",
       "      <td>Provence-Alpes-C√¥te d'Azur</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  departmentCode           departmentName  regionCode  \\\n",
       "0             01                      Ain          84   \n",
       "1             02                    Aisne          32   \n",
       "2             03                   Allier          84   \n",
       "3             04  Alpes-de-Haute-Provence          93   \n",
       "4             05             Hautes-Alpes          93   \n",
       "\n",
       "                   regionName  \n",
       "0        Auvergne-Rh√¥ne-Alpes  \n",
       "1             Hauts-de-France  \n",
       "2        Auvergne-Rh√¥ne-Alpes  \n",
       "3  Provence-Alpes-C√¥te d'Azur  \n",
       "4  Provence-Alpes-C√¥te d'Azur  "
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(df_dept_reg.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_union = pd.merge(df_union, df_dept_reg[['departmentCode', 'departmentName', 'regionName']],\n",
    "                    how='left', left_on='dept', right_on='departmentCode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['act_area', 'act_date', 'btrip_area', 'comp_info', 'company',\n",
       "       'empl_date', 'job_title', 'location', 'nb_pos', 'pos_descr', 'pos_stt',\n",
       "       'pos_type', 'profil', 'pub_date', 'recruit_proc', 'recruit_resp',\n",
       "       'ref_comp', 'ref_site', 'req_exp', 'salary', 'url', 'origin', 'city',\n",
       "       'dept', 'departmentCode', 'departmentName', 'regionName'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_union.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_union.drop('departmentCode', 1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>**Average salary**</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the average salary feature it is necessary to deal with the multiple possible salary string format coming from the Indeed job board. String format expression can vary on time period (e.g. hour, week, year ...), on digit format (e.g. 32 k‚Ç¨ VS 32 000 ‚Ç¨) or even on job contract types (e.g. Employed VS Freelancer). \n",
    "<br>\n",
    "I did my best to automate the process with regards to this high variability.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_salary(salary):\n",
    "    \"\"\"Return the average salary from the text salary attribute (mostly corresponding to an interval)\"\"\"\n",
    "    match = re.findall(r\"(\\d+)\", re.sub(r\"(?<=\\d)\\s+(?=\\d)\", \"\", str(salary))) #re.sub to remove empty space in digits\n",
    "    if match != None:\n",
    "        try:\n",
    "            sal_freq = ['an', 'mois', 'semaine', 'jour', 'heure']\n",
    "            sal_factor = [1, 12, 52.14, 228/1.5, 35 * 52.14]\n",
    "            freq_res = re.search(r'|'.join(sal_freq), str(salary)).group()\n",
    "            sal_avg = np.array(match).astype(np.float).mean() * sal_factor[sal_freq.index(freq_res)]\n",
    "            if sal_avg > 1000:\n",
    "                sal_avg /= 1000\n",
    "            assert sal_avg > 20 and sal_avg < 200\n",
    "            return sal_avg\n",
    "        except:\n",
    "            return np.NaN\n",
    "    else:\n",
    "        return np.NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_union['avg_sal'] = df_union['salary'].map(get_salary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>**Job categories**</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eventually, let's try to create the data-related position category feature from the job title one.\n",
    "<br>\n",
    "<br>\n",
    "I made arbitrary choice to categorize this new attribute. I am mostly interested by job offers in data science and analysis. As a result, I gave less importance to details for other data-related job categories. In particular, it can be noticed that the Big Data category might aggregate several type of trades that are inherently different. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_cat_pat = [['(scientist', 'science', 'learning', 'miner', 'mining)'],\n",
    "               ['(analyst', r'\\bbi\\b', 'marketing', 'business)'],\n",
    "               [r'(\\bbig\\b', 'data engineer', r'architecte?', r'd[e,√©]v', 'data ing√©nieur', 'ing√©nieur data)'],\n",
    "               [r'(projec?t)'],\n",
    "               ['(data manager', 'manager data', 'gestionnaire', 'steward', 'governance)']]\n",
    "job_cat_pat_flat = [item for sublist in job_cat_pat for item in sublist]\n",
    "\n",
    "job_cat_labels = [\"Data scientist\",\n",
    "                  \"Data analyst & BI\",\n",
    "                  \"Big Data (engineer, dev, archi)\",\n",
    "                  \"IT Project Manager (data related)\",\n",
    "                  \"Data Manager/Officer\",\n",
    "                  \"Unclassified\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cat_dummies(job_title):\n",
    "    \"\"\"From job_title string return a list of dummies corresponding to data-related job categories\"\"\"\n",
    "    dummies = [0] * len(job_cat_pat)\n",
    "    job_res = re.finditer('|'.join(job_cat_pat_flat), job_title, flags=re.IGNORECASE)\n",
    "    for match in job_res:\n",
    "        ind = match.groups().index(match.group())\n",
    "        dummies[ind] = 1\n",
    "    if not any(dummies): #no match, \"Others\" category\n",
    "        dummies.append(1)\n",
    "    else:\n",
    "        dummies.append(0)\n",
    "    return dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_union['job_cat'] = df_union['job_title'].map(get_cat_dummies)\n",
    "df_union[job_cat_labels] = pd.DataFrame(df_union['job_cat'].values.tolist(), index=df_union.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data scientist</th>\n",
       "      <th>Data analyst &amp; BI</th>\n",
       "      <th>Big Data (engineer, dev, archi)</th>\n",
       "      <th>IT Project Manager (data related)</th>\n",
       "      <th>Data Manager/Officer</th>\n",
       "      <th>Unclassified</th>\n",
       "      <th>job_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>R&amp;D DATA MANAGER / DATA ENGINEER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Marketing Data Analyst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Data Engineer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Forme toi et deviens D√©veloppeur sp√©cialit√© DA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Lead Technique Hadoop - Big Data</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Data scientist  Data analyst & BI  Big Data (engineer, dev, archi)  \\\n",
       "0               0                  0                                1   \n",
       "1               0                  1                                0   \n",
       "2               0                  0                                1   \n",
       "3               0                  1                                1   \n",
       "4               0                  0                                1   \n",
       "\n",
       "   IT Project Manager (data related)  Data Manager/Officer  Unclassified  \\\n",
       "0                                  0                     1             0   \n",
       "1                                  0                     0             0   \n",
       "2                                  0                     0             0   \n",
       "3                                  0                     0             0   \n",
       "4                                  0                     0             0   \n",
       "\n",
       "                                           job_title  \n",
       "0                   R&D DATA MANAGER / DATA ENGINEER  \n",
       "1                             Marketing Data Analyst  \n",
       "2                                      Data Engineer  \n",
       "3  Forme toi et deviens D√©veloppeur sp√©cialit√© DA...  \n",
       "4                   Lead Technique Hadoop - Big Data  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_union[job_cat_labels + ['job_title'] ].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In prevision of the Bokeh App, there are two last attributes to add to df_union :\n",
    "* the dataframe index as a column to display in Bokeh a unique ID key number for every offer;\n",
    "* the text similiraty between a resume/CV and the offer (similarity will be computed later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_union['index_0'] = df_union.index\n",
    "df_union['sim'] = np.NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>**Text similarity**</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the text similarity metric between a resume and a job offer I decided to work with the Dice similarity.\n",
    "\n",
    "The Dice similarity metric measure the similarity between two documents d1 and d2 by relying on the number of common term between d1 and d2.\n",
    "<br>\n",
    "<br>\n",
    "**<center>SimDice(d1, d2) = 2 Nc / (N1 + N2)</center>**\n",
    "<br>\n",
    "*Where Nc stands for the number of common words between d1 and d2 and N1 and N2 respectively correspond to the number of term in d1 and d2.*\n",
    "<br>\n",
    "<br>\n",
    "The length of job offer may vary a lot in our job offer corpus (see below), and so its number of words, which would have a strong impact on the Dice metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "offer_word_number = df_union['pos_descr'].map(lambda x: len(str(x).split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x20a34d0dc18>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(offer_word_number, kde=False, norm_hist=False, axlabel=\"Word count number\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a consequence, we need to define a specific word vocabulary in order to only retain the relevant terms related to data, education, work experience and skill. \n",
    "<br>\n",
    "**Then the Dice metric will favorize job offers which present high number of word in common with the resume while penalyzing offers that are either too indefinite or too demanding in regards with the candidate resume.**\n",
    "<br>\n",
    "<br> \n",
    "For further information on text similarity measurement, a nice synthesis is available here [[8]](#ref8) (French only)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I defined a first pecific vocabulary adapted to my resume and my job searching.\n",
    "<br> It should be improved in the future, especially in order to cover most of the hazardous spelling for technical skill or tool (e.g. \"Scikit-learn\" could also probably be found as \"Sklearn\" or \"Scikit learn\" in some job offers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#programming languages usefull for data\n",
    "prog_lang_l = ['Python', 'R', 'C', 'C#', 'C++', 'SAS', 'SPSS', 'VBA', 'SQL', 'NoSQL', 'Fortran', 'Matlab',\n",
    "               'Perl', 'Julia', 'Ruby', 'Scala', 'Java', 'JavaScript', 'PHP', 'HTLM', 'CSS']\n",
    "\n",
    "#other computing skills : IDE, OS, Big data Framework, Cloud Computing, Software BI & Data viz, Database, GIS/SIG, collab\n",
    "other_comput_l = ['PyCharm', 'Jupyter', 'RStudio', 'Visual Studio',\n",
    "                  'Windows', 'Linux', 'Mac', 'iOS', 'Android',\n",
    "                  'Hadoop', 'MapReduce', 'Hive', 'Kafka', 'Spark', 'Pig',\n",
    "                  'Azure', 'AWS', 'Google Cloud',\n",
    "                  'Tableau', 'Power BI', 'Qlikview',\n",
    "                  'HBase', 'Cassandra', 'MongoDB', 'Access', 'PostgreSQL', 'Oracle',\n",
    "                  'SIG', 'MapInfo', 'ArcGIS', 'QGIS',\n",
    "                  'Git', 'GitHub',\n",
    "                  'Microsoft Office']\n",
    "\n",
    "#Python and R main libraries for data (oriented in favor of python and my profile)\n",
    "data_library_l = ['Numpy', 'Scipy', 'Pandas', ' Dyplr', 'Tidyr',\n",
    "                  'Matplotlib', 'Seaborn', 'Plotly', 'Bokeh', 'ggplot2', 'Shiny',\n",
    "                  'Theano', 'TensorFlow', 'Keras', 'Pytorch', 'Scikit-learn', 'Statsmodels',\n",
    "                  'NLTK', 'Beautiful Soup', 'Selenium',\n",
    "                  'OpenCV']\n",
    "\n",
    "data_skills_l = ['Statistiques', 'Data mining', 'Data wrangling', 'Data visualization',\n",
    "                 'Machine Learning', 'Deep Learning', 'Clustering', 'Computer vision', 'NLP']\n",
    "\n",
    "education_l = ['Master', 'Ing√©nieur', 'Docteur',\n",
    "               'Math√©matiques', 'Informatiques', 'Physique', 'Chimie', 'Biologie',\n",
    "               '√©conomie', 'Marketing', 'Finance', 'Actuariat']\n",
    "\n",
    "language_l = ['Fran√ßais', 'Anglais', 'Allemand', 'Espagnol', 'Arabe', 'Chinois', 'Japonais']\n",
    "\n",
    "gal_prof_skill_l = ['Gestion de projets', \"Gestion d'affaires\", 'Management',\n",
    "                    'R√©daction', 'Pr√©sentation', 'Communication', 'Relationnel',\n",
    "                    'Agile', 'Scrum'] # Add soft skills later ?\n",
    "\n",
    "#Oriented according to my resume. Some are redundant with education -> not added\n",
    "data_sectors_l = ['Ing√©nierie', 'Sant√©', '√©nergie', 'D√©chets', 'Transports', 'Industrie',#environnement tag too confusing\n",
    "                  'Pharmaceutique', 'Biotechnologies',\n",
    "                  'Smart city', 'IoT',\n",
    "                  'Assurances', 'Banques',\n",
    "                  'Publicit√©', 'Web', 'e-commerce',\n",
    "                  'Grande distribution']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute all these sub-vocabularies into a general skill list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_skill_l = prog_lang_l + other_comput_l + data_library_l + data_skills_l + education_l + language_l + \\\n",
    "                  gal_prof_skill_l + data_sectors_l\n",
    "overall_skill_l = [skill.lower() for skill in overall_skill_l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "129"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(overall_skill_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used the CountVectorizer class provided by Scikit-learn with a special token pattern to handle compound nouns more easily (e.g. I do not want that \"scikit-learn\" stands for 2 words and received a double weight in comparison to Pandas for example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_pattern = '\\w+\\-?\\w*'\n",
    "word_counter = CountVectorizer(strip_accents=None, lowercase=True,\n",
    "                               analyzer='word', token_pattern=tok_pattern, ngram_range=(1, 2),\n",
    "                               vocabulary=overall_skill_l, binary=True)\n",
    "some_offer_corpus = [\n",
    "    'Offre en √©conomie vous devrez maitriser Power BI, les outils de Data visualization et parler Allemand',\n",
    "    'Vous etes un chinois fran√ßais en data science et machine learning (scikit-learn, pandas)'\n",
    "]\n",
    "X = word_counter.fit_transform(some_offer_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word counter seems to work fine. We can use it to transform in advance all our job offers into a bag of specific words.\n",
    "<br>\n",
    "For the candidate resume, it will be provided later by the Bokeh user via an input file widget in order to compute the Dice similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(95, 129)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_sim = word_counter.fit_transform(df_union['pos_descr'])\n",
    "X_sim.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x20a36524f28>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEKCAYAAAACS67iAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAFO5JREFUeJzt3X2QZXV95/H3R8BFQESW1qgwGWIhPsSEaEcMKIpPiy4RKUkhaBailUlSiQEj6+K6iaxbiTE+YKpcNSPisBHQaCBR1wcQQYzgKMPTDI6i4VkJMxQuCCYKznf/OKf1punHe+/Q3b95v6q6+txzzz3ne849/bm/+7v3/DpVhSRp5XvYUhcgSRoPA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUiJ0fyo3ts88+tXr16odyk5K04m3YsOHOqpqYb7mHNNBXr17NFVdc8VBuUpJWvCQ3L2Q5u1wkqREGuiQ1wkCXpEYY6JLUCANdkhoxb6AnOTPJliSbZrjvlCSVZJ/tU54kaaEW0kJfBxwxfWaS/YAXA7eMuSZJ0hDmDfSquhS4a4a7TgfeBPg/7CRpGRiqDz3Jy4HvVdU1Y65HkjSkRV8pmmQ34C3ASxa4/BpgDcCqVasWuzk9BM5ZP3yv2fEH+5xKy8UwLfQnAvsD1yS5CdgXuDLJL8y0cFWtrarJqpqcmJh3KAJJ0pAW3UKvqo3AY6Zu96E+WVV3jrEuSdIiLeRri+cClwMHJrktyeu2f1mSpMWat4VeVcfNc//qsVUjSRqaV4pKUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNWLeQE9yZpItSTYNzHtnkm8luTbJ+Un22r5lSpLms5AW+jrgiGnzLgR+uap+BbgeePOY65IkLdK8gV5VlwJ3TZt3QVU90N/8GrDvdqhNkrQI4+hDfy3wudnuTLImyRVJrti6desYNidJmslIgZ7kLcADwNmzLVNVa6tqsqomJyYmRtmcJGkOOw/7wCQnAEcCL6yqGl9JkqRhDBXoSY4A/hvwvKr60XhLkiQNYyFfWzwXuBw4MMltSV4HvA94JHBhkquTfHA71ylJmse8LfSqOm6G2R/eDrVIkkbglaKS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWrEvIGe5MwkW5JsGpi3d5ILk3yn//3o7VumJGk+C2mhrwOOmDbvVOCiqjoAuKi/LUlaQvMGelVdCtw1bfZRwFn99FnAK8ZclyRpkYbtQ39sVd0O0P9+zPhKkiQNY+ftvYEka4A1AKtWrdremxurc9bfMvRjjz94Ze2rpJVv2Bb6HUkeB9D/3jLbglW1tqomq2pyYmJiyM1JkuYzbKB/Cjihnz4B+MfxlCNJGtZCvrZ4LnA5cGCS25K8DvhL4MVJvgO8uL8tSVpC8/ahV9Vxs9z1wjHXIkkagVeKSlIjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWrEdh8+V4szypC9knZsttAlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNWKkQE/yhiTXJdmU5Nwku46rMEnS4gwd6EmeAPwxMFlVvwzsBLxqXIVJkhZn1C6XnYFHJNkZ2A34/uglSZKGMXSgV9X3gHcBtwC3A3dX1QXTl0uyJskVSa7YunXr8JVKkuY0SpfLo4GjgP2BxwO7J3nN9OWqam1VTVbV5MTExPCVSpLmNEqXy4uAG6tqa1XdD5wHHDKesiRJizVKoN8CPDvJbkkCvBDYPJ6yJEmLNUof+nrgk8CVwMZ+XWvHVJckaZFG+hd0VfVW4K1jqkWSNAKvFJWkRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEaMdKWotBTOWX/LUI87/uBVY65EWl5soUtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1AgDXZIaYaBLUiMMdElqxEiBnmSvJJ9M8q0km5P8xrgKkyQtzqhjufw18PmqOibJw4HdxlCTJGkIQwd6kj2Bw4ATAarqJ8BPxlOWJGmxRuly+SVgK/CRJFclOSPJ7mOqS5K0SKmq4R6YTAJfAw6tqvVJ/hq4p6r+dNpya4A1AKtWrXrmzTffPGLJizfscKua37BD0q6058Shd7WUkmyoqsn5lhulhX4bcFtVre9vfxJ4xvSFqmptVU1W1eTExMQIm5MkzWXoQK+qfwFuTXJgP+uFwDfHUpUkadFG/ZbL64Gz+2+43AD8zuglSZKGMVKgV9XVwLz9OpKk7c8rRSWpEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1AgDXZIaMXKgJ9kpyVVJPjOOgiRJwxlHC/0kYPMY1iNJGsFIgZ5kX+A/A2eMpxxJ0rBGbaG/F3gTsG0MtUiSRrDzsA9MciSwpao2JHn+HMutAdYArFq1atjNaZk6Z/0tS11Cs4Y9tscfPPzf2VJsU+MzSgv9UODlSW4CPga8IMlHpy9UVWurarKqJicmJkbYnCRpLkMHelW9uar2rarVwKuAL1XVa8ZWmSRpUfweuiQ1Yug+9EFVdQlwyTjWJUkaji10SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1AgDXZIaYaBLUiMMdElqxFiuFJVa5yiEWglsoUtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0YOtCT7Jfk4iSbk1yX5KRxFiZJWpxRBud6AHhjVV2Z5JHAhiQXVtU3x1SbJGkRhm6hV9XtVXVlP/1DYDPwhHEVJklanLEMn5tkNfBrwPoZ7lsDrAFYtWr4oUSHHb5UWkqet3oojfyhaJI9gL8HTq6qe6bfX1Vrq2qyqiYnJiZG3ZwkaRYjBXqSXejC/OyqOm88JUmShjHKt1wCfBjYXFXvGV9JkqRhjNJCPxT4beAFSa7uf142prokSYs09IeiVfVPQMZYiyRpBF4pKkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjRjL8LmSdmxLMUzw8QcPPxx3q2yhS1IjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWrESIGe5Igk307y3SSnjqsoSdLiDR3oSXYC/jfwUuCpwHFJnjquwiRJizNKC/1ZwHer6oaq+gnwMeCo8ZQlSVqsUQL9CcCtA7dv6+dJkpbAKMPnZoZ59aCFkjXAmv7mvUm+PeT29gHuHPKxOwqP0dx2iOPz6tEevmKO0Yj7OaylOj6/uJCFRgn024D9Bm7vC3x/+kJVtRZYO8J2AEhyRVVNjrqelnmM5ubxmZ/HaG7L/fiM0uXyDeCAJPsneTjwKuBT4ylLkrRYQ7fQq+qBJH8EfAHYCTizqq4bW2WSpEUZ6V/QVdVngc+OqZb5jNxtswPwGM3N4zM/j9HclvXxSdWDPseUJK1AXvovSY1YEYHuEANzS3JTko1Jrk5yxVLXsxwkOTPJliSbBubtneTCJN/pfz96KWtcSrMcn9OSfK8/j65O8rKlrHEpJdkvycVJNie5LslJ/fxlfQ4t+0B3iIEFO7yqDlrOX6l6iK0Djpg271Tgoqo6ALiov72jWseDjw/A6f15dFD/GdmO6gHgjVX1FODZwB/2ubOsz6FlH+g4xICGUFWXAndNm30UcFY/fRbwioe0qGVkluOjXlXdXlVX9tM/BDbTXQm/rM+hlRDoDjEwvwIuSLKhvzJXM3tsVd0O3R8s8Jglrmc5+qMk1/ZdMsuqO2GpJFkN/BqwnmV+Dq2EQF/QEAM7uEOr6hl03VJ/mOSwpS5IK9IHgCcCBwG3A+9e2nKWXpI9gL8HTq6qe5a6nvmshEBf0BADO7Kq+n7/ewtwPl03lR7sjiSPA+h/b1niepaVqrqjqn5aVduAD7GDn0dJdqEL87Or6rx+9rI+h1ZCoDvEwByS7J7kkVPTwEuATXM/aof1KeCEfvoE4B+XsJZlZyqoekezA59HSQJ8GNhcVe8ZuGtZn0Mr4sKi/utT7+XnQwz8+RKXtGwk+SW6Vjl0V/6e4/GBJOcCz6cbHe8O4K3APwB/B6wCbgF+q6p2yA8GZzk+z6frbingJuD3pvqLdzRJngN8BdgIbOtn/3e6fvRlew6tiECXJM1vJXS5SJIWwECXpEYY6JLUCANdkhphoEtSIwz0ISQ5PcnJA7e/kOSMgdvvTvInI6z/tCSnjFrnENs9aClG2Ety75jXd25/+fobhnz885McMnB7XZJjxlfhUDWdnGS3paxhGEkuSbKoAePGfT7sSAz04VwGHAKQ5GF03+V92sD9hwBfXciK+tEkl4uDgBU1ZGqSnafd/gXgkKr6lao6fZh10H0f+5AZFl1KJwMrLtC3t3TMsZ4HYjhf5ed/8E+ju6Luh0keneQ/AE8BrupPtncm2dSPV34s/KwFeHGSc+guXCDJW/ox378IHDjTRpM8Nsn5Sa7pf6ZeVP6k38amqXcOSVZPG+v6lCSn9dOXJHlHkq8nuT7Jc/urcN8GHNuPhX3stG2fmOS8JJ/vx4L+q4H77h2YPibJun56XZIP9Pt6Q5Ln9YM+bZ5aZuBx705yZZKLkkz0857Yb29Dkq8kefLAet+T5GLgHdMO0wXAY/p9eG7/ruNrfYv9/KkBp/pj8BdJvgycNFDHauD3gTdMraO/67Akl/X7cczA8v81yTf69f/PWZ63I/p9uybJRf28f/curH/uVqe78vf/9stuSnJskj8GHg9c3O8zSY7rz6lNSd4xsJ57++d2Q5IvJnlWv683JHn5DLV9PAPvyvpj+8okuyb5SL+Nq5Ic3t+/U5J39fOvTfL6fv6f9cdhU5K1SQbHYHpNf+w2JXnWXPs/rbY9+vPhyn57R009R/059H7gSuBPk5w+8LjfTTJ4deeOo6r8GeKH7kq6VcDv0QXA/6Jr3R4KXNov80rgQrorXB9Ld2XZ4+hagPcB+/fLPZMu2HcD9gS+C5wywzY/TjdIEP06HzXw2N2BPYDr6EaGWw1sGnjsKcBp/fQlwLv76ZcBX+ynTwTeN8v+ngjc0G9zV+BmYL/+vnsHljsGWNdPr6Mb7jh0w47eAzydriGxATioX66AV/fTfzZVA9140wf00wcDXxpY72eAnWaoc/p+Xws8r59+G/DegWPw/ln29bTB499v7xN93U+lG84ZumEW1vb797C+psOmrWuCbrTQqed671m2samv/ZXAhwbmP2rgfNunn3483bk0QXd18JeAVwwcy5f20+fTvcDtAvwqcPUM+3o0cFY//fC+1kcAbwQ+0s9/cr+9XYE/oBvfZOdp+7P3wDr/FvjNgeP8oX76sKnnZrb9Hzyf+n3bs5/eh+7vIv1x2gY8u79vd+CfgV3625cBT1/qjFiKH1vow5tqpR8CXN7/TN2+rF/mOcC51Q14dAfwZeDX+/u+XlU39tPPBc6vqh9VN6LbbGPVvIBuRDz6dd7db+P8qrqvqu4FzuvXN5+pwYY20P2BLMRFVXV3Vf0b8E3gFxfwmE9X91e2EbijqjZWN/jTdQPb3Ub3YgXwUeA56Ua5OwT4RJKrgb+hezGc8omq+ulcG07yKGCvqvpyP+ssulCZ8vEHP2pW/1BV26rqm3QvztAF+kuAq+haik8GDpj2uGfTvcDfCFDzXya+EXhR38p+bv8cT/frwCVVtbWqHgDOHtivnwCfH1jXl6vq/n569Qzr+hzwgnTvLF/a1/qvdOfV3/Y1f4vuBfxJwIuAD/bbHdyfw5OsT7KR7jwd7II8t1/2UmDPJHvNcwymBPiLJNcCX6QbNnvq2N9cVV/r13sf3Yvakf27uF2qauMCt9GU6X2HWripfvSn07UubqVr1dwDnNkvM9PQv1Pum3Z72DEYZtvGA/z7LrVdp93/4/73T1n4efDjgenBxw3WPtt2tk17/LY5tlt0tf+/qjpolmWmH79hLGYdg7Vn4Pfbq+pv5nhcmPm5nfH5qarrkzyT7p3T25NcUFVvm2Gds7m/fwGFgWNeVdvy4M8KqKp/S3IJ8J+AY+nDd45tPGh/kuwKvB+YrKpb03XtDZ4H0/e/mP/8BHg13buQZ1bV/UluGlhu+nN3Bt1YK98CPjJL7c2zhT68rwJHAnf1reW7gL2A36BrrQNcStcnvVPfL3wY8PUZ1nUpcHSSR6QbOfE3Z9nmRXRveaf6MvfsH/uKJLulG23xaLpBhe6g60v+j33r68gF7NMPgUcuYLnp7kjylHQfTh09xOMfRtdVA3A88E/9O5Ubk/wW/OzDr19dzEr71u0PBvrBf5vuXdJ8FnocvgC8tn83QZInJJn+Dw8uB56XZP9+mb37+TcBz+jnPQOYuv/xwI+q6qPAu6aWmVbT+n6d+6T7UP24Be7XbD4G/A7dO7sv9PMupQtUkjyJrnvx23RdOL8/9eLQ789UyN7ZH4vp3wia+uzoOcDd/fMy4/5P8yhgSx/mhzPHO8KqWk83zPbx/PxFaYdjC314G+n69c6ZNm+Pqrqzv30+XcBfQ9cqeVNV/Uv/tvBnqurKJB8HrqZ7a/uVWbZ5ErA2yevoWsh/UFWXp/uAceqF4oyqugogydvo/vhvpGu5zOdi4NS+i+PtVbXQLolT6fqPb6V7t7LHAh835T7gaUk2AHfTBwBdoHwgyf+g6wf+GN2xXIwTgA+m+8rfDXTBNZ9PA5/sP4R7/WwLVdUFSZ4CXN5/Bngv8BoGxsiuqq3p/ovUef0L3hbgxXT90P+lP9bfAK7vH/J04J1JtgH307+A0/XVfy7J7VV1eJI30z1fAT5bVaMM43oB8H+AT1X3bx6ha3F/sO9CeQA4sap+nO7ruU8Crk1yP13/+PuSfIju/L+p359BP0hyGd3nQ6/t5822/4POBj6d7h+fX8385/Df0X0u84OF7nhrHG1RUhOSfIbun1xftNS1LBW7XCStaEn2SnI98K87cpiDLXRJaoYtdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktSI/w+caO0YVflZPgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "specific_offer_word_number = X_sim.sum(axis=1)\n",
    "sns.distplot(specific_offer_word_number, kde=False, norm_hist=False, bins=range(np.max(X_sim.sum(axis=1))), \n",
    "             axlabel=\"Word count number for the custom vocabulary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word counter looks fine and will be saved as well as the bag of words array for the job offers just a moment later in the all in one function. Let's first define the necessary directory and file paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "SKILL_WORD_COUNTER_DIR = './skill_word_counter'\n",
    "os.makedirs(SKILL_WORD_COUNTER_DIR, exist_ok=True)\n",
    "SKILL_WORD_COUNTER_PATH = os.path.join(SKILL_WORD_COUNTER_DIR, 'skill_word_counter.pkl')\n",
    "X_SIM_PATH = os.path.join(SKILL_WORD_COUNTER_DIR, 'X_sim.pkl')   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>c) All in one function</h4>\n",
    "<a id=\"2.1.c\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_or_update_df_union():\n",
    "    \"\"\"Create or update df union from each single job board database : from data import&cleaning to feature engineering.\n",
    "    Also create/update the word counting sparse matrix for job_offers (used for Dice similarity)\"\"\"\n",
    "    \n",
    "    #First import all the jb board database\n",
    "    DB_SAVING_DIR = './job_db'\n",
    "    DB_APEC_NEW_SAVING_PATH = os.path.join(DB_SAVING_DIR, 'apec_new.xlsx')\n",
    "    DB_INDEED_NEW_SAVING_PATH = os.path.join(DB_SAVING_DIR, 'indeed_new.xlsx')\n",
    "    DB_UNION_SAVING_PATH = os.path.join(DB_SAVING_DIR, 'union_database.xlsx') #union of individual databases\n",
    "\n",
    "    dateparse = lambda x: pd.datetime.strptime(x, '%d/%m/%Y') #to parse date format when importing the dataframe\n",
    "    df={} # initiate the dict for the job database\n",
    "    update = os.path.isfile(DB_UNION_SAVING_PATH)\n",
    "    if update:\n",
    "        file_name_pat='\\*_new.xlsx' #only *_new.xlsx files (preprocessing task has been previously executed on an old db)\n",
    "    else:\n",
    "        file_name_pat='\\*_db.xlsx' #only *_db.xlsx files (preprocessing task has never been executed before)\n",
    "    \n",
    "    for filepath in glob.glob(DB_SAVING_DIR + file_name_pat):\n",
    "        name_origin = re.search(r'\\w+(?=_(db|new)\\.xlsx)', filepath).group()\n",
    "        df[name_origin] = pd.read_excel(filepath, parse_dates=['pub_date'], date_parser=dateparse)\n",
    "        df[name_origin]['origin'] = name_origin\n",
    "        print(\"Raw data - {}: {} job offers\".format(name_origin, len(df[name_origin])))\n",
    "    \n",
    "    #Proceed to some data cleaning\n",
    "    mask={} #initiate the dict for boolean mask filtering\n",
    "    mask['apec'] = df['apec']['job_title'].map(lambda x: re.match(r'.*\\b(data|donn√©es?|bi|business intelligence)\\b.*',\n",
    "                                                                  str(x), flags=re.IGNORECASE)).notnull()\n",
    "    mask['indeed'] = df['indeed']['pos_type'].map(lambda x: re.match(r'.*\\b(stage|alternance|apprentissage)\\b.*',\n",
    "                                                                     str(x), flags=re.IGNORECASE)).isnull()\n",
    "    for key in df.keys():\n",
    "        df[key] = df[key][mask[key]]\n",
    "        print(\"After filtering - {}: {} job offers\".format(key, len(df[key])))    \n",
    "      \n",
    "    df['apec']['pos_descr'] = df['apec']['comp_info'] + '\\n' + df['apec']['pos_descr'] + '\\n' + df['apec']['profil']\n",
    "    \n",
    "    # Feature engineering\n",
    "    ## City and Department\n",
    "    df['apec']['city'], df['apec']['dept'] = zip(*df['apec']['location'].map(get_city_and_dept_apec))\n",
    "    df['indeed']['city'], df['indeed']['dept'] = zip(*df['indeed']['location'].map(get_city_and_dept_indeed))\n",
    "    \n",
    "    df_union = pd.concat([df[key] for key in df.keys()], ignore_index=True, sort=False) #aggregate all the databases\n",
    "    \n",
    "    ## Region\n",
    "    DEPT_PATH = './dpt_reg_fr.csv'\n",
    "    DEPT_DL_URL = 'https://raw.githubusercontent.com/GuillaumeHarel/portfolio-projects/master/'\\\n",
    "                  '%231_Webscraping_%26_Bokeh/Auxiliary_files/Dept_Region_France.csv'\n",
    "    urllib.request.urlretrieve(DEPT_DL_URL, DEPT_PATH)\n",
    "    df_dept_reg = pd.read_csv(DEPT_PATH, dtype={'regionCode': 'Int32'})\n",
    "    df_union = pd.merge(df_union, df_dept_reg[['departmentCode', 'departmentName', 'regionName']],\n",
    "                        how='left', left_on='dept', right_on='departmentCode')\n",
    "    df_union.drop('departmentCode', 1, inplace=True)\n",
    "    \n",
    "    ##Average Salary\n",
    "    df_union['avg_sal'] = df_union['salary'].map(get_salary)\n",
    "    \n",
    "    ##Job categories\n",
    "    df_union['job_cat'] = df_union['job_title'].map(get_cat_dummies)\n",
    "    df_union[job_cat_labels] = pd.DataFrame(df_union['job_cat'].values.tolist(), index=df_union.index)\n",
    "    \n",
    "    ##Index\n",
    "    df_union['index_0'] = df_union.index\n",
    "    \n",
    "    ##Similarity\n",
    "    df_union['sim'] = np.NaN\n",
    "    \n",
    "    #Compute word counting sparse matrix for job offers (used later for Dice similarity)\n",
    "    tok_pattern = '\\w+\\-?\\w*'\n",
    "    word_counter = CountVectorizer(strip_accents=None, lowercase=True,\n",
    "                                   analyzer='word', token_pattern=tok_pattern, ngram_range=(1, 2),\n",
    "                                   vocabulary=overall_skill_l, binary=True)\n",
    "    X_sim = word_counter.fit_transform(df_union['pos_descr'])\n",
    "    \n",
    "    SKILL_WORD_COUNTER_DIR = './skill_word_counter'\n",
    "    os.makedirs(SKILL_WORD_COUNTER_DIR, exist_ok=True)\n",
    "    SKILL_WORD_COUNTER_PATH = os.path.join(SKILL_WORD_COUNTER_DIR, 'skill_word_counter.pkl')\n",
    "    X_SIM_PATH = os.path.join(SKILL_WORD_COUNTER_DIR, 'X_sim.pkl')\n",
    "    \n",
    "    if update:\n",
    "        with open(X_SIM_PATH, 'rb') as input:\n",
    "            X_sim_db = pickle.load(input)\n",
    "        X_sim_full = vstack([X_sim_db, X_sim])\n",
    "        with open(X_SIM_PATH, 'wb') as output:\n",
    "            pickle.dump(X_sim_full, output)\n",
    "        with open(SKILL_WORD_COUNTER_PATH, 'wb') as output:\n",
    "            pickle.dump(word_counter, output)\n",
    "        print('\\nWord counter transformer and word counting sparse matrix have been updated and saved')\n",
    "        \n",
    "    else:    \n",
    "        with open(X_SIM_PATH, 'wb') as output:\n",
    "            pickle.dump(X_sim, output)\n",
    "        with open(SKILL_WORD_COUNTER_PATH, 'wb') as output:\n",
    "            pickle.dump(word_counter, output)\n",
    "        print('\\nWord counter transformer and word counting sparse matrix have been created and saved')\n",
    "    \n",
    "    \n",
    "    #Eventually save or update df_union\n",
    "    if update:\n",
    "        df_union_full = update_excel_file(DB_UNION_SAVING_PATH, df_union)\n",
    "        for filepath in glob.glob(DB_SAVING_DIR + file_name_pat):\n",
    "            os.remove(filepath) #avoid adding same new file twice\n",
    "        print('\\nDone: the Union DB has been updated and contains {} offers'.format(len(df_union_full)))\n",
    "    else:\n",
    "        with pd.ExcelWriter(DB_UNION_SAVING_PATH, options={'strings_to_urls': False}) as writer:\n",
    "            df_union.to_excel(writer, index=False)\n",
    "        print('\\nDone: the Union DB has been created and contains {} offers'.format(len(df_union)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data - apec: 17 job offers\n",
      "Raw data - indeed: 94 job offers\n",
      "After filtering - apec: 6 job offers\n",
      "After filtering - indeed: 89 job offers\n",
      "\n",
      "Word counter transformer and word counting sparse matrix have been updated and saved\n",
      "\n",
      "Done: the Union DB has been updated and contains 5269 offers\n"
     ]
    }
   ],
   "source": [
    "create_or_update_df_union()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data preprocessing is complete. It's time to build the Bokeh App for the custom aggregated job board.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2) Custom aggregated job board using Bokeh</h3>\n",
    "<a id=\"2.2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to have a nice interactive and synchronized dashboard for the job database, it is necessary to run a Bokeh server to handle our Bokeh App. This will keep the Bokeh model objects in python and in the browser in synchronization with each other, allowing to trigger callbacks to update data when control widgets are manipulated and respond to queries using the full power of python. It will also enable to include some nice custom CSS styling for the dashboard display in browser !\n",
    "<br>\n",
    "The different files required to properly run the Bokeh server and build our dashboard App will be presented in the current chapter section of the Notebook. \n",
    "<br>\n",
    "**However, the Bokeh server must be run via the Windows terminal and all the necessary files must be placed in specific fold/subfolders of the current project directory as described in the Bokeh App architecture subchapter below**.\n",
    "<br> \n",
    "It is not possible to run the Bokeh dashboard App for the aggregated job board directly from the Notebook.\n",
    "<br> \n",
    "<br> \n",
    "For furter information, please read Bokeh documentation [[9]](#ref9) (especially the following user guide section : \"Running a Bokeh Server\", \"Adding Interactions\")\n",
    "A very complete and didactical blog article to get hands on Bokeh is also available here: [[10]](#ref10), [[11]](#ref11), [[12]](#ref12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>a) Bokeh App architecture</h4>\n",
    "<a id=\"2.2.a\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It was necessary to use the \"directory format\" for the Bokeh App in order to include some custom CSS styling.\n",
    "The name of my directory is web_scraping and it respects the following general architecture:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "web_scraping\n",
    "   |\n",
    "   +---job_db\n",
    "   |    +---apec_db.xlsx\n",
    "   |    +---indeed_db.xlsx\n",
    "   |    +---union_database.xlsx\n",
    "   | \n",
    "   +---skill_word_counter\n",
    "   |    +---skill_word_counter.pkl\n",
    "   |    +---X_sim.pkl\n",
    "   |\n",
    "   +---templates\n",
    "   |    +---index.html\n",
    "   |    +---styles.css \n",
    "   |\n",
    "   +---main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*main.py* contains the python code for the interactive dashboard App while the template subdirectory contains an *index.html* Jinja template file and *styles.css* file for custom css styling.\n",
    "<br>\n",
    "These three files are reproduced in the coming sub-chapter and are available in my GitHub repository.\n",
    "<br>\n",
    "<br>\n",
    "To run the Bokeh server with the directory format Bokeh App, in Windows terminal :\n",
    "* Activate the proper python environment;\n",
    "* Change directory to the parent directory of the working directory;\n",
    "* Run the following command:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bokeh serve --show web_scraping\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>b) Bokeh App</h4>\n",
    "<a id=\"2.2.b\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**main.py** (*#not to be executed in the Notebook*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import os\n",
    "import base64\n",
    "import pickle\n",
    "\n",
    "from bokeh.models.widgets import HTMLTemplateFormatter, StringFormatter, DateFormatter\n",
    "from bokeh.models.widgets import DataTable, TableColumn, RadioButtonGroup, TextInput, TextAreaInput, Dropdown,\\\n",
    "    RangeSlider, CheckboxGroup, Div, FileInput\n",
    "from bokeh.models import ColumnDataSource, Panel, Tabs\n",
    "from bokeh.io import curdoc\n",
    "from bokeh.layouts import gridplot, column\n",
    "\n",
    "#manage the directories and paths\n",
    "PROJECT_DIR = './web_scraping'\n",
    "DB_SAVING_DIR = os.path.join(PROJECT_DIR, 'job_db')\n",
    "UNION_DB_PATH = os.path.join(DB_SAVING_DIR, 'union_database.xlsx')\n",
    "SKILL_WORD_COUNTER_DIR = os.path.join(PROJECT_DIR, 'skill_word_counter')\n",
    "WORD_COUNTER_PATH = os.path.join(SKILL_WORD_COUNTER_DIR, 'skill_word_counter.pkl')\n",
    "JOB_OFFER_SPR_MATRIX_PATH = os.path.join(SKILL_WORD_COUNTER_DIR, 'X_sim.pkl')\n",
    "\n",
    "#import df_union\n",
    "df_union = pd.read_excel(UNION_DB_PATH)\n",
    "\n",
    "#A few \"metrics\" and  labels for later\n",
    "site_source = ['Apec', 'Indeed']\n",
    "origin_source = ['All sites'] + site_source\n",
    "total_job_numbers = [len(df_union)] + [len(df_union[df_union['origin'] == origin.lower()]) for origin in site_source]\n",
    "min_salary = [(np.min(df_union['avg_sal'])//5-1)*5] + \\\n",
    "             [(np.min(df_union.loc[df_union['origin'] == origin.lower(), 'avg_sal'])//5-1)*5 for origin in site_source]\n",
    "\n",
    "max_salary = [(np.max(df_union['avg_sal'])//5+1)*5] + \\\n",
    "             [(np.max(df_union.loc[df_union['origin'] == origin.lower(), 'avg_sal'])//5+1)*5 for origin in site_source]\n",
    "\n",
    "location_choice = [\"France\", \"Auvergne-Rh√¥ne-Alpes\", \"Rh√¥ne\", \"Lyon\"]\n",
    "location_scale_col = [None, 'regionName', 'departmentName', 'city'] #usefull later for RadioGroupbutton\n",
    "\n",
    "job_cat_labels = [\"Data scientist\",\n",
    "                  \"Data analyst & BI\",\n",
    "                  \"Big Data (engineer, dev, archi)\",\n",
    "                  \"IT Project Manager (data related)\",\n",
    "                  \"Data Manager/Officer\",\n",
    "                  \"Unclassified\"]\n",
    "\n",
    "#column to keep to display in Bokeh App\n",
    "ord_kept_columnns = ['index_0', 'pub_date', 'job_title', 'company', 'location', 'pos_type',\n",
    "                     'nb_pos', 'req_exp', 'salary', 'act_area', 'url', 'sim']\n",
    "\n",
    "#names of the kept columns to display in Bokeh App (datatable)\n",
    "ord_names = ['#', 'Date', 'Title', 'Company', 'Location', 'Type',\n",
    "             'Nb', 'Experience', 'Salary', 'Activity area', 'URL', 'Similarity']\n",
    "\n",
    "#respective length of each column in Bokeh datatable (in px)\n",
    "ord_kept_columnns_len = [30, 60, 120, 100, 110, 30,\n",
    "                         20, 80, 80, 130, 35, 50]\n",
    "\n",
    "\n",
    "#Define all column format for the Bokeh datatable\n",
    "date_formatter = DateFormatter(format='%d/%m/%Y')\n",
    "##URL format to display origin (e.g. apec, indeed) instead of URL link\n",
    "url_formatter = HTMLTemplateFormatter(template='<a href=\"<%= url %>\"><%= origin %></a>')\n",
    "##Add a \"hover\" tool for datatable to display long text using a HTMLTemplateFormatter\n",
    "template_long_text = \"\"\"<span href=\"#\" data-toggle=\"tooltip\" title=\"<%= value %>\"><%= value %></span>\"\"\"\n",
    "text_tooltip = HTMLTemplateFormatter(template=template_long_text)\n",
    "##One more HTLMTemplateFormatter with underscore js to customise font color of Dice similarity according to its value\n",
    "template_sim = \"\"\"\n",
    "            <div style=\"color: <%= \n",
    "                    (function colorfromint(){\n",
    "                        if(sim>0.4){return('green')}\n",
    "                        else if (sim>0.2){return('orange')}\n",
    "                        else if (sim>=0) {return('red')}\n",
    "                        else {return('black')}\n",
    "                        }()) %>;\"> \n",
    "                <%= value %>\n",
    "                </font>\n",
    "            </div>\n",
    "            \"\"\"\n",
    "template_sim_formatter = HTMLTemplateFormatter(template=template_sim)\n",
    "\n",
    "index_formatter = StringFormatter(font_style='italic', text_align='right', text_color=(142, 142, 161))\n",
    "\n",
    "formatters = [index_formatter] + [date_formatter] + [text_tooltip]*(len(ord_names)-4) + \\\n",
    "             [url_formatter] + [template_sim_formatter]\n",
    "\n",
    "\n",
    "\n",
    "#TableColumn object necessary to build the Bokeh datatable\n",
    "Columns = [TableColumn(field=Ci, title=Ti, width=Wi, formatter=Fi) for Ci, Ti, Wi, Fi in zip(\n",
    "    ord_kept_columnns, ord_names, ord_kept_columnns_len, formatters)]\n",
    "\n",
    "\n",
    "#custom template for a Div object that will be included in the Bokeh App to display the current number of job offers\n",
    "template_div = (\"\"\"\n",
    "      <div class='content'>\n",
    "       <div class='name'> {site_name} </div>\n",
    "        <span class='number'>{number}<small>/{total}</small> </span>\n",
    "      </div>\n",
    "      \"\"\")\n",
    "\n",
    "\n",
    "#initiate all the dict that will contain widget objects.\n",
    "# One panel per job board origin and one independent database for each panel!\n",
    "cv_input = {}\n",
    "location_select = {}\n",
    "div_job_number = {}\n",
    "select_1 = {}\n",
    "select_2 = {}\n",
    "select_3 = {}\n",
    "\n",
    "df_source = {}\n",
    "data_table_panel = {}\n",
    "source = {}\n",
    "table_row_1 = {}\n",
    "table_row_2 = {}\n",
    "table_row_3 = {}\n",
    "table_row_4 = {}\n",
    "table_bloc = {}\n",
    "grid = {}\n",
    "tab = {}\n",
    "\n",
    "#age max for job offer age\n",
    "age_max = \"100000\"\n",
    "\n",
    "#build all widget objects and aggregate them in a grid plot for each panel (according to job board origin).\n",
    "for idx, origin in enumerate(origin_source):\n",
    "    cv_input[origin] = FileInput(accept='.txt')\n",
    "    location_select[origin] = RadioButtonGroup(labels=location_choice, active=0,\n",
    "                                               css_classes=['custom_group_button_bokeh'])\n",
    "    text = template_div.format(site_name=origin,\n",
    "                               number=total_job_numbers[idx],\n",
    "                               total=total_job_numbers[idx])\n",
    "    div_job_number[origin] = Div(text=text, height=50)\n",
    "    select_1[origin] = Dropdown(value=age_max, label='Publication date', css_classes=['custom_button_bokeh'],\n",
    "                                menu=[(\"All\", age_max),\n",
    "                                      (\"Less than 1 day\", \"1\"),\n",
    "                                      (\"Less than 3 days\", \"3\"),\n",
    "                                      (\"Less than 7 days\", \"7\"),\n",
    "                                      (\"Less than 14 days\", \"14\"),\n",
    "                                      (\"Less than 30 days\", \"30\")\n",
    "                                      ]\n",
    "                                )\n",
    "    # WARNING for Dropdown button, use value param. to set default value (and not default_value param. !!!)\n",
    "\n",
    "    select_2[origin] = CheckboxGroup(labels=job_cat_labels, active=list(range(len(job_cat_labels))))\n",
    "\n",
    "    select_3[origin] = RangeSlider(title=\"Salary(k‚Ç¨)\", start=min_salary[idx], end=max_salary[idx], step=5,\n",
    "                                   value=(min_salary[idx], max_salary[idx]))\n",
    "\n",
    "    if origin == 'All sites':\n",
    "        df_source[origin] = df_union\n",
    "    else:\n",
    "        df_source[origin] = df_union[df_union['origin'] == origin.lower()]\n",
    "    source[origin] = ColumnDataSource(df_source[origin])\n",
    "    data_table_panel[origin] = DataTable(columns=Columns, source=source[origin],\n",
    "                                         reorderable=True, fit_columns=True, index_position=None,\n",
    "                                         width=1000, height=260, row_height=23,\n",
    "                                         css_classes=[\"my-table\"])\n",
    "    table_row_1[origin] = TextInput(value='', title=\"Job title\")\n",
    "    table_row_2[origin] = TextInput(value='', title=\"Company\")\n",
    "    table_row_3[origin] = TextInput(value='', title=\"Location\")\n",
    "    table_row_4[origin] = TextInput(value='', title=\"Recruitment responsible\")\n",
    "    table_bloc[origin] = TextAreaInput(value='', title=\"Job description\", cols=1000, max_length=5000, rows=11)\n",
    "    grid[origin] = gridplot([[cv_input[origin], location_select[origin]],\n",
    "                             [column(select_1[origin],\n",
    "                                     select_2[origin],\n",
    "                                     select_3[origin],\n",
    "                                     div_job_number[origin]),\n",
    "                              data_table_panel[origin]],\n",
    "                             [column(table_row_1[origin],\n",
    "                                     table_row_2[origin],\n",
    "                                     table_row_3[origin],\n",
    "                                     table_row_4[origin]),\n",
    "                              table_bloc[origin]]])\n",
    "\n",
    "    tab[origin] = Panel(child=grid[origin], title=origin)\n",
    "\n",
    "tabs = Tabs(tabs=[tab[origin] for origin in origin_source])\n",
    "\n",
    "\n",
    "def function_source(attr, old, new):\n",
    "    \"\"\"Display information in appropriate table_row/bloc Widgets on row selection in the datatable\"\"\"\n",
    "    active_panel = origin_source[tabs.active]\n",
    "    try:\n",
    "        selected_index = source[active_panel].selected.indices[0]\n",
    "        table_row_1[active_panel].value = str(source[active_panel].data[\"job_title\"][selected_index])\n",
    "        table_row_2[active_panel].value = str(source[active_panel].data[\"company\"][selected_index])\n",
    "        table_row_3[active_panel].value = str(source[active_panel].data[\"location\"][selected_index])\n",
    "        table_row_4[active_panel].value = str(source[active_panel].data[\"recruit_resp\"][selected_index])\n",
    "        table_bloc[active_panel].value = str(source[active_panel].data[\"pos_descr\"][selected_index])\n",
    "    except IndexError:\n",
    "        pass\n",
    "\n",
    "\n",
    "def make_dataset(offer_age, min_sal, max_sal, job_cat, loc):\n",
    "    \"\"\"Make a subset of the full dataset according to filters defined by user via Bokeh widgets\"\"\"\n",
    "    data = df_source[origin_source[tabs.active]]\n",
    "    today = pd.Timestamp.today().floor(\"D\")\n",
    "    min_date = today - datetime.timedelta(days=int(offer_age))\n",
    "    date_mask = data['pub_date'].between(min_date, today)\n",
    "    salary_mask = ((data['avg_sal'] >= min_sal) & (data['avg_sal'] <= max_sal)) | (data['avg_sal'].isnull())\n",
    "    cat_mask = (data[job_cat] == 1).any(axis=1)\n",
    "    if loc == 0:\n",
    "        loc_mask = [True] * len(data)\n",
    "    elif loc < 3:\n",
    "        loc_mask = data[location_scale_col[loc]] == location_choice[loc]\n",
    "    else:\n",
    "        loc_mask = data[location_scale_col[loc]].map(lambda x: bool(re.match(location_choice[loc],\n",
    "                                                                             str(x), flags=re.IGNORECASE)))\n",
    "    sub_df = data[date_mask & salary_mask & cat_mask & loc_mask]\n",
    "    return sub_df\n",
    "\n",
    "\n",
    "def update_dpdown_label():\n",
    "    \"\"\"Updata dropdown button label according to user selection\"\"\"\n",
    "    active_panel = origin_source[tabs.active]\n",
    "    offer_age = select_1[active_panel].value\n",
    "    if offer_age == age_max:\n",
    "        new_label = \"Publication date : All\"\n",
    "    else:\n",
    "        new_label = \"Publication date : Less than {} day(s)\".format(offer_age)\n",
    "    select_1[active_panel].label = new_label\n",
    "\n",
    "\n",
    "def update_div_job_numbers():\n",
    "    \"\"\"Update job number display in the Div object according to current activated filters\"\"\"\n",
    "    new_site_name = origin_source[tabs.active]\n",
    "    new_number = len(source[origin_source[tabs.active]].data['index'])\n",
    "    total = total_job_numbers[tabs.active]\n",
    "    new_text = template_div.format(site_name=new_site_name,\n",
    "                           number=new_number,\n",
    "                           total=total)\n",
    "    div_job_number[origin_source[tabs.active]].update(text=new_text)\n",
    "\n",
    "\n",
    "def change_data_source():\n",
    "    \"\"\"Update the source datatable via the make_dataset function and filters provided by user via widgets\"\"\"\n",
    "    active_panel = origin_source[tabs.active]\n",
    "    offer_age = select_1[active_panel].value\n",
    "    min_sal = select_3[active_panel].value[0]\n",
    "    max_sal = select_3[active_panel].value[1]\n",
    "    job_cat = [select_2[active_panel].labels[i] for i in select_2[active_panel].active]\n",
    "    loc = location_select[active_panel].active\n",
    "    new_src = make_dataset(offer_age=offer_age, min_sal=min_sal, max_sal=max_sal, job_cat=job_cat, loc=loc)\n",
    "    source[active_panel].data.update(ColumnDataSource(new_src).data) #if passing directly the the new src in update, issue with index\n",
    "\n",
    "\n",
    "def compute_similarity(cv, word_counter_path=WORD_COUNTER_PATH, offer_spr_matrix_path=JOB_OFFER_SPR_MATRIX_PATH):\n",
    "    \"\"\" Transforms a resume into a word counting sparse matrix by a word counter (restricted vocabulary) and compute\n",
    "    Dice index metric with word counting sparse matrix for job offers.\"\"\"\n",
    "    with open(word_counter_path, 'rb') as input:\n",
    "        word_counter = pickle.load(input)\n",
    "    with open(offer_spr_matrix_path, 'rb') as input:\n",
    "        X_sim = pickle.load(input)\n",
    "    Y = word_counter.transform([cv]) #word_counter iterates over raw text document, not raw text\n",
    "    Z = X_sim.multiply(Y) #element-wise multiplication for sparse matrix\n",
    "    sim_mat = 2 * Z.sum(axis=1)/(X_sim.sum(axis=1)+Y.sum())\n",
    "    sim_arr = np.around(np.squeeze(np.asarray(sim_mat)), decimals=2) #convert matrix object into array\n",
    "    D_sim = pd.Series(sim_arr, index=df_union.index)\n",
    "    return D_sim\n",
    "\n",
    "\n",
    "def update_similarity(attr, old, new):\n",
    "    \"\"\"Compute similarity between job offers and a resume provided by the user via the InputFile widget\"\"\"\n",
    "    active_panel = origin_source[tabs.active]\n",
    "    cv_64_enc_str = cv_input[active_panel].value\n",
    "    # Encoding the Base64 encoded string into bytes\n",
    "    cv_64_enc_b = cv_64_enc_str.encode('utf-8')\n",
    "    # Decoding the Base64 bytes\n",
    "    cv_enc_b = base64.b64decode(cv_64_enc_b)\n",
    "    # Decoding the bytes to string\n",
    "    cv_str = cv_enc_b.decode('utf-8')\n",
    "    D_sim = compute_similarity(cv=cv_str)\n",
    "    if active_panel != 'All sites':\n",
    "        mask_origin = df_source['All sites']['origin'] == active_panel.lower()\n",
    "        D_sim = D_sim[mask_origin]\n",
    "    df_source[active_panel]['sim'] = D_sim\n",
    "    change_data_source()\n",
    "\n",
    "#triggers the functions according to widget modifications by user\n",
    "for origin in origin_source:\n",
    "    cv_input[origin].on_change(\"value\", update_similarity)\n",
    "    select_1[origin].on_change(\"value\", lambda attr, old, new: change_data_source())\n",
    "    select_1[origin].on_change(\"value\", lambda attr, old, new: update_dpdown_label())\n",
    "    select_3[origin].on_change(\"value\", lambda attr, old, new: change_data_source())\n",
    "    select_2[origin].on_change(\"active\", lambda attr, old, new: change_data_source())\n",
    "    location_select[origin].on_change(\"active\", lambda attr, old, new: change_data_source())\n",
    "    source[origin].selected.on_change('indices', function_source)\n",
    "    source[origin].on_change('data', lambda attr, old, new: update_div_job_numbers())\n",
    "\n",
    "curdoc().add_root(tabs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**index.html**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "    <head>\n",
    "      <meta charset=\"utf-8\">\n",
    "      {{ bokeh_css }}\n",
    "      {{ bokeh_js }}\n",
    "      <style type=\"text/css\">{% include 'styles.css' %}</style>\n",
    "    </head>\n",
    "    <body>\n",
    "      {{ plot_div|indent(8) }}\n",
    "      {{ plot_script|indent(8) }}\n",
    "    </body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**styles.css**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".content{\n",
    "    color: #2b6980;      \n",
    "    width: 100px;\n",
    "    height: 30px;\n",
    "\ttext-align:center;\n",
    "\tdisplay:inline-block;\n",
    "\tbackground-color: #dbe7eb;\n",
    "\tfont-weight: bold;\n",
    "\tposition: absolute;\n",
    "\tleft: 200px;\n",
    "}\n",
    "\n",
    ".name{\n",
    "    font-size: 6pt;      \n",
    "}\n",
    "\n",
    ".number{\n",
    "    font-size: 10pt;\n",
    "}\n",
    "\n",
    "\n",
    ".custom_button_bokeh button.bk.bk-btn.bk-btn-default {\n",
    "\tcolor: black;\n",
    "\tfont-size: 9pt;\n",
    "\tbackground-color: #ff841278;\n",
    "\tborder-color: #999;\n",
    "\tpadding : 6px 32px;\n",
    "}\n",
    ".custom_button_bokeh div.bk.bk-menu.bk-below {\n",
    "\tposition: absolute;\n",
    "\tleft: 0;\n",
    "\twidth: 100%;\n",
    "\tz-index: 100;\n",
    "\tcursor: pointer;\n",
    "\tfont-size: 12px;\n",
    "\tbackground-color: #fff;\n",
    "\tborder: 1px solid #ccc;\n",
    "\tborder-radius: 4px;\n",
    "\tbox-shadow: 0 6px 12px rgba(0,0,0,0.175);\n",
    "\tbackground-color: antiquewhite;\n",
    "}\n",
    "\n",
    ".custom_group_button_bokeh div.bk.bk-btn.bk-btn-default {\n",
    "\tcolor: black;\n",
    "\tfont-size: 9pt;\n",
    "\tbackground-color: #b4cdd57a;\n",
    "\tborder-color: #999;\n",
    "\tpadding : 6px 32px;\n",
    "}\n",
    "\n",
    ".custom_group_button_bokeh div.bk.bk-btn.bk-btn-default.bk-active {\n",
    "\tcolor: black;\n",
    "\tfont-size: 9pt;\n",
    "\tbackground-color: #a6cedb7a;\n",
    "\tborder-color: #999;\n",
    "\tpadding : 6px 32px;\n",
    "\tfont-weight : bold;\n",
    "}\n",
    "\n",
    ".bk-root .bk-tabs-header .bk-tab.bk-active {\n",
    "    color: #4d4d4d;\n",
    "    background-color: #ffd26059;\n",
    "    border-color: #e6e6e6;\n",
    "    font-weight: bold;\n",
    "}\n",
    "\n",
    ".my-table .slick-header-column:nth-child(1) .slick-column-name{\n",
    "    float:right !important;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**And the final result for the interactive Bokeh dashboard for the aggregated job board in action:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Bokeh_Gif_URL](https://s3.gifyu.com/images/bokeh.gif \"Bokeh dashboard sample\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>III) Exploratory data analysis (on going...)</h2>\n",
    "<a id=\"3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<h2><u>Useful links</u></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] https://selenium-python.readthedocs.io/index.html#\n",
    "<a id=\"ref1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[2] https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
    "<a id=\"ref2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[3] http://jonathansoma.com/lede/foundations-2018/classes/selenium/selenium-windows-install/\n",
    "<a id=\"ref3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[4] https://medium.com/ymedialabs-innovation/web-scraping-using-beautiful-soup-and-selenium-for-dynamic-page-2f8ad15efe25\n",
    "<a id=\"ref4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[5] http://sametmax.com/parser-du-html-avec-beautifulsoup/\n",
    "<a id=\"ref5\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[6] http://blogs.quovantis.com/how-to-write-awesome-xpaths-for-test-automation-in-selenium/\n",
    "<a id=\"ref6\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[7] https://www.pluralsight.com/guides/web-scraping-with-selenium\n",
    "<a id=\"ref2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[8] https://hal.archives-ouvertes.fr/hal-00874280/document\n",
    "<a id=\"ref8\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[9] https://bokeh.pydata.org/en/latest/docs/user_guide.html\n",
    "<a id=\"ref9\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[10] https://towardsdatascience.com/data-visualization-with-bokeh-in-python-part-one-getting-started-a11655a467d4\n",
    "<a id=\"ref10\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[11] https://towardsdatascience.com/data-visualization-with-bokeh-in-python-part-ii-interactions-a4cf994e2512\n",
    "<a id=\"ref11\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[12] https://towardsdatascience.com/data-visualization-with-bokeh-in-python-part-iii-a-complete-dashboard-dc6a86aa6e23\n",
    "<a id=\"ref12\"></a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
