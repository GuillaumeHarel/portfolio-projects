{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center> Web scraping multiple job boards with Selenium & BeautifulSoup and aggregating the job offers in an interactive Bokeh dashboard </center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://i.imgur.com/ixs0UDr.png' width=\"1000\" height=\"1000\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><u>Context and objectives</u></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is focused on a personal project which aims at centralizing in a single app my job searches on several online job boards. To date, in the framework of this project, \"only\" the APEC and Indeed job websites have been scraped and the job search queries have been restricted to data-related positions in France. \n",
    "<br>\n",
    "\n",
    "This project can be divided into three tasks with distinct purposes. \n",
    "<br>\n",
    "The first chapter of the notebook tackles in details the web scraping exercise with the Python libraries Selenium and BeautifulSoup while the second one is specifically dedicated to the construction of the personal job dashboard using the Bokeh library. Finally, the last task will consist in an exploratory data analysis of all the information collected in order to have an overall picture of the data-related job market in France and enlighten the most valuable and sought-after soft and hard skills.\n",
    "<br>\n",
    "<br>\n",
    "<u>*Key skills:*</u>\n",
    "<br>\n",
    "**Unstructured data**, **Web scraping**, **Text mining**, **Dataviz**, **Text similarity metrics**\n",
    "<br>\n",
    "<u>*Key libraries:*</u>\n",
    "<br>\n",
    "**Selenium**, **BeautifulSoup**, **Bokeh**, **Pandas**, **Re**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<h2><center><u>Table of contents</u></center></h2>\n",
    "\n",
    "[<h3>I) Web scraping the job boards</h3>](#1)\n",
    "[<h4>1) APEC job board </h4>](#1.1)\n",
    "[<h5>&emsp;a) Step by step approach </h5>](#1.1.a) \n",
    "[<h5>&emsp;b) All in one functions </h5>](#1.1.b)  \n",
    "[<h4>2) Indeed job board </h4>](#1.2)\n",
    "[<h5>&emsp;a) Step by step approach </h5>](#1.2.a)\n",
    "[<h5>&emsp;b) All in one functions </h5>](#1.2.b)\n",
    "<br>\n",
    "[<h3>II) Using Bokeh to build a custom aggregated job board</h3>](#2)\n",
    "[<h4>1) Preprocessing the raw data </h4>](#2.1)\n",
    "[<h5>&emsp;a) Data cleaning </h5>](#2.1.a)\n",
    "[<h5>&emsp;b) Feature engineering </h5>](#2.1.b)\n",
    "[<h5>&emsp;c) All in one function </h5>](#2.1.c)\n",
    "<br>\n",
    "[<h4>2) Custom aggregated job board using Bokeh </h4>](#2.2)\n",
    "[<h5>&emsp;a) Bokeh App architecture </h5>](#2.2.a)\n",
    "[<h5>&emsp;a) Bokeh App </h5>](#2.2.b)\n",
    "<br>\n",
    "[<h3>III) Exploratory data analysis (on going...)</h3>](#3)\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>I) Web scraping the job boards</h2>\n",
    "<a id=\"1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For dynamic web scraping I used the powerful library combination of Selenium and BeautifulSoup. BeautifulSoup would be self-sufficient for static scraping but does not handle Javascript components and cannot deal with the few necessary event-actions for the targeted websites (e.g. closing a popup, navigating to next page, scroll down the page to display its full content ...)\n",
    "<br>\n",
    "Selenium allows web browser interactions/navigation directly from Python via a chosen web driver and can also fetch the HTML page source for the current DOM of a website. Afterwards, BeautifulSoup can perform the parsing job of the document using a custom parser (here \"lxml\").\n",
    "<br>\n",
    "<br>\n",
    "For furter information on how to install and use these libraries, please read the available documentations [[1]](#ref1), [[2]](#ref2).\n",
    "Other usefull contents browsed during my journey through web scraping : [[3]](#ref3), [[4]](#ref4), [[5]](#ref4), [[6]](#ref6), [[7]](#ref7).\n",
    "<br>\n",
    "<br>\n",
    "<u>*Prior to scrape a website, its general terms of use should be checked.*</u>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1) APEC job board</h3>\n",
    "<a id=\"1.1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>a) Step by step approach</h4>\n",
    "<a id=\"1.1.a\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the APEC job board, the process of retrieving the target information from unstructured data will be presented step by step. Feel free to directly move to the next section if you are already familiar with Selenium and BeautifulSoup. \n",
    "<br>\n",
    "Please note that APEC is a French job board and that some information will be displayed in french.\n",
    "<br>\n",
    "<br>\n",
    "First, let's import all the necessary libaries for the Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "import glob\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import string\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from six.moves import urllib\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy.sparse import vstack\n",
    "import seaborn as sns\n",
    "\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "\n",
    "import bokeh\n",
    "\n",
    "from IPython.display import display, Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version:  3.7.3 (default, Apr 24 2019, 15:29:51) [MSC v.1915 64 bit (AMD64)]\n",
      "Numpy version:  1.16.4\n",
      "Scipy version:  1.3.0\n",
      "Pandas version:  0.24.2\n",
      "Scikit-learn version:  0.21.2\n",
      "Seaborn version:  0.9.0\n",
      "Selenium version:  3.141.0\n",
      "BeautifulSoup version:  4.7.1\n",
      "Bokeh version:  1.3.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Python version: \", sys.version)\n",
    "print(\"Numpy version: \", np.__version__)\n",
    "print(\"Scipy version: \", scipy.__version__)\n",
    "print(\"Pandas version: \", pd.__version__)\n",
    "print(\"Scikit-learn version: \", sklearn.__version__)\n",
    "print(\"Seaborn version: \", sns.__version__)\n",
    "print(\"Selenium version: \", selenium.__version__)\n",
    "print(\"BeautifulSoup version: \", bs4.__version__)\n",
    "print(\"Bokeh version: \", bokeh.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define and initiate the Selenium web driver. Headless option is removed here in order to see what we are actually doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--ignore-certificate-errors')\n",
    "options.add_argument('--incognito')\n",
    "#options.add_argument('--headless')\n",
    "driver = webdriver.Chrome(options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_url = \"https://cadres.apec.fr/home/mes-offres-d-emploi.html\"\n",
    "driver.get(some_url)\n",
    "time.sleep(2)\n",
    "driver.find_element_by_xpath(\"//button[@title='Accepter']\").click() #cookie popup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://i.imgur.com/AOI2MwI.png' width=\"600\" height=\"600\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selenium allows you to find and access elements of a web page according to multiple ways (report to documentation). Here, all we need to do is access to two elements : What position we are looking for and Where. \n",
    "<br>\n",
    "To easily find the proper information to access the different elements : Right click > Inspect element (manually)\n",
    "<br> \n",
    "As mentionned earlier, Selenium replicate all the intereactions a \"real end-user\" could have with its web brower. Here we clear the two blank fields and enter our proposition. Finally we press Enter Key while being in the location field to initiate our query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "kw_elem = driver.find_element_by_id(\"keywords\")\n",
    "kw_elem.clear()\n",
    "kw_elem.send_keys('data scientist OU data analyst')\n",
    "loc_elem = driver.find_element_by_xpath(\"//input[@placeholder='Ex : Paris, Lyon ...']\")\n",
    "loc_elem.clear()\n",
    "loc_elem.send_keys('Auvergne-Rhône-Alpes', Keys.RETURN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://i.imgur.com/XZmsNkv.png' width=\"600\" height=\"600\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next step is to collect all the job offer links returned by our query for every page of results. \n",
    "<br>\n",
    "Just one subtility : to display the \"Next\" Button to navigate between the pages, we first need to scroll down to the page bottom. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Navigating to next page (2)\n",
      "Navigating to next page (3)\n",
      "\n",
      " All the pages (3) have been read through\n"
     ]
    }
   ],
   "source": [
    "page_nb = 1\n",
    "job_links = []\n",
    "while True:\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    job_titles = driver.find_elements_by_class_name('offre-title')\n",
    "    for i in range(len(job_titles)):\n",
    "        job_link = job_titles[i].find_element_by_css_selector('a').get_attribute('href')\n",
    "        job_links.append(job_link)\n",
    "    try:\n",
    "        next_page = driver.find_element_by_link_text('Suiv.').click()\n",
    "        page_nb += 1\n",
    "        print(\"Navigating to next page ({})\".format(page_nb))\n",
    "    except:\n",
    "        print(\"\\n All the pages ({}) have been read through\".format(page_nb))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(job_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything seems fine. We are almost done with Selenium. \n",
    "<br> \n",
    "The last action consists in fetching the source code of every webpage stored in our job_links list. Let's move on to the parsing exercise with BeautifulSoup. As adviced by the documentation, I installed and used the lxml parser for speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_offer = job_links[1]\n",
    "driver.get(some_offer)\n",
    "page_source = driver.page_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(page_source, 'lxml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BeautifoulSoup provides a lot of means and alternatives to navigate through the DOM and retrieve the desired elements. Please, read the documentation for further information on how navigating/searching the tree and manipulating the different kind of objects. Like for Selenium, left-click > Inspect element, will be our best option to identify the target information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://i.imgur.com/ZJhM2Ge.png' width=\"700\" height=\"700\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'164451093W'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref = soup.find(string=re.compile('Ref\\. Apec'))\n",
    "re.findall(\"\\d+\\w*\", ref)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Data Scientist F/H'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_title = soup.find('h1', class_=\"text-uppercase ng-binding\").text\n",
    "job_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('BRAINCUBE', 'Issoire - 63')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comp_and_loc = soup.find_all('strong', class_=\"ng-binding\")\n",
    "compagny, location = comp_and_loc[1].text, comp_and_loc[2].text\n",
    "compagny, location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('CDI', '1')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_type = soup.find('span', class_=\"ng-binding ng-scope\").text\n",
    "nb_pos = next(soup.find('span', class_=\"ng-binding ng-scope\").parent.stripped_strings)\n",
    "pos_type, nb_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('30/07/2019', '30/07/2019')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pub_date = re.findall(\"(?:/*\\\\d+)+\", soup.find(string=re.compile('Publiée le')))[0]\n",
    "act_date = re.findall(\"(?:/*\\\\d+)+\", soup.find(string=re.compile('Actualisée le')))[0]\n",
    "pub_date, act_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BeautifulSoup also accepts function for tag. Here we want to retrieve all \"strong\" tag objects with no \"class\" attribute. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_strong_but_no_class(tag):\n",
    "    return (tag.name == \"strong\") and not(tag.has_attr('class'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<strong>Salaire :</strong>,\n",
       " <strong>Prise de poste :</strong>,\n",
       " <strong>Expérience dans le poste :</strong>,\n",
       " <strong>Statut du poste :</strong>,\n",
       " <strong>Zone de déplacement :</strong>,\n",
       " <strong>Secteur d’activité du poste :</strong>,\n",
       " <strong>Le poste et vos missions</strong>,\n",
       " <strong>Pour ce faire :</strong>,\n",
       " <strong>Votre profil</strong>,\n",
       " <strong>Votre équipe</strong>,\n",
       " <strong>En conclusion</strong>,\n",
       " <strong>Process d'intégration</strong>,\n",
       " <strong>Process de recrutement</strong>,\n",
       " <strong>Personne en charge du recrutement :</strong>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_info = soup.find_all(tag_strong_but_no_class)\n",
    "full_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 36 - 45 k€ brut annuel'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_info[0].next_sibling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Tous niveaux d'expérience acceptés\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_info_l = ['salary', 'empl_date', 'req_exp', 'pos_stt', 'btrip_area', 'act_area']\n",
    "relevant_info_d = {}\n",
    "for idx, info in enumerate(relevant_info_l):\n",
    "    relevant_info_d[info] = full_info[idx].next_sibling\n",
    "    \n",
    "relevant_info_d['req_exp']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will retrieve the longer text objects for the description of the position, the required profile, the entreprise and the recruitment process. Let's first try with the position description and then build a function to retrieve all these elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptif du poste\n",
      "Le poste et vos missions\n",
      "Intégré(e) au sein de l'équipe de Recherche vous êtes en charge de concevoir, développer et déployer des applications et des outils d'analyse prédictive\n",
      "Pour ce faire :\n",
      "Vous participez à la définition et aux orientations scientifiques et stratégiques du projet de Braincube et à l'organisation des travaux de recherche.\n",
      "Vous contribuez à l'avancée scientifique du projet en réalisant une part substantielle des travaux de recherche et de veille scientifique et technique.\n",
      "Vous accompagnez la transformation des productions scientifiques en preuves de concept économiquement valorisables.\n",
      "Vous communiquez les résultats des travaux auprès des équipes développement\n"
     ]
    }
   ],
   "source": [
    "pos_descr = soup.find(id='descriptif-du-poste')\n",
    "pos_descr_txt = \"\\n\".join(pos_descr.stripped_strings) #generator\n",
    "print(pos_descr_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_txt_from_soup(id_txt, soup):\n",
    "    \"\"\"Take a soup object and an id and return the full text corresponding to the section\"\"\"\n",
    "    elem = soup.find(id=id_txt)\n",
    "    if elem != None:\n",
    "        elem_txt = \"\\n\".join(elem.stripped_strings) #generator\n",
    "        if id_txt == 'entreprise': #noisy text\n",
    "            elem_txt = re.sub(\"\\nAutres offres de l'entreprise\", '', elem_txt)\n",
    "        elif id_txt == 'processus-de-recrutement': #noisy text\n",
    "            elem_txt = re.sub('\\n?.*POSTULER.*\\nImprimer\\nSignaler cette offre', '', \n",
    "                              elem_txt)\n",
    "        return elem_txt\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_in_l = ['descriptif-du-poste', 'profil-recherche', 'entreprise', 'processus-de-recrutement']\n",
    "id_out_l = ['pos_descr', 'profil', 'comp_info', 'recruit_proc']\n",
    "\n",
    "for id_in, id_out in zip(id_in_l, id_out_l):\n",
    "    relevant_info_d[id_out] = fetch_txt_from_soup(id_in, soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profil recherché\n",
      "Votre profil\n",
      "Vous justifiez d'une ou de plusieurs expériences d'au moins 2 ans en tant qu'ingénieur en sciences des données.\n",
      "Vous êtes titulaire idéalement d'un doctorat ou d'un Bac +5, type école d'ingénieur ou master équivalent, avec une spécialisation en Analyse de données\n",
      "Vous avez une expérience directe en intelligence artificielle et notamment en deep learning (Tensorflow , Torch…) vous êtes familier avec les algorithmes de classification , prédiction, régression (random forest, Lasso, bayesian network…)\n",
      "Vous êtes reconnu pour votre approche scientifique\n",
      "La maîtrise des langages de développement tels que Python ou R est nécessaire pour occuper le poste ainsi qu'une connaissance des modèles de base de données (SQL/noSQL).\n",
      "Votre équipe\n",
      "En tant que Data Scientist, vous évoluez au sein de notre équipe Recherche, composée par 2 Docteurs en Informatique et Mathématiques.\n",
      "Vous reportez votre activité auprès du Directeur de la Recherche et du Développement, co-fondateur de la société, à qui vous êtes directement rattaché(e).\n",
      "En conclusion\n",
      "Pourquoi devenir Braincuber ? -> Parce que cela ne présente que des avantages !\n",
      "un cadre de travail privilégié\n",
      "une mutuelle prise en charge à 100% en isolé\n",
      "un projet d'entreprise soutenu par 100% de nos salariés (sondage interne 2018)\n",
      "un séminaire annuel au ski !\n",
      "Chez Braincube les compétences sont importantes mais aussi la personnalité et les passions !\n",
      "N'hésitez pas à partager avec nous vos centres d'intérêt et vos expériences pro ou perso.\n",
      "Process d'intégration\n",
      "Nous sommes soucieux de l'intégration de nos collaborateurs et nous ferons tout pour que vous vous sentiez bien avec nous dès le début ?\n",
      "Des personnes en interne sont dédiées à la formation.\n",
      "Process de recrutement\n",
      "Notre process de recrutement est rapide.\n",
      "Nous traitons les candidatures quotidiennement. Nous vous contactons rapidement si vous aiguisez notre intérêt. On vous propose après un premier contact téléphonique de rencontrer nos managers et quelques collègues. Et on se décide sous quelques jours.\n"
     ]
    }
   ],
   "source": [
    "print(relevant_info_d['profil'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, the recruitment responsible (if provided)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Alexandra PINAUD - RH'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elem = soup.find('i')\n",
    "try:\n",
    "    recruit_resp = elem.previous_element + elem.text\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    recruit_resp=''\n",
    "\n",
    "recruit_resp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This is pretty much everything we need to do to scrap the APEC job board.\n",
    "<br> The only thing left is to define some nice functions to proceed the whole step by step approach and build our own job database.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>b) All in one functions</h4>\n",
    "<a id=\"1.1.b\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define 3 main functions to handle the full web scraping process and build our own database :\n",
    "* 1st : initiate the webdriver and request the job platform on position key words and location and precise a sorting option for the result;\n",
    "* 2nd : take the driver returned by the first function and browse all the result pages to fetch the URL of every job offer;\n",
    "* 3rd : load source code of every web page provided by the second function, scrape it with BeautifulSoup and store the information in a list of dictionnaries (in anticipation of the coming dataframe). \n",
    "\n",
    "Nothing special to mention for these 3 functions in comparison to the previous step by step presentation. I just added some time.sleep() in the code because of a few erros I encountered while dealing with bigger stream of data. This prevents server overwhelming, allows the page to fully load/wait for a specific element to appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_URL = \"https://cadres.apec.fr/home/mes-offres-d-emploi.html\"\n",
    "JOB_KW = \"data scientist OU data analyst\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_SAVING_DIR = './job_db'\n",
    "os.makedirs(DB_SAVING_DIR, exist_ok=True)\n",
    "DB_APEC_SAVING_PATH = os.path.join(DB_SAVING_DIR, 'apec_db.xlsx')\n",
    "DB_APEC_NEW_SAVING_PATH = os.path.join(DB_SAVING_DIR, 'apec_new.xlsx') #save also new offer in a seperate file for late"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_apec_platform(root_url=ROOT_URL, job_kw=JOB_KW, job_loc=None,\n",
    "                          sort_by='Pertinence'):\n",
    "    \"\"\"Initiate the webdrive and request the job platform on position key words and location and\n",
    "    precise a sorting option for the result\"\"\"\n",
    "    global driver\n",
    "    try:\n",
    "        driver.quit()\n",
    "    except:\n",
    "        pass\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--ignore-certificate-errors')\n",
    "    options.add_argument('--incognito')\n",
    "    options.add_argument('--headless')\n",
    "    options.add_argument(\"--window-size=1366, 768\")\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    driver.get(root_url)\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(2)\n",
    "    try:\n",
    "        driver.find_element_by_xpath(\"//button[@title='Accepter']\").click() #cookie popup\n",
    "        time.sleep(2)\n",
    "    except:\n",
    "        pass\n",
    "    kw_elem = driver.find_element_by_id(\"keywords\")\n",
    "    kw_elem.clear()\n",
    "    kw_elem.send_keys(job_kw)\n",
    "    if job_loc != None:\n",
    "        loc_elem = driver.find_element_by_xpath(\"//input[@placeholder='Ex : Paris, Lyon ...']\")\n",
    "        loc_elem.clear()\n",
    "        loc_elem.send_keys(job_loc, Keys.RETURN)\n",
    "    else:\n",
    "        driver.find_element_by_class_name('btn-block').click()\n",
    "    sort_by_el = driver.find_element_by_link_text(sort_by)\n",
    "    sort_by_el.click()\n",
    "    print(\"Job search request was performed on:\\n\"\n",
    "          \"Job title: {}, Job location: {}, sorted by {}\".format(job_kw, job_loc, sort_by))\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_apec_job_links(driver, old_db_path=DB_APEC_SAVING_PATH, \n",
    "                           max_consec_existing_link=40):\n",
    "    \"\"\"Take the driver request and browse all the result pages to fetch the URL of every job offer\"\"\"\n",
    "    page_nb = 1\n",
    "    job_links = []\n",
    "    consec_existing_link = 0\n",
    "    update = os.path.isfile(old_db_path)\n",
    "    if update is True:\n",
    "        print('A previous database already exists. Only new job links will be fetched')\n",
    "        old_db = pd.read_excel(old_db_path)\n",
    "        old_job_links = old_db['url'].to_list()\n",
    "    while True:\n",
    "        time.sleep(1)\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(1)\n",
    "        job_titles = driver.find_elements_by_class_name('offre-title')\n",
    "        for i in range(len(job_titles)):\n",
    "            job_link = job_titles[i].find_element_by_css_selector('a').get_attribute('href')\n",
    "            job_link = re.split('&totalCount', job_link)[0]\n",
    "            if (update is False) or (job_link not in old_job_links):\n",
    "                job_links.append(job_link)\n",
    "                consec_existing_link = 0\n",
    "            else:\n",
    "                consec_existing_link += 1\n",
    "                if consec_existing_link >= max_consec_existing_link:\n",
    "                    print(\"\\nNo more new links to retrieve\")\n",
    "                    break  \n",
    "        try:\n",
    "            assert consec_existing_link < max_consec_existing_link\n",
    "            next_page = driver.find_element_by_link_text('Suiv.').click()\n",
    "            page_nb += 1\n",
    "            print(\"Navigating to next page ({})\".format(page_nb))\n",
    "        except:\n",
    "            print(\"\\nAll the pages ({}) have been read through\".format(page_nb))\n",
    "            break\n",
    "    return job_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapping_apec_offers(job_links, parser='lxml', LIMIT=None):\n",
    "    \"\"\"Load source code of every web page from the URL job_links list, scrape it with BeautifulSoup \n",
    "    and store the information in a list of dictionnaries\"\"\"\n",
    "    global driver\n",
    "    try:\n",
    "        driver.quit()\n",
    "    except:\n",
    "        pass\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--ignore-certificate-errors')\n",
    "    options.add_argument('--incognito')\n",
    "    options.add_argument('--headless')\n",
    "    options.add_argument(\"--window-size=1366, 768\")\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    job_parser_l = []\n",
    "    relevant_info_l = ['salary', 'empl_date', 'req_exp', 'pos_stt', 'btrip_area', 'act_area']\n",
    "    id_in_l = ['descriptif-du-poste', 'profil-recherche', 'entreprise', \n",
    "               'processus-de-recrutement']\n",
    "    id_out_l = ['pos_descr', 'profil', 'comp_info', 'recruit_proc']\n",
    "    for x, link in enumerate(job_links[:LIMIT]):\n",
    "        size = len(job_links)\n",
    "        if x % 100 == 0:\n",
    "            print('\\n{} offers have been parsed out of {}'.format(x, size))\n",
    "        try:\n",
    "            job_parser_d = {}\n",
    "            driver.get(link)\n",
    "            page_source = driver.page_source\n",
    "            soup = BeautifulSoup(page_source, parser)\n",
    "            ref = soup.find(string=re.compile('Ref\\. Apec'))\n",
    "            job_parser_d['ref_site'] = re.findall(\"Ref. Apec : (\\d*\\w*)\", ref)[0]\n",
    "            ref = soup.find(string=re.compile('Ref\\. Société'))\n",
    "            if ref != None:\n",
    "                job_parser_d['ref_comp'] = re.findall(\"Ref. Société : (\\d*\\w*)\", ref)[0]\n",
    "            else:\n",
    "                job_parser_d['ref_comp'] = ''\n",
    "            job_title = soup.find('h1', class_=\"text-uppercase ng-binding\").text\n",
    "            job_parser_d['job_title'] = re.sub(' F/H', '', job_title)\n",
    "            comp_and_loc = soup.find_all('strong', class_=\"ng-binding\")\n",
    "            job_parser_d['company'] = comp_and_loc[1].text\n",
    "            job_parser_d['location'] = comp_and_loc[2].text\n",
    "            pos_type = soup.find('span', class_=\"ng-binding ng-scope\")\n",
    "            job_parser_d['pos_type'] = pos_type.text\n",
    "            job_parser_d['nb_pos'] = next(pos_type.parent.stripped_strings)\n",
    "            pub_date = re.findall(\"(?:/*\\\\d+)+\", soup.find(string=re.compile('Publiée le')))[0]\n",
    "            job_parser_d['pub_date'] = pub_date\n",
    "            act_date = re.findall(\"(?:/*\\\\d+)+\", soup.find(string=re.compile('Actualisée le')))[0]\n",
    "            job_parser_d['act_date'] = act_date\n",
    "            pos_info = soup.find_all(tag_strong_but_no_class)\n",
    "            for idx, info in enumerate(relevant_info_l):\n",
    "                job_parser_d[info] = pos_info[idx].next_sibling            \n",
    "            for id_in, id_out in zip(id_in_l, id_out_l):\n",
    "                job_parser_d[id_out] = fetch_txt_from_soup(id_txt=id_in, soup=soup)\n",
    "            recruiter = soup.find('i')\n",
    "            try:\n",
    "                recruit_resp = recruiter.previous_element + recruiter.text\n",
    "            except :\n",
    "                recruit_resp = ''\n",
    "            job_parser_d['recruit_resp'] = recruit_resp\n",
    "            job_parser_d['url'] = link\n",
    "                        \n",
    "            job_parser_l.append(job_parser_d)\n",
    "        except:\n",
    "            print('\\n', '~-'*20, 'WARNING', '~-'*20,\n",
    "            '\\nAn issue was encountered with an offer. It might probably be no longer'\n",
    "                ' available at the parsing time.',\n",
    "            '\\nAs a result this offer could not be added to the database. '\n",
    "            'You should investigate job_link number {}'.format(x),\n",
    "            '\\n', '~-'*20, 'WARNING', '~-'*20)\n",
    "    print(\"\\nAll the offers have been processed\")\n",
    "    return job_parser_l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's wrap these three functions into one to perform the full web scraping process and create/update the APEC job database. Here I chose to work with a simple Excel file but a real relational database could be a better option (in particular to feed and update the database !). \n",
    "<br> Let's first define a short function to feed an excel file with new recordings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_excel_file(old_file_path, new_df, keep_index=False, index_label='index_0'):\n",
    "    \"\"\"Feed new recordings from a dataframe to an old excel file with same format\"\"\"\n",
    "    df_db = pd.read_excel(old_file_path)\n",
    "    df_full = df_db.append(new_df, ignore_index=True, sort=False)\n",
    "    if keep_index:\n",
    "        df_full.drop(index_label, 1, inplace=True) #remove previous irrelevant index to prevent non alignment concatenation in future\n",
    "    with pd.ExcelWriter(old_file_path, options={'strings_to_urls': False}) as writer:\n",
    "            df_full.to_excel(writer, index=keep_index, index_label=index_label)\n",
    "    return df_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_apec_db(db_path=DB_APEC_SAVING_PATH, new_file_path=DB_APEC_NEW_SAVING_PATH):\n",
    "    \"\"\"Perform the full web scraping process and create/update the APEC database\"\"\"\n",
    "    driver = request_apec_platform(sort_by='Date')\n",
    "    time.sleep(1)\n",
    "    job_links_new = fetch_apec_job_links(driver=driver, old_db_path=db_path)\n",
    "    print('{} new offers were founded and will be parsed'.format(len(job_links_new)))\n",
    "    new_job_parser = scrapping_apec_offers(job_links=job_links_new)\n",
    "    df_apec_new = pd.DataFrame(new_job_parser)\n",
    "    update = os.path.isfile(db_path)\n",
    "    if update:\n",
    "        df_apec_full = update_excel_file(db_path, df_apec_new)\n",
    "        with pd.ExcelWriter(new_file_path, options={'strings_to_urls': False}) as writer:\n",
    "            df_apec_new.to_excel(writer, index=False)\n",
    "        print('\\nDone: the new offers have been saved and added to the APEC DB')\n",
    "        return df_apec_full\n",
    "    else:\n",
    "        with pd.ExcelWriter(db_path, options={'strings_to_urls': False}) as writer:\n",
    "            df_apec_new.to_excel(writer, index=False)\n",
    "        print('\\nDone: the APEC DB has been created and the offers have been saved')\n",
    "        return df_apec_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job search request was performed on:\n",
      "Job title: data scientist OU data analyst, Job location: None, sorted by Date\n",
      "A previous database already exists. Only new job links will be fetched\n",
      "Navigating to next page (2)\n",
      "Navigating to next page (3)\n",
      "Navigating to next page (4)\n",
      "Navigating to next page (5)\n",
      "\n",
      "No more new links to retrieve\n",
      "\n",
      "All the pages (5) have been read through\n",
      "18 new offers were founded and will be parsed\n",
      "\n",
      "0 offers have been parsed out of 18\n",
      "\n",
      "All the offers have been processed\n",
      "\n",
      "Done: the new offers have been saved and added to the APEC DB\n"
     ]
    }
   ],
   "source": [
    "df_apec_full = update_apec_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>act_area</th>\n",
       "      <th>act_date</th>\n",
       "      <th>btrip_area</th>\n",
       "      <th>comp_info</th>\n",
       "      <th>company</th>\n",
       "      <th>empl_date</th>\n",
       "      <th>job_title</th>\n",
       "      <th>location</th>\n",
       "      <th>nb_pos</th>\n",
       "      <th>pos_descr</th>\n",
       "      <th>...</th>\n",
       "      <th>pos_type</th>\n",
       "      <th>profil</th>\n",
       "      <th>pub_date</th>\n",
       "      <th>recruit_proc</th>\n",
       "      <th>recruit_resp</th>\n",
       "      <th>ref_comp</th>\n",
       "      <th>ref_site</th>\n",
       "      <th>req_exp</th>\n",
       "      <th>salary</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Conseil pour les affaires et autres conseils ...</td>\n",
       "      <td>15/05/2019</td>\n",
       "      <td>Pas de déplacement</td>\n",
       "      <td>Entreprise\\nNous cherchons pour notre client d...</td>\n",
       "      <td>MINEO</td>\n",
       "      <td>Dès que possible</td>\n",
       "      <td>Data Analyst/Scientist</td>\n",
       "      <td>Aix-en-Provence - 13</td>\n",
       "      <td>1</td>\n",
       "      <td>Descriptif du poste\\nAmené à travailler sur de...</td>\n",
       "      <td>...</td>\n",
       "      <td>CDI</td>\n",
       "      <td>Profil recherché\\nDe ce fait nous recherchons ...</td>\n",
       "      <td>11/04/2019</td>\n",
       "      <td>Processus de recrutement\\nPersonne en charge d...</td>\n",
       "      <td>Grégoire CLEMENT - Founder</td>\n",
       "      <td>NaN</td>\n",
       "      <td>164141813W</td>\n",
       "      <td>Minimum 3 ans</td>\n",
       "      <td>A partir de 40 k€ brut annuel</td>\n",
       "      <td>https://cadres.apec.fr/offres-emploi-cadres/0_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Conseil en systèmes et logiciels informatiques</td>\n",
       "      <td>15/05/2019</td>\n",
       "      <td>Départementale</td>\n",
       "      <td>Entreprise\\nModis recrute 1000 Talents en 2019...</td>\n",
       "      <td>MODIS FRANCE</td>\n",
       "      <td>Dès que possible</td>\n",
       "      <td>Consultant Data analyst / Data scientist</td>\n",
       "      <td>Toulouse - 31</td>\n",
       "      <td>1</td>\n",
       "      <td>Descriptif du poste\\nVous souhaitez côtoyer au...</td>\n",
       "      <td>...</td>\n",
       "      <td>CDI</td>\n",
       "      <td>Profil recherché\\nDe formation Bac+5/ Ingénieu...</td>\n",
       "      <td>15/05/2019</td>\n",
       "      <td>Processus de recrutement\\nPersonne en charge d...</td>\n",
       "      <td>Lory Lo Moro - Recruteur</td>\n",
       "      <td>MID</td>\n",
       "      <td>164236918W</td>\n",
       "      <td>Minimum 1 an</td>\n",
       "      <td>A négocier</td>\n",
       "      <td>https://cadres.apec.fr/offres-emploi-cadres/0_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Autre mise à disposition de ressources humaines</td>\n",
       "      <td>03/06/2019</td>\n",
       "      <td>Départementale</td>\n",
       "      <td>Entreprise\\nNous sommes un cabinet de conseil ...</td>\n",
       "      <td>ATHANOR INFORMATIQUE</td>\n",
       "      <td>Dès que possible</td>\n",
       "      <td>DATA ANALYST / DATA SCIENTIST</td>\n",
       "      <td>Suresnes - 92</td>\n",
       "      <td>1</td>\n",
       "      <td>Descriptif du poste\\nData Analyst/ Data Scient...</td>\n",
       "      <td>...</td>\n",
       "      <td>CDI</td>\n",
       "      <td>Profil recherché\\nData Analyst/ Data Scientist...</td>\n",
       "      <td>03/06/2019</td>\n",
       "      <td>Processus de recrutement\\nNous recevrons les c...</td>\n",
       "      <td>THIERRY JOUDELAT - Direction</td>\n",
       "      <td>DATA</td>\n",
       "      <td>164266817W</td>\n",
       "      <td>Minimum 2 ans</td>\n",
       "      <td>A négocier</td>\n",
       "      <td>https://cadres.apec.fr/offres-emploi-cadres/0_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Conseil pour les affaires et autres conseils ...</td>\n",
       "      <td>29/05/2019</td>\n",
       "      <td>Pas de déplacement</td>\n",
       "      <td>Entreprise\\nEasy Partner fait aujourd'hui part...</td>\n",
       "      <td>EASY PARTNER</td>\n",
       "      <td>Dès que possible</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Paris 02 - 75</td>\n",
       "      <td>1</td>\n",
       "      <td>Descriptif du poste\\nLa société :\\nNotre clien...</td>\n",
       "      <td>...</td>\n",
       "      <td>CDI</td>\n",
       "      <td>Profil recherché\\nLe profil :\\nVous maîtrisez ...</td>\n",
       "      <td>29/05/2019</td>\n",
       "      <td>Processus de recrutement\\nPersonne en charge d...</td>\n",
       "      <td>. Masson - Consultant en recrutement</td>\n",
       "      <td>NaN</td>\n",
       "      <td>164259645W</td>\n",
       "      <td>Minimum 1 an</td>\n",
       "      <td>40 - 45 k€ brut annuel</td>\n",
       "      <td>https://cadres.apec.fr/offres-emploi-cadres/0_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Conseil pour les affaires et autres conseils ...</td>\n",
       "      <td>29/05/2019</td>\n",
       "      <td>Régionale</td>\n",
       "      <td>Entreprise\\nDu plaisir à faire son travail, da...</td>\n",
       "      <td>SIDERLOG</td>\n",
       "      <td>Dès que possible</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Niort - 79</td>\n",
       "      <td>1</td>\n",
       "      <td>Descriptif du poste\\nPour renforcer son équipe...</td>\n",
       "      <td>...</td>\n",
       "      <td>CDI</td>\n",
       "      <td>Profil recherché\\nDe formation décisionnelle, ...</td>\n",
       "      <td>29/05/2019</td>\n",
       "      <td>Processus de recrutement\\nPersonne en charge d...</td>\n",
       "      <td>Françoise LEGER - Assistante chargée de recrut...</td>\n",
       "      <td>Data</td>\n",
       "      <td>164279571W</td>\n",
       "      <td>Minimum 5 ans</td>\n",
       "      <td>A négocier</td>\n",
       "      <td>https://cadres.apec.fr/offres-emploi-cadres/0_...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            act_area    act_date  \\\n",
       "0   Conseil pour les affaires et autres conseils ...  15/05/2019   \n",
       "1     Conseil en systèmes et logiciels informatiques  15/05/2019   \n",
       "2    Autre mise à disposition de ressources humaines  03/06/2019   \n",
       "3   Conseil pour les affaires et autres conseils ...  29/05/2019   \n",
       "4   Conseil pour les affaires et autres conseils ...  29/05/2019   \n",
       "\n",
       "            btrip_area                                          comp_info  \\\n",
       "0   Pas de déplacement  Entreprise\\nNous cherchons pour notre client d...   \n",
       "1       Départementale  Entreprise\\nModis recrute 1000 Talents en 2019...   \n",
       "2       Départementale  Entreprise\\nNous sommes un cabinet de conseil ...   \n",
       "3   Pas de déplacement  Entreprise\\nEasy Partner fait aujourd'hui part...   \n",
       "4            Régionale  Entreprise\\nDu plaisir à faire son travail, da...   \n",
       "\n",
       "                company          empl_date  \\\n",
       "0                 MINEO   Dès que possible   \n",
       "1          MODIS FRANCE   Dès que possible   \n",
       "2  ATHANOR INFORMATIQUE   Dès que possible   \n",
       "3          EASY PARTNER   Dès que possible   \n",
       "4              SIDERLOG   Dès que possible   \n",
       "\n",
       "                                  job_title              location nb_pos  \\\n",
       "0                    Data Analyst/Scientist  Aix-en-Provence - 13      1   \n",
       "1  Consultant Data analyst / Data scientist         Toulouse - 31      1   \n",
       "2             DATA ANALYST / DATA SCIENTIST         Suresnes - 92      1   \n",
       "3                            Data Scientist         Paris 02 - 75      1   \n",
       "4                            Data Scientist            Niort - 79      1   \n",
       "\n",
       "                                           pos_descr  ... pos_type  \\\n",
       "0  Descriptif du poste\\nAmené à travailler sur de...  ...      CDI   \n",
       "1  Descriptif du poste\\nVous souhaitez côtoyer au...  ...      CDI   \n",
       "2  Descriptif du poste\\nData Analyst/ Data Scient...  ...      CDI   \n",
       "3  Descriptif du poste\\nLa société :\\nNotre clien...  ...      CDI   \n",
       "4  Descriptif du poste\\nPour renforcer son équipe...  ...      CDI   \n",
       "\n",
       "                                              profil    pub_date  \\\n",
       "0  Profil recherché\\nDe ce fait nous recherchons ...  11/04/2019   \n",
       "1  Profil recherché\\nDe formation Bac+5/ Ingénieu...  15/05/2019   \n",
       "2  Profil recherché\\nData Analyst/ Data Scientist...  03/06/2019   \n",
       "3  Profil recherché\\nLe profil :\\nVous maîtrisez ...  29/05/2019   \n",
       "4  Profil recherché\\nDe formation décisionnelle, ...  29/05/2019   \n",
       "\n",
       "                                        recruit_proc  \\\n",
       "0  Processus de recrutement\\nPersonne en charge d...   \n",
       "1  Processus de recrutement\\nPersonne en charge d...   \n",
       "2  Processus de recrutement\\nNous recevrons les c...   \n",
       "3  Processus de recrutement\\nPersonne en charge d...   \n",
       "4  Processus de recrutement\\nPersonne en charge d...   \n",
       "\n",
       "                                        recruit_resp ref_comp    ref_site  \\\n",
       "0                         Grégoire CLEMENT - Founder      NaN  164141813W   \n",
       "1                           Lory Lo Moro - Recruteur      MID  164236918W   \n",
       "2                       THIERRY JOUDELAT - Direction     DATA  164266817W   \n",
       "3               . Masson - Consultant en recrutement      NaN  164259645W   \n",
       "4  Françoise LEGER - Assistante chargée de recrut...     Data  164279571W   \n",
       "\n",
       "          req_exp                          salary  \\\n",
       "0   Minimum 3 ans   A partir de 40 k€ brut annuel   \n",
       "1    Minimum 1 an                      A négocier   \n",
       "2   Minimum 2 ans                      A négocier   \n",
       "3    Minimum 1 an          40 - 45 k€ brut annuel   \n",
       "4   Minimum 5 ans                      A négocier   \n",
       "\n",
       "                                                 url  \n",
       "0  https://cadres.apec.fr/offres-emploi-cadres/0_...  \n",
       "1  https://cadres.apec.fr/offres-emploi-cadres/0_...  \n",
       "2  https://cadres.apec.fr/offres-emploi-cadres/0_...  \n",
       "3  https://cadres.apec.fr/offres-emploi-cadres/0_...  \n",
       "4  https://cadres.apec.fr/offres-emploi-cadres/0_...  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_apec_full.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>act_area</th>\n",
       "      <th>act_date</th>\n",
       "      <th>btrip_area</th>\n",
       "      <th>comp_info</th>\n",
       "      <th>company</th>\n",
       "      <th>empl_date</th>\n",
       "      <th>job_title</th>\n",
       "      <th>location</th>\n",
       "      <th>nb_pos</th>\n",
       "      <th>pos_descr</th>\n",
       "      <th>...</th>\n",
       "      <th>pos_type</th>\n",
       "      <th>profil</th>\n",
       "      <th>pub_date</th>\n",
       "      <th>recruit_proc</th>\n",
       "      <th>recruit_resp</th>\n",
       "      <th>ref_comp</th>\n",
       "      <th>ref_site</th>\n",
       "      <th>req_exp</th>\n",
       "      <th>salary</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1803</th>\n",
       "      <td>Études de marché et sondages</td>\n",
       "      <td>14/08/2019</td>\n",
       "      <td>Pas de déplacement</td>\n",
       "      <td>Entreprise\\nIRI, le leader mondial dans les do...</td>\n",
       "      <td>INFORMATION RESOURCES</td>\n",
       "      <td>Dès que possible</td>\n",
       "      <td>CHARGE(E) D’ETUDES – Service Modèles  IRI – Ch...</td>\n",
       "      <td>Chambourcy - 78</td>\n",
       "      <td>1</td>\n",
       "      <td>Descriptif du poste\\nIRI recherche un(e) Charg...</td>\n",
       "      <td>...</td>\n",
       "      <td>CDI</td>\n",
       "      <td>Profil recherché\\nVous êtes diplômé(e) d’un BA...</td>\n",
       "      <td>14/08/2019</td>\n",
       "      <td>Processus de recrutement\\nPersonne en charge d...</td>\n",
       "      <td>Solene Lavergne - Responsable RH</td>\n",
       "      <td></td>\n",
       "      <td>164480915W</td>\n",
       "      <td>Tous niveaux d'expérience acceptés</td>\n",
       "      <td>34 - 40 k€ brut annuel</td>\n",
       "      <td>https://cadres.apec.fr/offres-emploi-cadres/0_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1804</th>\n",
       "      <td>Conseil pour les affaires et autres conseils ...</td>\n",
       "      <td>14/08/2019</td>\n",
       "      <td>Internationale</td>\n",
       "      <td>Entreprise\\nAdoc Talent Management recrute un·...</td>\n",
       "      <td>ADOC TALENT MANAGEMENT</td>\n",
       "      <td>Dès que possible</td>\n",
       "      <td>Chef·fe d'équipe développement</td>\n",
       "      <td>Paris 12 - 75</td>\n",
       "      <td>1</td>\n",
       "      <td>Descriptif du poste\\nEn lien direct avec le C....</td>\n",
       "      <td>...</td>\n",
       "      <td>CDI</td>\n",
       "      <td>Profil recherché\\nTitulaire d’un diplôme d’ing...</td>\n",
       "      <td>26/07/2019</td>\n",
       "      <td>Processus de recrutement\\nPersonne en charge d...</td>\n",
       "      <td>Faustine Bizet - Consultante en Recrutement</td>\n",
       "      <td></td>\n",
       "      <td>164441597W</td>\n",
       "      <td>Minimum 3 ans</td>\n",
       "      <td>A négocier</td>\n",
       "      <td>https://cadres.apec.fr/offres-emploi-cadres/0_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1805</th>\n",
       "      <td>Conseil en systèmes et logiciels informatiques</td>\n",
       "      <td>14/08/2019</td>\n",
       "      <td>Pas de déplacement</td>\n",
       "      <td>Entreprise\\nAlligator, est une société au cœur...</td>\n",
       "      <td>ALLIGATOR</td>\n",
       "      <td>Dès que possible</td>\n",
       "      <td>Expert Azure DevOps</td>\n",
       "      <td>Valbonne - 06</td>\n",
       "      <td>1</td>\n",
       "      <td>Descriptif du poste\\nMission\\nVous évoluerez d...</td>\n",
       "      <td>...</td>\n",
       "      <td>CDI</td>\n",
       "      <td>Profil recherché\\nProfil\\nDiplôme supérieur da...</td>\n",
       "      <td>14/08/2019</td>\n",
       "      <td>Processus de recrutement\\nPersonne en charge d...</td>\n",
       "      <td>Céline Grimaldi - Responsable RH &amp; recrutement</td>\n",
       "      <td></td>\n",
       "      <td>164480855W</td>\n",
       "      <td>Minimum 4 ans</td>\n",
       "      <td>A négocier</td>\n",
       "      <td>https://cadres.apec.fr/offres-emploi-cadres/0_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1806</th>\n",
       "      <td>Conseil en systèmes et logiciels informatiques</td>\n",
       "      <td>14/08/2019</td>\n",
       "      <td>Pas de déplacement</td>\n",
       "      <td>Entreprise\\nQUI SOMMES-NOUS ?\\nADONIS est une ...</td>\n",
       "      <td>ADONIS</td>\n",
       "      <td>Dès que possible</td>\n",
       "      <td>Lead Technique Hadoop - Big Data</td>\n",
       "      <td>Paris 08 - 75</td>\n",
       "      <td>1</td>\n",
       "      <td>Descriptif du poste\\nDescription de la mission...</td>\n",
       "      <td>...</td>\n",
       "      <td>CDI</td>\n",
       "      <td>Profil recherché\\nProfil recherché :\\nVous pré...</td>\n",
       "      <td>14/08/2019</td>\n",
       "      <td>Processus de recrutement\\nProcess RH ADONIS :\\...</td>\n",
       "      <td>AGNES LACOMBE - DRH</td>\n",
       "      <td>LT_HADOOP_BIGDATA_140819</td>\n",
       "      <td>164480795W</td>\n",
       "      <td>Minimum 2 ans</td>\n",
       "      <td>40 - 50 k€ brut annuel</td>\n",
       "      <td>https://cadres.apec.fr/offres-emploi-cadres/0_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1807</th>\n",
       "      <td>Conseil en systèmes et logiciels informatiques</td>\n",
       "      <td>14/08/2019</td>\n",
       "      <td>Pas de déplacement</td>\n",
       "      <td>Entreprise\\nQUI SOMMES-NOUS ?\\nADONIS est une ...</td>\n",
       "      <td>ADONIS</td>\n",
       "      <td>Dès que possible</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Paris 08 - 75</td>\n",
       "      <td>1</td>\n",
       "      <td>Descriptif du poste\\nDans le cadre de notre dé...</td>\n",
       "      <td>...</td>\n",
       "      <td>CDI</td>\n",
       "      <td>Profil recherché\\nProfil recherché :\\nDe forma...</td>\n",
       "      <td>14/08/2019</td>\n",
       "      <td>Processus de recrutement\\nProcess RH ADONIS :\\...</td>\n",
       "      <td>AGNES LACOMBE - DRH</td>\n",
       "      <td>DATA_SCIENT_AS_140819</td>\n",
       "      <td>164480667W</td>\n",
       "      <td>Minimum 5 ans</td>\n",
       "      <td>40 - 55 k€ brut annuel</td>\n",
       "      <td>https://cadres.apec.fr/offres-emploi-cadres/0_...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               act_area    act_date  \\\n",
       "1803                       Études de marché et sondages  14/08/2019   \n",
       "1804   Conseil pour les affaires et autres conseils ...  14/08/2019   \n",
       "1805     Conseil en systèmes et logiciels informatiques  14/08/2019   \n",
       "1806     Conseil en systèmes et logiciels informatiques  14/08/2019   \n",
       "1807     Conseil en systèmes et logiciels informatiques  14/08/2019   \n",
       "\n",
       "               btrip_area                                          comp_info  \\\n",
       "1803   Pas de déplacement  Entreprise\\nIRI, le leader mondial dans les do...   \n",
       "1804       Internationale  Entreprise\\nAdoc Talent Management recrute un·...   \n",
       "1805   Pas de déplacement  Entreprise\\nAlligator, est une société au cœur...   \n",
       "1806   Pas de déplacement  Entreprise\\nQUI SOMMES-NOUS ?\\nADONIS est une ...   \n",
       "1807   Pas de déplacement  Entreprise\\nQUI SOMMES-NOUS ?\\nADONIS est une ...   \n",
       "\n",
       "                     company          empl_date  \\\n",
       "1803   INFORMATION RESOURCES   Dès que possible   \n",
       "1804  ADOC TALENT MANAGEMENT   Dès que possible   \n",
       "1805               ALLIGATOR   Dès que possible   \n",
       "1806                  ADONIS   Dès que possible   \n",
       "1807                  ADONIS   Dès que possible   \n",
       "\n",
       "                                              job_title         location  \\\n",
       "1803  CHARGE(E) D’ETUDES – Service Modèles  IRI – Ch...  Chambourcy - 78   \n",
       "1804                     Chef·fe d'équipe développement    Paris 12 - 75   \n",
       "1805                                Expert Azure DevOps    Valbonne - 06   \n",
       "1806                   Lead Technique Hadoop - Big Data    Paris 08 - 75   \n",
       "1807                                     Data Scientist    Paris 08 - 75   \n",
       "\n",
       "     nb_pos                                          pos_descr  ... pos_type  \\\n",
       "1803      1  Descriptif du poste\\nIRI recherche un(e) Charg...  ...      CDI   \n",
       "1804      1  Descriptif du poste\\nEn lien direct avec le C....  ...      CDI   \n",
       "1805      1  Descriptif du poste\\nMission\\nVous évoluerez d...  ...      CDI   \n",
       "1806      1  Descriptif du poste\\nDescription de la mission...  ...      CDI   \n",
       "1807      1  Descriptif du poste\\nDans le cadre de notre dé...  ...      CDI   \n",
       "\n",
       "                                                 profil    pub_date  \\\n",
       "1803  Profil recherché\\nVous êtes diplômé(e) d’un BA...  14/08/2019   \n",
       "1804  Profil recherché\\nTitulaire d’un diplôme d’ing...  26/07/2019   \n",
       "1805  Profil recherché\\nProfil\\nDiplôme supérieur da...  14/08/2019   \n",
       "1806  Profil recherché\\nProfil recherché :\\nVous pré...  14/08/2019   \n",
       "1807  Profil recherché\\nProfil recherché :\\nDe forma...  14/08/2019   \n",
       "\n",
       "                                           recruit_proc  \\\n",
       "1803  Processus de recrutement\\nPersonne en charge d...   \n",
       "1804  Processus de recrutement\\nPersonne en charge d...   \n",
       "1805  Processus de recrutement\\nPersonne en charge d...   \n",
       "1806  Processus de recrutement\\nProcess RH ADONIS :\\...   \n",
       "1807  Processus de recrutement\\nProcess RH ADONIS :\\...   \n",
       "\n",
       "                                        recruit_resp  \\\n",
       "1803                Solene Lavergne - Responsable RH   \n",
       "1804     Faustine Bizet - Consultante en Recrutement   \n",
       "1805  Céline Grimaldi - Responsable RH & recrutement   \n",
       "1806                             AGNES LACOMBE - DRH   \n",
       "1807                             AGNES LACOMBE - DRH   \n",
       "\n",
       "                      ref_comp    ref_site  \\\n",
       "1803                            164480915W   \n",
       "1804                            164441597W   \n",
       "1805                            164480855W   \n",
       "1806  LT_HADOOP_BIGDATA_140819  164480795W   \n",
       "1807     DATA_SCIENT_AS_140819  164480667W   \n",
       "\n",
       "                                  req_exp                   salary  \\\n",
       "1803   Tous niveaux d'expérience acceptés   34 - 40 k€ brut annuel   \n",
       "1804                        Minimum 3 ans               A négocier   \n",
       "1805                        Minimum 4 ans               A négocier   \n",
       "1806                        Minimum 2 ans   40 - 50 k€ brut annuel   \n",
       "1807                        Minimum 5 ans   40 - 55 k€ brut annuel   \n",
       "\n",
       "                                                    url  \n",
       "1803  https://cadres.apec.fr/offres-emploi-cadres/0_...  \n",
       "1804  https://cadres.apec.fr/offres-emploi-cadres/0_...  \n",
       "1805  https://cadres.apec.fr/offres-emploi-cadres/0_...  \n",
       "1806  https://cadres.apec.fr/offres-emploi-cadres/0_...  \n",
       "1807  https://cadres.apec.fr/offres-emploi-cadres/0_...  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_apec_full.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2) Indeed job board</h3>\n",
    "<a id=\"1.2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>a) Step by step approach</h4>\n",
    "<a id=\"1.2.a\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The step by step process for the Indeed job board would basically be alike the one for the APEC job board (even much simpler since the indeed offers contain less info and are not as well organised). Let's directly move to the all in one chapter. \n",
    "<br> \n",
    "Please note that I use the French site version for Indeed and that a few information might be in french too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>b) All in one functions</h4>\n",
    "<a id=\"1.2.b\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exactly like for the APEC job board, let's define 3 main functions to handle the full web scraping process and build our own database :\n",
    "* 1st : initiate the webdriver and request the job platform on title-position key words and location and precise a sorting option for the result;\n",
    "* 2nd : take the driver returned by the first function and browse all the result pages to fetch the URL of every job offer;\n",
    "* 3rd : load source code of every web page provided by the second function, scrape it with BeautifulSoup and store the information in a list of dictionnaries (in anticipation of the coming dataframe). \n",
    "\n",
    "Here too I added some time.sleep() in the code because of a few erros I encountered while dealing with a bigger data stream. This prevents server overwhelming, allows the page to fully load/wait for a specific element to appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_URL = \"https://www.indeed.fr/advanced_search\"\n",
    "JOB_KW = \"data\" # allow to scrap on job title on Indeed :)\n",
    "JOB_LOC = 'France'\n",
    "NB_PER_PAGE = '50'\n",
    "DB_INDEED_SAVING_PATH = os.path.join(DB_SAVING_DIR, 'indeed_db.xlsx')\n",
    "DB_INDEED_NEW_SAVING_PATH = os.path.join(DB_SAVING_DIR, 'indeed_new.xlsx') #save also new offer in a seperate file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_indeed_platform(root_url=ROOT_URL, job_kw=JOB_KW, job_loc=None, \n",
    "                            lim=NB_PER_PAGE, sort_by='pertinence'):\n",
    "    \"\"\"Initiate the webdrive and request the job platform on position key words and location and\n",
    "    precise a sorting option for the result\"\"\"\n",
    "    global driver\n",
    "    try:\n",
    "        driver.quit()\n",
    "    except:\n",
    "        pass\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--ignore-certificate-errors')\n",
    "    options.add_argument('--incognito')\n",
    "    #options.add_argument('--headless')\n",
    "    #options.add_argument(\"--window-size=1366, 768\")\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    driver.get(root_url)\n",
    "    try:   \n",
    "        driver.find_element_by_css_selector(\"[class='tos-Button tos-Button-white']\").click()\n",
    "        time.sleep(1)\n",
    "    except:\n",
    "        pass \n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    kw_elem = driver.find_element_by_id(\"as_ttl\")\n",
    "    kw_elem.clear()\n",
    "    kw_elem.send_keys(job_kw)\n",
    "    nb_per_page = Select(driver.find_element_by_id('limit'))\n",
    "    nb_per_page.select_by_visible_text(lim)\n",
    "    sort_by_el = Select(driver.find_element_by_id('sort'))\n",
    "    sort_by_el.select_by_visible_text(sort_by)\n",
    "    if job_loc != None:\n",
    "        loc_elem = driver.find_element_by_id(\"where\")\n",
    "        loc_elem.clear()\n",
    "        loc_elem.send_keys(job_loc, Keys.RETURN)\n",
    "    else:\n",
    "        driver.find_element_by_id('fj').click()\n",
    "    print(\"Job search request was performed on:\\n\"\n",
    "          \"Job title: {}, Job location: {}, sorted by {}\".format(job_kw, job_loc, sort_by))\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_indeed_job_links(driver, old_db_path=DB_INDEED_SAVING_PATH, \n",
    "                           max_consec_existing_link=40):\n",
    "    \"\"\"Take the driver request and browse all the result pages to fetch the URL of every job offer\"\"\"\n",
    "    page_nb = 1\n",
    "    job_links = []\n",
    "    consec_existing_link = 0\n",
    "    update = os.path.isfile(old_db_path)\n",
    "    if update is True:\n",
    "        print('A previous database already exists. Only new job links will be fetched')\n",
    "        old_db = pd.read_excel(old_db_path)\n",
    "        old_job_links = old_db['url'].to_list()\n",
    "    while True:\n",
    "        time.sleep(random.randint(10, 30))\n",
    "        try:\n",
    "            popup = driver.find_element_by_class_name('icl-CloseButton')\n",
    "            popup.click() #close popup\n",
    "        except:\n",
    "            pass\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        job_titles = driver.find_elements_by_class_name(\"title\")\n",
    "        for i in range(len(job_titles)):\n",
    "            job_link = job_titles[i].find_element_by_css_selector('a').get_attribute('href')\n",
    "            if (update is False) or (job_link not in old_job_links):\n",
    "                job_links.append(job_link)\n",
    "                consec_existing_link = 0\n",
    "            else:\n",
    "                consec_existing_link += 1\n",
    "                if consec_existing_link >= max_consec_existing_link:\n",
    "                    print(\"\\nNo more new links to retrieve\")\n",
    "                    break  \n",
    "        try:\n",
    "            assert consec_existing_link < max_consec_existing_link\n",
    "            next_page = driver.find_element_by_link_text('Suivant »').click()\n",
    "            page_nb += 1\n",
    "            print(\"Navigating to next page ({})\".format(page_nb))\n",
    "        except:\n",
    "            print(\"\\nAll the pages ({}) have been read through\".format(page_nb))\n",
    "            break\n",
    "    return job_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indeed_delta_to_date(soup):\n",
    "    \"\"\"Change the Indeed very unconvenient delta date (e.g : \"One hour ago\", \"two weeks ago\") into a classic one\"\"\"\n",
    "    job_age = soup.find(\"div\", class_='jobsearch-JobMetadataFooter').text\n",
    "    if job_age == None:\n",
    "        return ''\n",
    "    else:\n",
    "        digits = re.findall('(\\d+)\\+?\\s(?:jours?|heures?|mois?)', job_age)[0]\n",
    "        now = datetime.datetime.now()\n",
    "        match = re.search('heure', job_age)\n",
    "        if match != None:\n",
    "            date = now - datetime.timedelta(hours=int(digits))\n",
    "        elif re.search('jour', job_age) != None:\n",
    "            date = now - datetime.timedelta(days=int(digits))\n",
    "        else:\n",
    "            date = now - datetime.timedelta(days=30)\n",
    "        return date.strftime('%d/%m/%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapping_indeed_offers(job_links, parser='lxml', LIMIT=None):\n",
    "    \"\"\"Load source code of every web page from the URL job_links list, scrape it with BeautifulSoup \n",
    "    and store the information in a list of dictionnaries\"\"\"\n",
    "    icons_l = ['icl-IconFunctional icl-IconFunctional--location icl-IconFunctional--md',\n",
    "               'icl-IconFunctional icl-IconFunctional--jobs icl-IconFunctional--md',\n",
    "               'icl-IconFunctional icl-IconFunctional--salary icl-IconFunctional--md']\n",
    "    att_out_l = ['location', 'pos_type', 'salary']\n",
    "    job_parser_l = []\n",
    "    while True:\n",
    "        global driver\n",
    "        try:\n",
    "            driver.quit()\n",
    "        except:\n",
    "            pass\n",
    "        options = webdriver.ChromeOptions()\n",
    "        options.add_argument('--ignore-certificate-errors')\n",
    "        options.add_argument('--incognito')\n",
    "        #options.add_argument('--headless')\n",
    "        #options.add_argument(\"--window-size=1366, 768\")\n",
    "        driver = webdriver.Chrome(options=options)\n",
    "        some_link = job_links[random.randint(0, len(job_links))]\n",
    "        driver.get(some_link)\n",
    "        page_source = driver.page_source\n",
    "        soup = BeautifulSoup(page_source, parser)\n",
    "        elem = soup.find(class_=icons_l[0])\n",
    "        if elem == None:\n",
    "            print('Webpage not displayed correctly, restarting the web driver')\n",
    "        else:\n",
    "            print('\\nWebpage displayed correctly, parsing will start... :)')\n",
    "            break\n",
    "   \n",
    "    for x, link in enumerate(job_links[:LIMIT]):\n",
    "        size = len(job_links)\n",
    "        if x % 100 == 0:\n",
    "            print('\\n{} offers have been parsed out of {}'.format(x, size))\n",
    "        try:\n",
    "            job_parser_d = {}\n",
    "            driver.get(link)\n",
    "            page_source = driver.page_source\n",
    "            soup = BeautifulSoup(page_source, parser)\n",
    "            job_title = soup.find('h3').text\n",
    "            job_parser_d['job_title'] = re.sub('\\s+[-(]?\\s?(?:F/H|H/F)\\)?', '', \n",
    "                                               job_title, flags=re.IGNORECASE)\n",
    "            comp = soup.find('div', class_='icl-u-lg-mr--sm icl-u-xs-mr--xs')\n",
    "            job_parser_d['company'] = comp.text\n",
    "            for icon, att_out in zip(icons_l, att_out_l):\n",
    "                elem = soup.find(class_=icon)\n",
    "                if elem!=None:\n",
    "                    job_parser_d[att_out] = elem.next_sibling.text\n",
    "                else:\n",
    "                    job_parser_d[att_out] = ''\n",
    "            pos_descr = soup.find(id='jobDescriptionText')\n",
    "            job_parser_d['pos_descr'] = pos_descr.text\n",
    "            job_parser_d['pub_date'] = indeed_delta_to_date(soup=soup)\n",
    "            job_parser_d['url'] = link                     \n",
    "            job_parser_l.append(job_parser_d)\n",
    "        except:\n",
    "            print('\\n', '~-'*20, 'WARNING', '~-'*20,\n",
    "            '\\nAn issue was encountered with an offer. It might probably be no longer'\n",
    "                ' available at the parsing time.',\n",
    "            '\\nAs a result this offer could not be added to the database. '\n",
    "            'You should investigate job_link number {}'.format(x),\n",
    "            '\\n', '~-'*20, 'WARNING', '~-'*20)\n",
    "    print(\"\\nAll the offers have been processed\")\n",
    "    return job_parser_l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, wrap these three functions into one to perform the full web scraping process and create/update the Indeed job database. Here I chose to work with a simple Excel file but a real relational database could be a better option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_indeed_db(db_path=DB_INDEED_SAVING_PATH, new_file_path=DB_INDEED_NEW_SAVING_PATH):\n",
    "    \"\"\"Perform the full web scraping process and create/update the Indeed database\"\"\"\n",
    "    job_driver = request_indeed_platform(sort_by='date', job_loc='France')\n",
    "    time.sleep(1)\n",
    "    job_links_new = fetch_indeed_job_links(driver=driver, old_db_path=db_path)\n",
    "    print('{} new offers were founded and will be parsed'.format(len(job_links_new)))\n",
    "    new_job_parser = scrapping_indeed_offers(job_links=job_links_new)\n",
    "    df_indeed_new = pd.DataFrame(new_job_parser)\n",
    "    update = os.path.isfile(db_path)\n",
    "    if update:\n",
    "        df_indeed_full=update_excel_file(db_path, df_indeed_new)\n",
    "        with pd.ExcelWriter(new_file_path, options={'strings_to_urls': False}) as writer:\n",
    "            df_indeed_new.to_excel(writer, index=False)\n",
    "        print('\\nDone: the new offers have been saved and added to the Indeed DB')\n",
    "        return df_indeed_full\n",
    "    else:\n",
    "        with pd.ExcelWriter(db_path, options={'strings_to_urls': False}) as writer:\n",
    "            df_indeed_new.to_excel(writer, index=False)\n",
    "        print('\\nDone: the Indeed DB has been created and the offers have been saved')\n",
    "        return df_indeed_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job search request was performed on:\n",
      "Job title: data, Job location: France, sorted by date\n",
      "A previous database already exists. Only new job links will be fetched\n",
      "Navigating to next page (2)\n",
      "Navigating to next page (3)\n",
      "Navigating to next page (4)\n",
      "Navigating to next page (5)\n",
      "\n",
      "No more new links to retrieve\n",
      "\n",
      "All the pages (5) have been read through\n",
      "94 new offers were founded and will be parsed\n",
      "\n",
      "Webpage displayed correctly, parsing will start... :)\n",
      "\n",
      "0 offers have been parsed out of 94\n",
      "\n",
      "All the offers have been processed\n",
      "\n",
      "Done: the new offers have been saved and added to the Indeed DB\n"
     ]
    }
   ],
   "source": [
    "df_indeed_full = update_indeed_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>job_title</th>\n",
       "      <th>location</th>\n",
       "      <th>pos_descr</th>\n",
       "      <th>pos_type</th>\n",
       "      <th>pub_date</th>\n",
       "      <th>salary</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OUTREMER TELECOM</td>\n",
       "      <td>Ingénieur IP Data</td>\n",
       "      <td>Fort-de-France (MQ)</td>\n",
       "      <td>Outremer Telecom, filiale du groupe Altice, so...</td>\n",
       "      <td>CDI</td>\n",
       "      <td>10/05/2019</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.indeed.fr/pagead/clk?mo=r&amp;ad=-6NYl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Digital Virgo</td>\n",
       "      <td>Ingénieur Data Architecte</td>\n",
       "      <td>Aix-en-Provence (13)</td>\n",
       "      <td>WE ARE DIGITAL VIRGO | SMART DATA PERFORMER\\n\\...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13/05/2019</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.indeed.fr/pagead/clk?mo=r&amp;ad=-6NYl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Novencia</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Paris (75)</td>\n",
       "      <td>Contexte\\nData is fuel ! Quelle que soit la fa...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10/05/2019</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.indeed.fr/pagead/clk?mo=r&amp;ad=-6NYl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5ème Agence</td>\n",
       "      <td>Data Manager / Développeur SQL</td>\n",
       "      <td>Bordeaux (33)</td>\n",
       "      <td>Sous la responsabilité du DSI, le Data Manager...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10/05/2019</td>\n",
       "      <td>30 000 € - 40 000 € par an</td>\n",
       "      <td>https://www.indeed.fr/pagead/clk?mo=r&amp;ad=-6NYl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Total</td>\n",
       "      <td>Data scientist</td>\n",
       "      <td>Courbevoie (92)</td>\n",
       "      <td>Au sein de la Branche Exploration Production, ...</td>\n",
       "      <td>CDI</td>\n",
       "      <td>10/05/2019</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.indeed.fr/pagead/clk?mo=r&amp;ad=-6NYl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            company                       job_title              location  \\\n",
       "0  OUTREMER TELECOM               Ingénieur IP Data   Fort-de-France (MQ)   \n",
       "1     Digital Virgo       Ingénieur Data Architecte  Aix-en-Provence (13)   \n",
       "2          Novencia                    Data Analyst            Paris (75)   \n",
       "3       5ème Agence  Data Manager / Développeur SQL         Bordeaux (33)   \n",
       "4             Total                  Data scientist       Courbevoie (92)   \n",
       "\n",
       "                                           pos_descr pos_type    pub_date  \\\n",
       "0  Outremer Telecom, filiale du groupe Altice, so...      CDI  10/05/2019   \n",
       "1  WE ARE DIGITAL VIRGO | SMART DATA PERFORMER\\n\\...      NaN  13/05/2019   \n",
       "2  Contexte\\nData is fuel ! Quelle que soit la fa...      NaN  10/05/2019   \n",
       "3  Sous la responsabilité du DSI, le Data Manager...      NaN  10/05/2019   \n",
       "4  Au sein de la Branche Exploration Production, ...      CDI  10/05/2019   \n",
       "\n",
       "                       salary  \\\n",
       "0                         NaN   \n",
       "1                         NaN   \n",
       "2                         NaN   \n",
       "3  30 000 € - 40 000 € par an   \n",
       "4                         NaN   \n",
       "\n",
       "                                                 url  \n",
       "0  https://www.indeed.fr/pagead/clk?mo=r&ad=-6NYl...  \n",
       "1  https://www.indeed.fr/pagead/clk?mo=r&ad=-6NYl...  \n",
       "2  https://www.indeed.fr/pagead/clk?mo=r&ad=-6NYl...  \n",
       "3  https://www.indeed.fr/pagead/clk?mo=r&ad=-6NYl...  \n",
       "4  https://www.indeed.fr/pagead/clk?mo=r&ad=-6NYl...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_indeed_full.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>job_title</th>\n",
       "      <th>location</th>\n",
       "      <th>pos_descr</th>\n",
       "      <th>pos_type</th>\n",
       "      <th>pub_date</th>\n",
       "      <th>salary</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5073</th>\n",
       "      <td>SILLIKER - MERIEUX NUTRISCIENCES FRANCE</td>\n",
       "      <td>Data Scientist - BIOFORTIS - CDI - Saint-Herbl...</td>\n",
       "      <td>Saint-Herblain (44)</td>\n",
       "      <td>#JobAlert #Stats #DataScientist #Nantes #Mérie...</td>\n",
       "      <td>CDI</td>\n",
       "      <td>01/08/2019</td>\n",
       "      <td>40 000 € par an</td>\n",
       "      <td>https://www.indeed.fr/pagead/clk?mo=r&amp;ad=-6NYl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5074</th>\n",
       "      <td>OUTREMER TELECOM</td>\n",
       "      <td>Ingénieur IP Data</td>\n",
       "      <td>Fort-de-France (MQ)</td>\n",
       "      <td>Outremer Telecom, filiale du groupe Altice, so...</td>\n",
       "      <td>CDI</td>\n",
       "      <td>19/07/2019</td>\n",
       "      <td></td>\n",
       "      <td>https://www.indeed.fr/pagead/clk?mo=r&amp;ad=-6NYl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5075</th>\n",
       "      <td>Teradata</td>\n",
       "      <td>Data Developer</td>\n",
       "      <td>92160 Antony</td>\n",
       "      <td>Requisition Number:\\n205033\\n\\nPosition Title:...</td>\n",
       "      <td></td>\n",
       "      <td>19/07/2019</td>\n",
       "      <td></td>\n",
       "      <td>https://www.indeed.fr/pagead/clk?mo=r&amp;ad=-6NYl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5076</th>\n",
       "      <td>Novencia</td>\n",
       "      <td>Consultant / Auditeur Data Protection – Data P...</td>\n",
       "      <td>Paris (75)</td>\n",
       "      <td>Contexte\\nLe RGPD a ouvert une nouvelle ère : ...</td>\n",
       "      <td></td>\n",
       "      <td>19/07/2019</td>\n",
       "      <td></td>\n",
       "      <td>https://www.indeed.fr/pagead/clk?mo=r&amp;ad=-6NYl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5077</th>\n",
       "      <td>Groupe Pierre &amp; Vacances - Center Parcs</td>\n",
       "      <td>Data Business Analyst</td>\n",
       "      <td>Paris (75)</td>\n",
       "      <td>Nous recherchons un business data analyst pass...</td>\n",
       "      <td>CDI</td>\n",
       "      <td>19/07/2019</td>\n",
       "      <td></td>\n",
       "      <td>https://www.indeed.fr/pagead/clk?mo=r&amp;ad=-6NYl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      company  \\\n",
       "5073  SILLIKER - MERIEUX NUTRISCIENCES FRANCE   \n",
       "5074                         OUTREMER TELECOM   \n",
       "5075                                 Teradata   \n",
       "5076                                 Novencia   \n",
       "5077  Groupe Pierre & Vacances - Center Parcs   \n",
       "\n",
       "                                              job_title             location  \\\n",
       "5073  Data Scientist - BIOFORTIS - CDI - Saint-Herbl...  Saint-Herblain (44)   \n",
       "5074                                  Ingénieur IP Data  Fort-de-France (MQ)   \n",
       "5075                                     Data Developer         92160 Antony   \n",
       "5076  Consultant / Auditeur Data Protection – Data P...           Paris (75)   \n",
       "5077                              Data Business Analyst           Paris (75)   \n",
       "\n",
       "                                              pos_descr pos_type    pub_date  \\\n",
       "5073  #JobAlert #Stats #DataScientist #Nantes #Mérie...      CDI  01/08/2019   \n",
       "5074  Outremer Telecom, filiale du groupe Altice, so...      CDI  19/07/2019   \n",
       "5075  Requisition Number:\\n205033\\n\\nPosition Title:...           19/07/2019   \n",
       "5076  Contexte\\nLe RGPD a ouvert une nouvelle ère : ...           19/07/2019   \n",
       "5077  Nous recherchons un business data analyst pass...      CDI  19/07/2019   \n",
       "\n",
       "               salary                                                url  \n",
       "5073  40 000 € par an  https://www.indeed.fr/pagead/clk?mo=r&ad=-6NYl...  \n",
       "5074                   https://www.indeed.fr/pagead/clk?mo=r&ad=-6NYl...  \n",
       "5075                   https://www.indeed.fr/pagead/clk?mo=r&ad=-6NYl...  \n",
       "5076                   https://www.indeed.fr/pagead/clk?mo=r&ad=-6NYl...  \n",
       "5077                   https://www.indeed.fr/pagead/clk?mo=r&ad=-6NYl...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_indeed_full.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The web scraping job is complete !**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>II) Using Bokeh to build a custom aggregated job board</h2>\n",
    "<a id=\"2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before jumping directly into Bokeh, it is necessary to perform some text mining on the raw data to identify and isolate useful features. These sub-chapter might be a bit redundant with the chapter 3 of the notebook and will be reorganised later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1) Preprocessing the raw data</h3>\n",
    "<a id=\"2.1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>a) Data cleaning</h4>\n",
    "<a id=\"2.1.a\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's import the job databases for each job board and proceed to some data cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data - apec: 18 job offers\n",
      "Raw data - indeed: 94 job offers\n"
     ]
    }
   ],
   "source": [
    "DB_UNION_SAVING_PATH = os.path.join(DB_SAVING_DIR, 'union_database.xlsx') #union of individual databases\n",
    "\n",
    "dateparse = lambda x: pd.datetime.strptime(x, '%d/%m/%Y') #to parse date format when importing the dataframe\n",
    "df={} # initiate the dict for the job database\n",
    "update = os.path.isfile(DB_UNION_SAVING_PATH)\n",
    "if update:\n",
    "    file_name_pat='\\*_new.xlsx' #only *_new.xlsx files (preprocessing task has been previously executed on an old db)\n",
    "else:\n",
    "    file_name_pat='\\*_db.xlsx' #only *_db.xlsx files (preprocessing task has never been executed before)\n",
    "    \n",
    "for filepath in glob.glob(DB_SAVING_DIR + file_name_pat):\n",
    "    name_origin = re.search(r'\\w+(?=_(db|new)\\.xlsx)', filepath).group()\n",
    "    df[name_origin] = pd.read_excel(filepath, parse_dates=['pub_date'], date_parser=dateparse)\n",
    "    df[name_origin]['origin'] = name_origin\n",
    "    print(\"Raw data - {}: {} job offers\".format(name_origin, len(df[name_origin])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After filtering - apec: 7 job offers\n",
      "After filtering - indeed: 90 job offers\n"
     ]
    }
   ],
   "source": [
    "mask={} #initiate the dict for boolean mask filtering\n",
    "mask['apec'] = df['apec']['job_title'].map(lambda x: re.match(r'.*\\b(data|données?|bi|business intelligence)\\b.*',\n",
    "                                                              str(x), flags=re.IGNORECASE)).notnull()\n",
    "mask['indeed'] = df['indeed']['pos_type'].map(lambda x: re.match(r'.*\\b(stage|alternance|apprentissage)\\b.*',\n",
    "                                                              str(x), flags=re.IGNORECASE)).isnull()\n",
    "for key in df.keys():\n",
    "    df[key] = df[key][mask[key]]\n",
    "    print(\"After filtering - {}: {} job offers\".format(key, len(df[key])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Job description for the APEC job board is separated into several columns. To have an harmonised and centralized aggregated database, it is necessary to reunite these features into a single one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['apec']['pos_descr'] = df['apec']['comp_info'] + '\\n' + df['apec']['pos_descr'] + '\\n' + df['apec']['profil']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>b) Feature engineering</h4>\n",
    "<a id=\"2.1.b\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>**City and Department**</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The location attribute of each dataframe can be transformed into two seperated attributes : City and Department. The regex pattern varies accros job boards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "apec_loc_regex_pat = r'\\s-\\s(?=\\d{2,})'\n",
    "indeed_loc_regex_pat = r'\\s\\(|\\)'\n",
    "\n",
    "def get_city_and_dept_apec(location):\n",
    "    \"\"\"Return city and departement from location attribute for the APEC database\"\"\"\n",
    "    regex_pat = apec_loc_regex_pat\n",
    "    try:\n",
    "        match_split = re.split(regex_pat, location)\n",
    "        city, dept = match_split[0], match_split[1]\n",
    "    except:\n",
    "        city, dept = np.NaN, np.NaN\n",
    "    return city, dept\n",
    "\n",
    "def get_city_and_dept_indeed(location):\n",
    "    \"\"\"Return city and departement from location attribute for the APEC database\"\"\"\n",
    "    regex_pat = indeed_loc_regex_pat\n",
    "    try:\n",
    "        match_split = re.split(regex_pat, location)\n",
    "        if len(match_split[0]) != len(location):\n",
    "            city, dept = match_split[0], match_split[1]\n",
    "        else:\n",
    "            unc_pat = r'(?<=\\d{5})\\s' #uncommon (old?) pattern that could be encountered\n",
    "            match_split = re.split(unc_pat, str(location))\n",
    "            dept, city = str(match_split[0])[:2], match_split[1]\n",
    "    except:\n",
    "        city, dept = np.NaN, np.NaN\n",
    "    return city, dept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['apec']['city'], df['apec']['dept'] = zip(*df['apec']['location'].map(get_city_and_dept_apec))\n",
    "df['indeed']['city'], df['indeed']['dept'] = zip(*df['indeed']['location'].map(get_city_and_dept_indeed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that everything is fine for the city and department features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paris 08     2\n",
      "Lyon 07      1\n",
      "Talence      1\n",
      "Paris 02     1\n",
      "Élancourt    1\n",
      "Name: city, dtype: int64\n",
      "Paris                 37\n",
      "Antony                 4\n",
      "Saint-Paul-lès-Dax     4\n",
      "Lyon                   4\n",
      "Fort-de-France         3\n",
      "Name: city, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df['apec']['city'].value_counts()[:5]) \n",
    "print(df['indeed']['city'].value_counts()[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75    4\n",
      "33    1\n",
      "69    1\n",
      "78    1\n",
      "Name: dept, dtype: int64\n",
      "75    40\n",
      "92    11\n",
      "69     8\n",
      "44     5\n",
      "40     4\n",
      "Name: dept, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df['apec']['dept'].value_counts()[:5]) \n",
    "print(df['indeed']['dept'].value_counts()[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now, the different job board database can be aggregated into a single one : df_union !** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_union = pd.concat([df[key] for key in df.keys()], ignore_index=True, sort=False)\n",
    "len(df_union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>act_area</th>\n",
       "      <th>act_date</th>\n",
       "      <th>btrip_area</th>\n",
       "      <th>comp_info</th>\n",
       "      <th>company</th>\n",
       "      <th>empl_date</th>\n",
       "      <th>job_title</th>\n",
       "      <th>location</th>\n",
       "      <th>nb_pos</th>\n",
       "      <th>pos_descr</th>\n",
       "      <th>...</th>\n",
       "      <th>recruit_proc</th>\n",
       "      <th>recruit_resp</th>\n",
       "      <th>ref_comp</th>\n",
       "      <th>ref_site</th>\n",
       "      <th>req_exp</th>\n",
       "      <th>salary</th>\n",
       "      <th>url</th>\n",
       "      <th>origin</th>\n",
       "      <th>city</th>\n",
       "      <th>dept</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Activités des sièges sociaux</td>\n",
       "      <td>17/08/2019</td>\n",
       "      <td>Pas de déplacement</td>\n",
       "      <td>Entreprise\\nQUI SOMMES-NOUS ?\\nL'activité Syst...</td>\n",
       "      <td>Thales</td>\n",
       "      <td>Dès que possible</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Élancourt - 78</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Entreprise\\nQUI SOMMES-NOUS ?\\nL'activité Syst...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>R0064534</td>\n",
       "      <td>164483014W</td>\n",
       "      <td>Minimum 2 ans</td>\n",
       "      <td>A négocier</td>\n",
       "      <td>https://cadres.apec.fr/offres-emploi-cadres/0_...</td>\n",
       "      <td>apec</td>\n",
       "      <td>Élancourt</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Autres organisations fonctionnant par adhésio...</td>\n",
       "      <td>16/08/2019</td>\n",
       "      <td>Pas de déplacement</td>\n",
       "      <td>Entreprise\\nBIOASTER est l'unique Institut de ...</td>\n",
       "      <td>BIOASTER</td>\n",
       "      <td>Dès que possible</td>\n",
       "      <td>R&amp;D DATA MANAGER / DATA ENGINEER</td>\n",
       "      <td>Lyon 07 - 69</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Entreprise\\nBIOASTER est l'unique Institut de ...</td>\n",
       "      <td>...</td>\n",
       "      <td>Processus de recrutement\\nPersonne en charge d...</td>\n",
       "      <td>Accompagner quotidiennement les scientifiques ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>164482537W</td>\n",
       "      <td>Minimum 2 ans</td>\n",
       "      <td>A négocier</td>\n",
       "      <td>https://cadres.apec.fr/offres-emploi-cadres/0_...</td>\n",
       "      <td>apec</td>\n",
       "      <td>Lyon 07</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Formation continue d'adultes</td>\n",
       "      <td>16/08/2019</td>\n",
       "      <td>Pas de déplacement</td>\n",
       "      <td>Entreprise\\nNotre Client est une ONG (Organisa...</td>\n",
       "      <td>MCP CONSEIL</td>\n",
       "      <td>Dès que possible</td>\n",
       "      <td>Marketing Data Analyst</td>\n",
       "      <td>Paris 01 - 75</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Entreprise\\nNotre Client est une ONG (Organisa...</td>\n",
       "      <td>...</td>\n",
       "      <td>Processus de recrutement\\nPersonne en charge d...</td>\n",
       "      <td>Charlène Ureta - Dirigeant de la société</td>\n",
       "      <td>NaN</td>\n",
       "      <td>164482266W</td>\n",
       "      <td>Minimum 3 ans</td>\n",
       "      <td>36 - 38 k€ brut annuel</td>\n",
       "      <td>https://cadres.apec.fr/offres-emploi-cadres/0_...</td>\n",
       "      <td>apec</td>\n",
       "      <td>Paris 01</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Conseil en systèmes et logiciels informatiques</td>\n",
       "      <td>16/08/2019</td>\n",
       "      <td>Régionale</td>\n",
       "      <td>Entreprise\\nALTIMA, CRÉATEUR D'EXPÉRIENCE(S) E...</td>\n",
       "      <td>Altima</td>\n",
       "      <td>Dès que possible</td>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Paris 02 - 75</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Entreprise\\nALTIMA, CRÉATEUR D'EXPÉRIENCE(S) E...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>talentplug</td>\n",
       "      <td>164481971W</td>\n",
       "      <td>Tous niveaux d'expérience acceptés</td>\n",
       "      <td>30 - 45 k€ brut annuel</td>\n",
       "      <td>https://cadres.apec.fr/offres-emploi-cadres/0_...</td>\n",
       "      <td>apec</td>\n",
       "      <td>Paris 02</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Formation continue d'adultes</td>\n",
       "      <td>14/08/2019</td>\n",
       "      <td>Pas de déplacement</td>\n",
       "      <td>Entreprise\\nTu souhaites devenir développeur s...</td>\n",
       "      <td>LA PISCINE BORDEAUX</td>\n",
       "      <td>Dès que possible</td>\n",
       "      <td>Forme toi et deviens Développeur spécialité DA...</td>\n",
       "      <td>Talence - 33</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Entreprise\\nTu souhaites devenir développeur s...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>164481351W</td>\n",
       "      <td>Tous niveaux d'expérience acceptés</td>\n",
       "      <td>A négocier</td>\n",
       "      <td>https://cadres.apec.fr/offres-emploi-cadres/0_...</td>\n",
       "      <td>apec</td>\n",
       "      <td>Talence</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            act_area    act_date  \\\n",
       "0                       Activités des sièges sociaux  17/08/2019   \n",
       "1   Autres organisations fonctionnant par adhésio...  16/08/2019   \n",
       "2                       Formation continue d'adultes  16/08/2019   \n",
       "3     Conseil en systèmes et logiciels informatiques  16/08/2019   \n",
       "4                       Formation continue d'adultes  14/08/2019   \n",
       "\n",
       "            btrip_area                                          comp_info  \\\n",
       "0   Pas de déplacement  Entreprise\\nQUI SOMMES-NOUS ?\\nL'activité Syst...   \n",
       "1   Pas de déplacement  Entreprise\\nBIOASTER est l'unique Institut de ...   \n",
       "2   Pas de déplacement  Entreprise\\nNotre Client est une ONG (Organisa...   \n",
       "3            Régionale  Entreprise\\nALTIMA, CRÉATEUR D'EXPÉRIENCE(S) E...   \n",
       "4   Pas de déplacement  Entreprise\\nTu souhaites devenir développeur s...   \n",
       "\n",
       "               company          empl_date  \\\n",
       "0               Thales   Dès que possible   \n",
       "1             BIOASTER   Dès que possible   \n",
       "2          MCP CONSEIL   Dès que possible   \n",
       "3               Altima   Dès que possible   \n",
       "4  LA PISCINE BORDEAUX   Dès que possible   \n",
       "\n",
       "                                           job_title        location  nb_pos  \\\n",
       "0                                       Data Analyst  Élancourt - 78     1.0   \n",
       "1                   R&D DATA MANAGER / DATA ENGINEER    Lyon 07 - 69     1.0   \n",
       "2                             Marketing Data Analyst   Paris 01 - 75     1.0   \n",
       "3                                      Data Engineer   Paris 02 - 75     2.0   \n",
       "4  Forme toi et deviens Développeur spécialité DA...    Talence - 33     1.0   \n",
       "\n",
       "                                           pos_descr  ...  \\\n",
       "0  Entreprise\\nQUI SOMMES-NOUS ?\\nL'activité Syst...  ...   \n",
       "1  Entreprise\\nBIOASTER est l'unique Institut de ...  ...   \n",
       "2  Entreprise\\nNotre Client est une ONG (Organisa...  ...   \n",
       "3  Entreprise\\nALTIMA, CRÉATEUR D'EXPÉRIENCE(S) E...  ...   \n",
       "4  Entreprise\\nTu souhaites devenir développeur s...  ...   \n",
       "\n",
       "                                        recruit_proc  \\\n",
       "0                                                NaN   \n",
       "1  Processus de recrutement\\nPersonne en charge d...   \n",
       "2  Processus de recrutement\\nPersonne en charge d...   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "\n",
       "                                        recruit_resp    ref_comp    ref_site  \\\n",
       "0                                                NaN    R0064534  164483014W   \n",
       "1  Accompagner quotidiennement les scientifiques ...         NaN  164482537W   \n",
       "2           Charlène Ureta - Dirigeant de la société         NaN  164482266W   \n",
       "3                                                NaN  talentplug  164481971W   \n",
       "4                                                NaN         NaN  164481351W   \n",
       "\n",
       "                               req_exp                   salary  \\\n",
       "0                        Minimum 2 ans               A négocier   \n",
       "1                        Minimum 2 ans               A négocier   \n",
       "2                        Minimum 3 ans   36 - 38 k€ brut annuel   \n",
       "3   Tous niveaux d'expérience acceptés   30 - 45 k€ brut annuel   \n",
       "4   Tous niveaux d'expérience acceptés               A négocier   \n",
       "\n",
       "                                                 url origin       city dept  \n",
       "0  https://cadres.apec.fr/offres-emploi-cadres/0_...   apec  Élancourt   78  \n",
       "1  https://cadres.apec.fr/offres-emploi-cadres/0_...   apec    Lyon 07   69  \n",
       "2  https://cadres.apec.fr/offres-emploi-cadres/0_...   apec   Paris 01   75  \n",
       "3  https://cadres.apec.fr/offres-emploi-cadres/0_...   apec   Paris 02   75  \n",
       "4  https://cadres.apec.fr/offres-emploi-cadres/0_...   apec    Talence   33  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_union.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, there are still a few preprocissing steps to go through for df_union. Let's continue text mining and feature engineering to fetch :\n",
    "* France region; \n",
    "* Average salary (salary is often provided as a string with an interval);\n",
    "* Category of data-related position (e.g.: data engineer, analyst, scientist ...).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>**Region**</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To add the Region feature, we will use a left join with a correspondence table for department and region in France. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEPT_PATH = './dpt_reg_fr.csv'\n",
    "DEPT_DL_URL = 'https://raw.githubusercontent.com/GuillaumeHarel/portfolio-projects/master/'\\\n",
    "              '%231_Webscraping_%26_Bokeh/Auxiliary_files/Dept_Region_France.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "urllib.request.urlretrieve(DEPT_DL_URL, DEPT_PATH)\n",
    "df_dept_reg = pd.read_csv(DEPT_PATH, dtype={'regionCode': 'Int32'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>departmentCode</th>\n",
       "      <th>departmentName</th>\n",
       "      <th>regionCode</th>\n",
       "      <th>regionName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01</td>\n",
       "      <td>Ain</td>\n",
       "      <td>84</td>\n",
       "      <td>Auvergne-Rhône-Alpes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>02</td>\n",
       "      <td>Aisne</td>\n",
       "      <td>32</td>\n",
       "      <td>Hauts-de-France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>03</td>\n",
       "      <td>Allier</td>\n",
       "      <td>84</td>\n",
       "      <td>Auvergne-Rhône-Alpes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>04</td>\n",
       "      <td>Alpes-de-Haute-Provence</td>\n",
       "      <td>93</td>\n",
       "      <td>Provence-Alpes-Côte d'Azur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>05</td>\n",
       "      <td>Hautes-Alpes</td>\n",
       "      <td>93</td>\n",
       "      <td>Provence-Alpes-Côte d'Azur</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  departmentCode           departmentName  regionCode  \\\n",
       "0             01                      Ain          84   \n",
       "1             02                    Aisne          32   \n",
       "2             03                   Allier          84   \n",
       "3             04  Alpes-de-Haute-Provence          93   \n",
       "4             05             Hautes-Alpes          93   \n",
       "\n",
       "                   regionName  \n",
       "0        Auvergne-Rhône-Alpes  \n",
       "1             Hauts-de-France  \n",
       "2        Auvergne-Rhône-Alpes  \n",
       "3  Provence-Alpes-Côte d'Azur  \n",
       "4  Provence-Alpes-Côte d'Azur  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_dept_reg.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_union = pd.merge(df_union, df_dept_reg[['departmentCode', 'departmentName', 'regionName']],\n",
    "                    how='left', left_on='dept', right_on='departmentCode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['act_area', 'act_date', 'btrip_area', 'comp_info', 'company',\n",
       "       'empl_date', 'job_title', 'location', 'nb_pos', 'pos_descr', 'pos_stt',\n",
       "       'pos_type', 'profil', 'pub_date', 'recruit_proc', 'recruit_resp',\n",
       "       'ref_comp', 'ref_site', 'req_exp', 'salary', 'url', 'origin', 'city',\n",
       "       'dept', 'departmentCode', 'departmentName', 'regionName'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_union.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_union.drop('departmentCode', 1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>**Average salary**</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the average salary feature it is necessary to deal with the multiple possible salary string formats coming from the Indeed job board. String format expressions can vary on time period (e.g. hour, week, year ...), on digit format (e.g. 32 k€ VS 32 000 €) or even on job contract types (e.g. Employed VS Freelancer). \n",
    "<br>\n",
    "I did my best to automate the process with regards to this high variability.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_salary(salary):\n",
    "    \"\"\"Return the average salary from the text salary attribute (mostly corresponding to an interval)\"\"\"\n",
    "    match = re.findall(r\"(\\d+)\", re.sub(r\"(?<=\\d)\\s+(?=\\d)\", \"\", str(salary))) #re.sub to remove empty space in digits\n",
    "    if match != None:\n",
    "        try:\n",
    "            sal_freq = ['an', 'mois', 'semaine', 'jour', 'heure']\n",
    "            sal_factor = [1, 12, 52.14, 228/1.5, 35 * 52.14]\n",
    "            freq_res = re.search(r'|'.join(sal_freq), str(salary)).group()\n",
    "            sal_avg = np.array(match).astype(np.float).mean() * sal_factor[sal_freq.index(freq_res)]\n",
    "            if sal_avg > 1000:\n",
    "                sal_avg /= 1000\n",
    "            assert sal_avg > 20 and sal_avg < 200\n",
    "            return sal_avg\n",
    "        except:\n",
    "            return np.NaN\n",
    "    else:\n",
    "        return np.NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_union['avg_sal'] = df_union['salary'].map(get_salary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>**Job categories**</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eventually, let's try to create the data-related position category feature from the job title one.\n",
    "<br>\n",
    "<br>\n",
    "I made arbitrary choice to categorize this new attribute. I am mostly interested by job offers in data science and analysis. As a result, I gave less importance to details for other data-related job categories. In particular, it can be noticed that the Big Data category might aggregate several type of trades that are inherently different. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_cat_pat = [['(scientist', 'science', 'learning', 'miner', 'mining)'],\n",
    "               ['(analyst', r'\\bbi\\b', 'marketing', 'business)'],\n",
    "               [r'(\\bbig\\b', 'data engineer', r'architecte?', r'd[e,é]v', 'data ingénieur', 'ingénieur data)'],\n",
    "               [r'(projec?t)'],\n",
    "               ['(data manager', 'manager data', 'gestionnaire', 'steward', 'governance)']]\n",
    "job_cat_pat_flat = [item for sublist in job_cat_pat for item in sublist]\n",
    "\n",
    "job_cat_labels = [\"Data scientist\",\n",
    "                  \"Data analyst & BI\",\n",
    "                  \"Big Data (engineer, dev, archi)\",\n",
    "                  \"IT Project Manager (data related)\",\n",
    "                  \"Data Manager/Officer\",\n",
    "                  \"Unclassified\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cat_dummies(job_title):\n",
    "    \"\"\"From job_title string return a list of dummies corresponding to data-related job categories\"\"\"\n",
    "    dummies = [0] * len(job_cat_pat)\n",
    "    job_res = re.finditer('|'.join(job_cat_pat_flat), job_title, flags=re.IGNORECASE)\n",
    "    for match in job_res:\n",
    "        ind = match.groups().index(match.group())\n",
    "        dummies[ind] = 1\n",
    "    if not any(dummies): #no match, \"Others\" category\n",
    "        dummies.append(1)\n",
    "    else:\n",
    "        dummies.append(0)\n",
    "    return dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_union['job_cat'] = df_union['job_title'].map(get_cat_dummies)\n",
    "df_union[job_cat_labels] = pd.DataFrame(df_union['job_cat'].values.tolist(), index=df_union.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data scientist</th>\n",
       "      <th>Data analyst &amp; BI</th>\n",
       "      <th>Big Data (engineer, dev, archi)</th>\n",
       "      <th>IT Project Manager (data related)</th>\n",
       "      <th>Data Manager/Officer</th>\n",
       "      <th>Unclassified</th>\n",
       "      <th>job_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Data Analyst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>R&amp;D DATA MANAGER / DATA ENGINEER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Marketing Data Analyst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Data Engineer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Forme toi et deviens Développeur spécialité DA...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Data scientist  Data analyst & BI  Big Data (engineer, dev, archi)  \\\n",
       "0               0                  1                                0   \n",
       "1               0                  0                                1   \n",
       "2               0                  1                                0   \n",
       "3               0                  0                                1   \n",
       "4               0                  1                                1   \n",
       "\n",
       "   IT Project Manager (data related)  Data Manager/Officer  Unclassified  \\\n",
       "0                                  0                     0             0   \n",
       "1                                  0                     1             0   \n",
       "2                                  0                     0             0   \n",
       "3                                  0                     0             0   \n",
       "4                                  0                     0             0   \n",
       "\n",
       "                                           job_title  \n",
       "0                                       Data Analyst  \n",
       "1                   R&D DATA MANAGER / DATA ENGINEER  \n",
       "2                             Marketing Data Analyst  \n",
       "3                                      Data Engineer  \n",
       "4  Forme toi et deviens Développeur spécialité DA...  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_union[job_cat_labels + ['job_title'] ].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In prevision of the Bokeh App, there are two last attributes to add to df_union :\n",
    "* the dataframe index as a column to display in Bokeh a unique ID key number for every offer;\n",
    "* the text similiraty between a resume/CV and the offer (similarity will be computed later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_union['index_0'] = df_union.index\n",
    "df_union['sim'] = np.NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>**Text similarity**</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the text similarity metric between a resume and a job offer I decided to work with the Dice similarity.\n",
    "<br>\n",
    "The Dice similarity metric measure the similarity between two documents d1 and d2 by relying on the number of common terms between d1 and d2.\n",
    "<br>\n",
    "<br>\n",
    "**<center>SimDice(d1, d2) = 2 Nc / (N1 + N2)</center>**\n",
    "<br>\n",
    "*Where Nc stands for the number of common words between d1 and d2 and N1 and N2 respectively correspond to the number of terms in d1 and d2.*\n",
    "<br>\n",
    "<br>\n",
    "The length of job offers may vary a lot in our job offer corpus (see below), and so its number of words, which would have a strong impact on the Dice metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "offer_word_number = df_union['pos_descr'].map(lambda x: len(str(x).split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1cebcd12160>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEKCAYAAAAVaT4rAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAFYJJREFUeJzt3X+0ZWV93/H3R35og6SAXAgC45iUEsGWgd7yM1oEQWRRkVYraCOxuCa6YBUaXClJ1hJL/olNxdaQQilMIakQq0IkBkGKNIDBgRkYYBAIFAiMQ5kho/yQVDv47R9nXzxzOXfu5Zwz947zvF9rnXX2fvaz9/Pcvfb93H33OfvZqSokSe143UJ3QJI0vwx+SWqMwS9JjTH4JakxBr8kNcbgl6TGGPyS1BiDX5IaY/BLUmO2X+gODLL77rvX4sWLF7obkvQzY+XKlc9W1cRc6m6Vwb948WJWrFix0N2QpJ8ZSf56rnW91CNJjTH4JakxBr8kNcbgl6TGGPyS1BiDX5IaY/BLUmMMfklqjMEvSY3ZKu/c1fy5avmTC92FgT582KKF7oK0zfKMX5IaY/BLUmMMfklqjMEvSY0x+CWpMbMGf5J9k9yS5MEkDyQ5uyvfLclNSR7p3nedYf3TuzqPJDl93D+AJOm1mcsZ/0bg3Kp6G3A4cGaSA4DzgJuraj/g5m5+E0l2A84HDgMOBc6f6Q+EJGl+zBr8VfV0Vd3dTb8APAjsDZwMXNlVuxJ4/4DV3wPcVFUbqur7wE3ACePouCRpOK/pGn+SxcDBwHJgz6p6Gnp/HIA9BqyyN/BU3/yarkyStEDmfOdukjcCXwXOqarnk8xptQFlNcP2lwJLARYt2vbu2txa75DdWm2t+8s7irUtmNMZf5Id6IX+F6vqmq74mSR7dcv3AtYNWHUNsG/f/D7A2kFtVNWlVTVZVZMTE3N6ULwkaQhz+VZPgMuBB6vqwr5F1wFT39I5HfjagNVvBI5Psmv3oe7xXZkkaYHM5Yz/KOBXgWOSrOpeJwK/BxyX5BHguG6eJJNJLgOoqg3A7wJ3da8LujJJ0gKZ9Rp/Vd3O4Gv1AMcOqL8C+Hjf/DJg2bAdlCSNl3fuSlJjDH5JaozBL0mNMfglqTEGvyQ1xuCXpMYY/JLUGINfkhpj8EtSYwx+SWqMwS9JjTH4JakxBr8kNcbgl6TGGPyS1BiDX5IaM+uDWJIsA04C1lXV27uyLwH7d1V2AX5QVUsGrPsE8ALwMrCxqibH1G9J0pBmDX7gCuAi4I+mCqrqQ1PTST4HPLeZ9d9VVc8O20FJ0njN5dGLtyZZPGhZ9yD2fwEcM95uSZK2lFGv8b8DeKaqHplheQHfTLIyydLNbSjJ0iQrkqxYv379iN2SJM1k1OA/Dbh6M8uPqqpDgPcCZyZ550wVq+rSqpqsqsmJiYkRuyVJmsnQwZ9ke+CfAV+aqU5Vre3e1wHXAocO254kaTxGOeN/N/BQVa0ZtDDJTkl2npoGjgdWj9CeJGkMZg3+JFcDdwD7J1mT5Ixu0alMu8yT5M1Jru9m9wRuT3IvcCfw51V1w/i6Lkkaxly+1XPaDOW/NqBsLXBiN/0YcNCI/ZMkjZl37kpSYwx+SWqMwS9JjTH4JakxBr8kNcbgl6TGGPyS1BiDX5IaY/BLUmMMfklqjMEvSY0x+CWpMQa/JDXG4Jekxhj8ktSYuTyIZVmSdUlW95V9Jsn3kqzqXifOsO4JSR5O8miS88bZcUnScOZyxn8FcMKA8s9X1ZLudf30hUm2A/6Q3oPWDwBOS3LAKJ2VJI1u1uCvqluBDUNs+1Dg0ap6rKp+DPwJcPIQ25EkjdEo1/jPSnJfdylo1wHL9wae6ptf05VJkhbQsMF/MfBLwBLgaeBzA+pkQFnNtMEkS5OsSLJi/fr1Q3ZLkjSboYK/qp6pqper6ifAf6V3WWe6NcC+ffP7AGs3s81Lq2qyqiYnJiaG6ZYkaQ6GCv4ke/XNngKsHlDtLmC/JG9NsiNwKnDdMO1JksZn+9kqJLkaOBrYPcka4Hzg6CRL6F26eQL49a7um4HLqurEqtqY5CzgRmA7YFlVPbBFfgpJ0pzNGvxVddqA4stnqLsWOLFv/nrgVV/1lCQtHO/claTGGPyS1BiDX5IaY/BLUmMMfklqjMEvSY0x+CWpMQa/JDXG4Jekxhj8ktQYg1+SGmPwS1JjDH5JaozBL0mNMfglqTEGvyQ1ZtbgT7Isybokq/vKfj/JQ0nuS3Jtkl1mWPeJJPcnWZVkxTg7LkkazlzO+K8ATphWdhPw9qr6h8BfAb+1mfXfVVVLqmpyuC5KksZp1uCvqluBDdPKvllVG7vZ7wD7bIG+SZK2gHFc4/9XwDdmWFbAN5OsTLJ0DG1JkkY068PWNyfJ7wAbgS/OUOWoqlqbZA/gpiQPdf9BDNrWUmApwKJFi0bpliRpM4Y+409yOnAS8JGqqkF1qmpt974OuBY4dKbtVdWlVTVZVZMTExPDdkuSNIuhgj/JCcC/Bd5XVS/NUGenJDtPTQPHA6sH1ZUkzZ+5fJ3zauAOYP8ka5KcAVwE7Ezv8s2qJJd0dd+c5Ppu1T2B25PcC9wJ/HlV3bBFfgpJ0pzNeo2/qk4bUHz5DHXXAid2048BB43UO0lzctXyJxe6CwN9+DA/r9saeeeuJDXG4Jekxhj8ktQYg1+SGmPwS1JjDH5JaozBL0mNMfglqTEGvyQ1ZqTROaXWbK13yEqvhWf8ktQYg1+SGmPwS1JjDH5JaozBL0mNMfglqTFzCv4ky5KsS7K6r2y3JDcleaR733WGdU/v6jzSPadXkrSA5nrGfwVwwrSy84Cbq2o/4OZufhNJdgPOBw6j96D182f6AyFJmh9zCv6quhXYMK34ZODKbvpK4P0DVn0PcFNVbaiq7wM38eo/IJKkeTTKNf49q+ppgO59jwF19gae6ptf05W9SpKlSVYkWbF+/foRuiVJ2pwt/eFuBpTVoIpVdWlVTVbV5MTExBbuliS1a5TgfybJXgDd+7oBddYA+/bN7wOsHaFNSdKIRgn+64Cpb+mcDnxtQJ0bgeOT7Np9qHt8VyZJWiBz/Trn1cAdwP5J1iQ5A/g94LgkjwDHdfMkmUxyGUBVbQB+F7ire13QlUmSFsichmWuqtNmWHTsgLorgI/3zS8Dlg3VO0nS2HnnriQ1xuCXpMYY/JLUGINfkhpj8EtSYwx+SWqMwS9JjTH4JakxBr8kNcbgl6TGGPyS1BiDX5IaY/BLUmMMfklqzJyGZf5ZctXyJxe6C5K0VRv6jD/J/klW9b2eT3LOtDpHJ3mur86nR++yJGkUQ5/xV9XDwBKAJNsB3wOuHVD1tqo6adh2JEnjNa5r/McC/7uq/npM25MkbSHjCv5TgatnWHZEknuTfCPJgWNqT5I0pJGDP8mOwPuALw9YfDfwlqo6CPgD4E83s52lSVYkWbF+/fpRuyVJmsE4zvjfC9xdVc9MX1BVz1fVi9309cAOSXYftJGqurSqJqtqcmJiYgzdkiQNMo7gP40ZLvMk+YUk6aYP7dr7mzG0KUka0kjf40/yc8BxwK/3lX0CoKouAT4AfDLJRuBvgVOrqkZpU5I0mpGCv6peAt40reySvumLgItGaUOSNF4O2SBJjTH4JakxBr8kNcbgl6TGGPyS1BiDX5IaY/BLUmMMfklqjMEvSY0x+CWpMQa/JDXG4Jekxhj8ktQYg1+SGmPwS1JjDH5Jasw4Hrb+RJL7k6xKsmLA8iT5QpJHk9yX5JBR25QkDW+kJ3D1eVdVPTvDsvcC+3Wvw4CLu3dJ0gKYj0s9JwN/VD3fAXZJstc8tCtJGmAcwV/AN5OsTLJ0wPK9gaf65td0ZZtIsjTJiiQr1q9fP4ZuSZIGGUfwH1VVh9C7pHNmkndOW54B69SrCqourarJqpqcmJgYQ7ckSYOMHPxVtbZ7XwdcCxw6rcoaYN+++X2AtaO2K0kazkjBn2SnJDtPTQPHA6unVbsO+Gj37Z7Dgeeq6ulR2pUkDW/Ub/XsCVybZGpbV1XVDUk+AVBVlwDXAycCjwIvAR8bsU1J0ghGCv6qegw4aED5JX3TBZw5SjuSpPHxzl1JaozBL0mNMfglqTEGvyQ1xuCXpMYY/JLUGINfkhpj8EtSYwx+SWqMwS9JjTH4JakxBr8kNcbgl6TGGPyS1JhRx+OXpJ85Vy1/cqG7MNCHD1s0L+0MfcafZN8ktyR5MMkDSc4eUOfoJM8lWdW9Pj1adyVJoxrljH8jcG5V3d09fnFlkpuq6rvT6t1WVSeN0I4kaYyGPuOvqqer6u5u+gXgQWDvcXVMkrRljOXD3SSLgYOB5QMWH5Hk3iTfSHLgONqTJA1v5A93k7wR+CpwTlU9P23x3cBbqurFJCcCfwrsN8N2lgJLARYtmp8POCSpRSOd8SfZgV7of7Gqrpm+vKqer6oXu+nrgR2S7D5oW1V1aVVNVtXkxMTEKN2SJG3GKN/qCXA58GBVXThDnV/o6pHk0K69vxm2TUnS6Ea51HMU8KvA/UlWdWW/DSwCqKpLgA8An0yyEfhb4NSqqhHalCSNaOjgr6rbgcxS5yLgomHbkCSNn3fuStpittY7ZFvnWD2S1BiDX5IaY/BLUmMMfklqjMEvSY0x+CWpMQa/JDXG4Jekxhj8ktQYg1+SGmPwS1JjDH5JaozBL0mNMfglqTEGvyQ1ZtRn7p6Q5OEkjyY5b8Dy1yf5Urd8eZLFo7QnSRrdKM/c3Q74Q+C9wAHAaUkOmFbtDOD7VfX3gM8Dnx22PUnSeIxyxn8o8GhVPVZVPwb+BDh5Wp2TgSu76a8Ax049fF2StDBGCf69gaf65td0ZQPrVNVG4DngTSO0KUka0SjP3B105l5D1OlVTJYCS7vZF5M8PELfthW7A88udCe2cu6j2bmPNm+r2T8fGW31t8y14ijBvwbYt29+H2DtDHXWJNke+LvAhkEbq6pLgUtH6M82J8mKqppc6H5szdxHs3MfbV6L+2eUSz13AfsleWuSHYFTgeum1bkOOL2b/gDwraoaeMYvSZofQ5/xV9XGJGcBNwLbAcuq6oEkFwArquo64HLgj5M8Su9M/9RxdFqSNLxRLvVQVdcD108r+3Tf9P8FPjhKG43z0tfs3Eezcx9tXnP7J155kaS2OGSDJDXG4F9ASfZNckuSB5M8kOTsrny3JDcleaR737UrT5IvdENg3JfkkIX9CeZHku2S3JPk6938W7shQB7phgTZsStvcoiQJLsk+UqSh7pj6QiPoU0l+Tfd79jqJFcneUPLx5HBv7A2AudW1duAw4Ezu2EvzgNurqr9gJu7eegNj7Ff91oKXDz/XV4QZwMP9s1/Fvh8t3++T29oEGh3iJD/BNxQVb8MHERvX3kMdZLsDfxrYLKq3k7vyyin0vJxVFW+tpIX8DXgOOBhYK+ubC/g4W76vwCn9dV/pd62+qJ3f8jNwDHA1+ndFPgssH23/Ajgxm76RuCIbnr7rl4W+mfYwvvn54HHp/+cHkOb7IupEQR2646LrwPvafk48ox/K9H9O3kwsBzYs6qeBuje9+iqzWWYjG3NfwR+E/hJN/8m4AfVGwIENt0HLQ4R8ovAeuC/dZfDLkuyEx5Dr6iq7wH/AXgSeJrecbGSho8jg38rkOSNwFeBc6rq+c1VHVC2zX4tK8lJwLqqWtlfPKBqzWHZtmp74BDg4qo6GPghP72sM0hz+6j7fONk4K3Am4Gd6F3ymq6Z48jgX2BJdqAX+l+sqmu64meS7NUt3wtY15XPZZiMbclRwPuSPEFv9Ndj6P0HsEs3BAhsug9e2T+zDRGyDVkDrKmq5d38V+j9IfAY+ql3A49X1fqq+n/ANcCRNHwcGfwLqBui+nLgwaq6sG9R/1AXp9O79j9V/tHumxmHA89N/Tu/Laqq36qqfapqMb0P475VVR8BbqE3BAi8ev80NURIVf0f4Kkk+3dFxwLfxWOo35PA4Ul+rvudm9pHzR5H3sC1gJL8CnAbcD8/vYb92/Su8/8PYBG9g/aDVbWhO2gvAk4AXgI+VlUr5r3jCyDJ0cCnquqkJL9I7z+A3YB7gH9ZVT9K8gbgj+l9VrIBOLWqHluoPs+XJEuAy4AdgceAj9E7qfMY6iT5d8CH6H2T7h7g4/Su5Td5HBn8ktQYL/VIUmMMfklqjMEvSY0x+CWpMQa/JDXG4Ne8SPL5JOf0zd+Y5LK++c8l+Y0Rtv+ZJJ8atZ9DtLskyYkL0O6L892mth0Gv+bLX9K7W5IkrwN2Bw7sW34k8O25bCjJdmPv3fCWAPMe/KPou1tVjTL4NV++TRf89AJ/NfBCkl2TvB54G3BPd0fp73fjpt+f5EPQu4ErvWcXXEXvhjeS/E6Sh5P8T2D/VzcJSfZMcm2Se7vX1B+f3+jaWD31n0iSxUlW9637qSSf6ab/V5LPJrkzyV8leUc3fvsFwIeSrJrqa9/6v5bkmiQ3dGO+//u+ZS/2TX8gyRXd9BVJLu5+1seS/JMky9IbZ/+Kadv/XJK7k9ycZKIr+6WuvZVJbkvyy33bvTDJLWyLwwzrNfEvv+ZFVa1NsjHJInp/AO6gd+fkEfRGP7yvqn6c5J/TO4s+iN5/BXclubXbzKHA26vq8ST/iN4wDgfTO47vpjfi4nRfAP6iqk7p/lN4Y7fux4DD6A3ItTzJX9Abk31ztq+qQ7tLO+dX1buTfJreOO9nzbDOkq6PPwIeTvIHVfXUDHWn7EpvXKL3AX9Gb8yij3f7YklVraI30NjdVXVu14fzgbPoPT/2E1X1SJLDgP/cbQvg7wPvrqqXZ2lf2ziDX/Np6qz/SOBCesF/JL3g/8uuzq8AV3fh9EwXyP8YeB64s6oe7+q9A7i2ql4CSHLdDG0eA3wUoNvmc91QGddW1Q+7da/ptjfTNqZMDaK3Elg8x5/55qp6rmvnu8Bb2HRY5EH+rKoqyf3AM1U19R/OA127q+gN8fGlrv5/B65Jb5TXI4Ev90ZmAOD1fdv9sqEvMPg1v6au8/8Depd6ngLOpRfqy7o6g4bEnfLDafPDjjcyUxsb2fTy5xumLf9R9/4yc//d+VHfdP96/X2fqZ2fTFv/J5tpt+j1/QdVtWSGOtP3nxrlNX7Np28DJwEbqurlqtoA7ELvcs8dXZ1b6V0z3667bv1O4M4B27oVOCXJ30myM/BPZ2jzZuCT8Mqze3++W/f93WiNOwGn0Bss7xlgjyRv6j53OGkOP9MLwM5zqDfdM0ne1n3QfcoQ67+On44s+WHg9u5ZDo8n+SC88nzdg4bYtrZxBr/m0/30rtt/Z1rZc1X1bDd/LXAfcC/wLeA3u6GHN1FVd9O71LGK3vMMbpuhzbOBd3WXTVYCB3brXkHvD8py4LKquqcbq/2CruzrwENz+JluAQ4Y9OHuLM7r2vgWvadCvVY/BA5MspLe5awLuvKPAGckuRd4gN4DSKRNODqnJDXGM35JaozBL0mNMfglqTEGvyQ1xuCXpMYY/JLUGINfkhpj8EtSY/4/nbwidR9iwgQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(offer_word_number, kde=False, norm_hist=False, axlabel=\"Word count number\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a consequence, it is necessary to define a specific word vocabulary in order to only retain the relevant terms related to data, education, work experience and skill. \n",
    "<br>\n",
    "**Then the Dice metric will favorize job offers which present high number of word in common with the resume while penalyzing offers that are either too indefinite or too demanding in regards with the candidate resume.**\n",
    "<br>\n",
    "<br> \n",
    "For further information on text similarity measurement, a nice synthesis is available here [[8]](#ref8) (French only)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I defined a first pecific vocabulary adapted to my resume and my job searching.\n",
    "<br> It should be improved in the future, especially in order to cover most of the hazardous spelling for technical skill or tool (e.g. \"Scikit-learn\" could also probably be found as \"Sklearn\" or \"Scikit learn\" in some job offers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#programming languages usefull for data\n",
    "prog_lang_l = ['Python', 'R', 'C', 'C#', 'C++', 'SAS', 'SPSS', 'VBA', 'SQL', 'NoSQL', 'Fortran', 'Matlab',\n",
    "               'Perl', 'Julia', 'Ruby', 'Scala', 'Java', 'JavaScript', 'PHP', 'HTLM', 'CSS']\n",
    "\n",
    "#other computing skills : IDE, OS, Big data Framework, Cloud Computing, Software BI & Data viz, Database, GIS/SIG, collab\n",
    "other_comput_l = ['PyCharm', 'Jupyter', 'RStudio', 'Visual Studio',\n",
    "                  'Windows', 'Linux', 'Mac', 'iOS', 'Android',\n",
    "                  'Hadoop', 'MapReduce', 'Hive', 'Kafka', 'Spark', 'Pig',\n",
    "                  'Azure', 'AWS', 'Google Cloud',\n",
    "                  'Tableau', 'Power BI', 'Qlikview',\n",
    "                  'HBase', 'Cassandra', 'MongoDB', 'Access', 'PostgreSQL', 'Oracle',\n",
    "                  'SIG', 'MapInfo', 'ArcGIS', 'QGIS',\n",
    "                  'Git', 'GitHub',\n",
    "                  'Microsoft Office']\n",
    "\n",
    "#Python and R main libraries for data (oriented in favor of python and my profile)\n",
    "data_library_l = ['Numpy', 'Scipy', 'Pandas', ' Dyplr', 'Tidyr',\n",
    "                  'Matplotlib', 'Seaborn', 'Plotly', 'Bokeh', 'ggplot2', 'Shiny',\n",
    "                  'Theano', 'TensorFlow', 'Keras', 'Pytorch', 'Scikit-learn', 'Statsmodels',\n",
    "                  'NLTK', 'Beautiful Soup', 'Selenium',\n",
    "                  'OpenCV']\n",
    "\n",
    "data_skills_l = ['Statistiques', 'Data mining', 'Data wrangling', 'Data visualization',\n",
    "                 'Machine Learning', 'Deep Learning', 'Clustering', 'Computer vision', 'NLP']\n",
    "\n",
    "education_l = ['Master', 'Ingénieur', 'Docteur',\n",
    "               'Mathématiques', 'Informatiques', 'Physique', 'Chimie', 'Biologie',\n",
    "               'économie', 'Marketing', 'Finance', 'Actuariat']\n",
    "\n",
    "language_l = ['Français', 'Anglais', 'Allemand', 'Espagnol', 'Arabe', 'Chinois', 'Japonais']\n",
    "\n",
    "gal_prof_skill_l = ['Gestion de projets', \"Gestion d'affaires\", 'Management',\n",
    "                    'Rédaction', 'Présentation', 'Communication', 'Relationnel',\n",
    "                    'Agile', 'Scrum'] # Add soft skills later ?\n",
    "\n",
    "#Oriented according to my resume. Some are redundant with education -> not added\n",
    "data_sectors_l = ['Ingénierie', 'Santé', 'énergie', 'Déchets', 'Transports', 'Industrie',#environnement tag too confusing\n",
    "                  'Pharmaceutique', 'Biotechnologies',\n",
    "                  'Smart city', 'IoT',\n",
    "                  'Assurances', 'Banques',\n",
    "                  'Publicité', 'Web', 'e-commerce',\n",
    "                  'Grande distribution']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute all these sub-vocabularies into a general skill list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_skill_l = prog_lang_l + other_comput_l + data_library_l + data_skills_l + education_l + language_l + \\\n",
    "                  gal_prof_skill_l + data_sectors_l\n",
    "overall_skill_l = [skill.lower() for skill in overall_skill_l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "129"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(overall_skill_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used the CountVectorizer class provided by Scikit-learn with a special token pattern to handle compound nouns more easily (e.g. I do not want that \"scikit-learn\" stands for 2 words and received a double weight in comparison to pandas for example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_pattern = '\\w+\\-?\\w*'\n",
    "word_counter = CountVectorizer(strip_accents=None, lowercase=True,\n",
    "                               analyzer='word', token_pattern=tok_pattern, ngram_range=(1, 2),\n",
    "                               vocabulary=overall_skill_l, binary=True)\n",
    "some_offer_corpus = [\n",
    "    'Offre en économie vous devrez maitriser Power BI, les outils de Data visualization et parler Allemand',\n",
    "    'Vous etes un chinois français en data science et machine learning (scikit-learn, pandas)'\n",
    "]\n",
    "X = word_counter.fit_transform(some_offer_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word counter seems to work fine. We can use it to transform in advance all our job offers into a bag of specific words.\n",
    "<br>\n",
    "For the candidate resume, it will be provided later by the Bokeh user via an input file widget in order to compute the Dice similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(97, 129)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_sim = word_counter.fit_transform(df_union['pos_descr'])\n",
    "X_sim.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1cebe19bac8>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEKCAYAAAACS67iAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAFCxJREFUeJzt3XuwbGV95vHvI2AQUZFh63g7OSaF1xhRT8SgonjJoCFBSlIImoFozUlSiQGjk8HJTGScmjiO16ly1BwVYSKg0UCijhcQQYwgygGEg0fRAQQiASwcFTQKnt/8sd6tnc2+djdn7/2e76dq1169evVav7V67afffrvXu1NVSJLWv3utdgGSpOkw0CWpEwa6JHXCQJekThjoktQJA12SOmGgS1InDHRJ6oSBLkmd2H1nbmy//farjRs37sxNStK6t3Xr1u9U1cxSy+3UQN+4cSOXXHLJztykJK17Sb61nOXscpGkThjoktQJA12SOmGgS1InDHRJ6oSBLkmdMNAlqRMGuiR1wkCXpE7s1CtFtTadfvH1Yz/2mAM3TLESSZOwhS5JnTDQJakTBrokdcJAl6ROGOiS1AkDXZI6YaBLUicMdEnqhIEuSZ0w0CWpEwa6JHViyUBPcnKSW5JsG5n3piRfS3JFkrOS7HPPlilJWspyWuinAIfOmXcO8CtV9avA1cBrp1yXJGmFlgz0qroAuG3OvLOr6q5284vAw++B2iRJKzCNPvSXA5+cwnokSROYKNCT/DlwF3DaIstsTnJJkktuvfXWSTYnSVrE2IGe5FjgMOClVVULLVdVW6pqU1VtmpmZGXdzkqQljPUfi5IcCvwH4FlV9cPpliRJGsdyvrZ4BnAR8OgkNyZ5BfAO4H7AOUkuT/Lue7hOSdISlmyhV9XR88x+3z1QiyRpAl4pKkmdMNAlqRMGuiR1wkCXpE4Y6JLUCQNdkjphoEtSJwx0SeqEgS5JnTDQJakTBrokdcJAl6ROGOiS1AkDXZI6YaBLUicMdEnqhIEuSZ0w0CWpEwa6JHXCQJekThjoktQJA12SOmGgS1Inlgz0JCcnuSXJtpF5+yY5J8k32u8H3rNlSpKWspwW+inAoXPmnQicW1X7A+e225KkVbRkoFfVBcBtc2YfDpzapk8FXjTluiRJKzRuH/qDq+omgPb7QdMrSZI0jt3v6Q0k2QxsBtiwYcM9vbl17/SLr1/tEiStU+O20G9O8hCA9vuWhRasqi1VtamqNs3MzIy5OUnSUsYN9I8Cx7bpY4G/n045kqRxLedri2cAFwGPTnJjklcA/x14fpJvAM9vtyVJq2jJPvSqOnqBu5475VokSRPwSlFJ6oSBLkmdMNAlqRMGuiR1wkCXpE4Y6JLUCQNdkjphoEtSJwx0SeqEgS5JnbjHh89dC8YdkvaYAx3uV9L6YQtdkjphoEtSJwx0SeqEgS5JnTDQJakTBrokdcJAl6ROGOiS1AkDXZI6YaBLUicMdEnqhIEuSZ2YKNCTvCrJVUm2JTkjyZ7TKkyStDJjB3qShwF/Amyqql8BdgNeMq3CJEkrM2mXy+7AfZLsDuwFfHvykiRJ4xh7PPSq+sckbwauB34EnF1VZ89dLslmYDPAhg2OL67JOb69NL9JulweCBwOPBJ4KHDfJC+bu1xVbamqTVW1aWZmZvxKJUmLmqTL5XnAtVV1a1XdCZwJHDSdsiRJKzVJoF8PPC3JXkkCPBfYPp2yJEkrNXagV9XFwEeAS4Er27q2TKkuSdIKTfRPoqvqdcDrplSLJGkCXikqSZ0w0CWpEwa6JHXCQJekThjoktQJA12SOmGgS1InDHRJ6oSBLkmdMNAlqRMTXfrfu3HH3QbH3l6LfD7VO1voktQJA12SOmGgS1InDHRJ6oSBLkmdMNAlqRMGuiR1wkCXpE4Y6JLUCQNdkjphoEtSJwx0SerERIGeZJ8kH0nytSTbk/z6tAqTJK3MpKMt/k/gU1V1ZJJ7A3tNoSZJ0hjGDvQk9wcOBo4DqKqfAD+ZTlmSpJWapIX+S8CtwPuTPBHYChxfVXeMLpRkM7AZYMMGx5TWYJKxySXNb5I+9N2BJwPvqqonAXcAJ85dqKq2VNWmqto0MzMzweYkSYuZJNBvBG6sqovb7Y8wBLwkaRWMHehV9U/ADUke3WY9F/jqVKqSJK3YpN9yeSVwWvuGyzXA701ekiRpHBMFelVdDmyaUi2SpAl4pagkdcJAl6ROGOiS1AkDXZI6YaBLUicMdEnqhIEuSZ0w0CWpEwa6JHXCQJekTkw6losW4HjfknY2W+iS1AkDXZI6YaBLUicMdEnqhIEuSZ0w0CWpEwa6JHXCQJekThjoktQJA12SOmGgS1InJg70JLsluSzJx6dRkCRpPNNooR8PbJ/CeiRJE5go0JM8HPhN4L3TKUeSNK5JW+hvB/4M2DGFWiRJExh7PPQkhwG3VNXWJM9eZLnNwGaADRs2jLs5rVGO+96XcZ/PYw70b3stmKSF/nTgt5NcB3wQeE6SD8xdqKq2VNWmqto0MzMzweYkSYsZO9Cr6rVV9fCq2gi8BPhsVb1sapVJklbE76FLUiem8j9Fq+p84PxprEuSNB5b6JLUCQNdkjphoEtSJwx0SeqEgS5JnTDQJakTBrokdcJAl6ROGOiS1AkDXZI6YaBLUiemMpaL1LvVGCfcscm1UrbQJakTBrokdcJAl6ROGOiS1AkDXZI6YaBLUicMdEnqhIEuSZ0w0CWpEwa6JHXCQJekTowd6EkekeS8JNuTXJXk+GkWJklamUkG57oLeHVVXZrkfsDWJOdU1VenVJskaQXGbqFX1U1VdWmb/gGwHXjYtAqTJK3MVPrQk2wEngRcPI31SZJWbuLx0JPsDfwtcEJVfX+e+zcDmwE2bNj5Y0NLq8nzVjvTRC30JHswhPlpVXXmfMtU1Zaq2lRVm2ZmZibZnCRpEZN8yyXA+4DtVfXW6ZUkSRrHJC30pwO/CzwnyeXt54VTqkuStEJj96FX1T8AmWItkqQJeKWoJHXCQJekThjoktQJA12SOmGgS1InDHRJ6oSBLkmdMNAlqRMGuiR1wkCXpE4Y6JLUiYnHQ5e0tqzGGOyrsc1jDhz//yv0yha6JHXCQJekThjoktQJA12SOmGgS1InDHRJ6oSBLkmdMNAlqRMGuiR1wkCXpE4Y6JLUiYkCPcmhSb6e5JtJTpxWUZKklRs70JPsBvwv4AXA44CjkzxuWoVJklZmkhb6U4FvVtU1VfUT4IPA4dMpS5K0UpME+sOAG0Zu39jmSZJWwSTjoWeeeXW3hZLNwOZ28/YkXx9ze/sB3xnzsbsKj9HiPD5LWzfH6KWrs9nVOj6/uJyFJgn0G4FHjNx+OPDtuQtV1RZgywTbASDJJVW1adL19MxjtDiPz9I8Rotb68dnki6XLwP7J3lkknsDLwE+Op2yJEkrNXYLvaruSvLHwKeB3YCTq+qqqVUmSVqRif6naFV9AvjElGpZysTdNrsAj9HiPD5L8xgtbk0fn1Td7XNMSdI65KX/ktSJdRHoDjGwuCTXJbkyyeVJLlntetaCJCcnuSXJtpF5+yY5J8k32u8HrmaNq2mB43NSkn9s59HlSV64mjWupiSPSHJeku1JrkpyfJu/ps+hNR/oDjGwbIdU1QFr+StVO9kpwKFz5p0InFtV+wPnttu7qlO4+/EBeFs7jw5on5Htqu4CXl1VjwWeBvxRy501fQ6t+UDHIQY0hqq6ALhtzuzDgVPb9KnAi3ZqUWvIAsdHTVXdVFWXtukfANsZroRf0+fQegh0hxhYWgFnJ9narszV/B5cVTfB8AcLPGiV61mL/jjJFa1LZk11J6yWJBuBJwEXs8bPofUQ6MsaYmAX9/SqejJDt9QfJTl4tQvSuvQu4JeBA4CbgLesbjmrL8newN8CJ1TV91e7nqWsh0Bf1hADu7Kq+nb7fQtwFkM3le7u5iQPAWi/b1nletaUqrq5qn5aVTuA97CLn0dJ9mAI89Oq6sw2e02fQ+sh0B1iYBFJ7pvkfrPTwG8A2xZ/1C7ro8CxbfpY4O9XsZY1ZzaomiPYhc+jJAHeB2yvqreO3LWmz6F1cWFR+/rU2/n5EAP/bZVLWjOS/BJDqxyGK39P9/hAkjOAZzOMjncz8Drg74C/ATYA1wO/U1W75AeDCxyfZzN0txRwHfD7s/3Fu5okzwA+D1wJ7Giz/yNDP/qaPYfWRaBLkpa2HrpcJEnLYKBLUicMdEnqhIEuSZ0w0CWpEwb6GJK8LckJI7c/neS9I7ffkuRPJ1j/SUleM2mdY2z3gNUYYS/J7VNe3xnt8vVXjfn4Zyc5aOT2KUmOnF6FY9V0QpK9VrOGcSQ5P8mKBoyb9vmwKzHQx3MhcBBAknsxfJf38SP3HwR8YTkraqNJrhUHAOtqyNQku8+5/a+Bg6rqV6vqbeOsg+H72AfNs+hqOgFYd4F+T8vAHGs8EOP5Aj//g388wxV1P0jywCS/ADwWuKydbG9Ksq2NV34U/KwFeF6S0xkuXCDJn7cx3z8DPHq+jSZ5cJKzknyl/cy+qPxp28a22XcOSTbOGev6NUlOatPnJ3ljki8luTrJM9tVuK8HjmpjYR81Z9vHJTkzyafaWND/Y+S+20emj0xySps+Jcm72r5ek+RZbdCn7bPLjDzuLUkuTXJukpk275fb9rYm+XySx4ys961JzgPeOOcwnQ08qO3DM9u7ji+2FvtZswNOtWPwl0k+Bxw/UsdG4A+AV82uo911cJIL234cObL8v0/y5bb+/7LA83Zo27evJDm3zfsX78Lac7cxw5W//6ctuy3JUUn+BHgocF7bZ5Ic3c6pbUneOLKe29tzuzXJZ5I8te3rNUl+e57aPpSRd2Xt2L44yZ5J3t+2cVmSQ9r9uyV5c5t/RZJXtvl/0Y7DtiRbkoyOwfSyduy2JXnqYvs/p7a92/lwadve4bPPUTuH3glcCvznJG8bedy/SzJ6deeuo6r8GeOH4Uq6DcDvMwTAf2Vo3T4duKAt82LgHIYrXB/McGXZQxhagHcAj2zLPYUh2PcC7g98E3jNPNv8EMMgQbR1PmDksfcF9gauYhgZbiOwbeSxrwFOatPnA29p0y8EPtOmjwPescD+Hgdc07a5J/At4BHtvttHljsSOKVNn8Iw3HEYhh39PvAEhobEVuCAtlwBL23TfzFbA8N40/u36QOBz46s9+PAbvPUOXe/rwCe1aZfD7x95Bi8c4F9PWn0+LftfbjV/TiG4ZxhGGZhS9u/e7WaDp6zrhmG0UJnn+t9F9jGtlb7i4H3jMx/wMj5tl+bfijDuTTDcHXwZ4EXjRzLF7Tpsxhe4PYAnghcPs++HgGc2qbv3Wq9D/Bq4P1t/mPa9vYE/pBhfJPd5+zPviPr/Gvgt0aO83va9MGzz81C+z96PrV9u3+b3o/h7yLtOO0Antbuuy/wf4E92u0LgSesdkasxo8t9PHNttIPAi5qP7O3L2zLPAM4o4YBj24GPgf8WrvvS1V1bZt+JnBWVf2whhHdFhqr5jkMI+LR1vm9to2zquqOqrodOLOtbymzgw1tZfgDWY5zq+p7VfXPwFeBX1zGYz5Ww1/ZlcDNVXVlDYM/XTWy3R0ML1YAHwCekWGUu4OADye5HPgrhhfDWR+uqp8utuEkDwD2qarPtVmnMoTKrA/d/VEL+ruq2lFVX2V4cYYh0H8DuIyhpfgYYP85j3sawwv8tQC19GXiVwLPa63sZ7bneK5fA86vqlur6i7gtJH9+gnwqZF1fa6q7mzTG+dZ1yeB52R4Z/mCVuuPGM6rv241f43hBfxRwPOAd7ftju7PIUkuTnIlw3k62gV5Rlv2AuD+SfZZ4hjMCvCXSa4APsMwbPbssf9WVX2xrfcOhhe1w9q7uD2q6splbqMrc/sOtXyz/ehPYGhd3MDQqvk+cHJbZr6hf2fdMef2uGMwLLSNu/iXXWp7zrn/x+33T1n+efDjkenRx43WvtB2dsx5/I5FtlsMtf+/qjpggWXmHr9xrGQdo7Vn5PcbquqvFnlcmP+5nff5qaqrkzyF4Z3TG5KcXVWvn2edC7mzvYDCyDGvqh25+2cFVNU/Jzkf+DfAUbTwXWQbd9ufJHsC7wQ2VdUNGbr2Rs+DuftfLH1+AryU4V3IU6rqziTXjSw397l7L8NYK18D3r9A7d2zhT6+LwCHAbe11vJtwD7ArzO01gEuYOiT3q31Cx8MfGmedV0AHJHkPhlGTvytBbZ5LsNb3tm+zPu3x74oyV4ZRls8gmFQoZsZ+pL/VWt9HbaMffoBcL9lLDfXzUkem+HDqSPGePy9GLpqAI4B/qG9U7k2ye/Azz78euJKVtpat98d6Qf/XYZ3SUtZ7nH4NPDy9m6CJA9LMvcfHlwEPCvJI9sy+7b51wFPbvOeDMze/1Dgh1X1AeDNs8vMqenits79MnyofvQy92shHwR+j+Gd3afbvAsYApUkj2LoXvw6QxfOH8y+OLT9mQ3Z77RjMfcbQbOfHT0D+F57Xubd/zkeANzSwvwQFnlHWFUXMwyzfQw/f1Ha5dhCH9+VDP16p8+Zt3dVfafdPosh4L/C0Cr5s6r6p/a28Geq6tIkHwIuZ3hr+/kFtnk8sCXJKxhayH9YVRdl+IBx9oXivVV1GUCS1zP88V/L0HJZynnAia2L4w1VtdwuiRMZ+o9vYHi3svcyHzfrDuDxSbYC36MFAEOgvCvJf2LoB/4gw7FciWOBd2f4yt81DMG1lI8BH2kfwr1yoYWq6uwkjwUuap8B3g68jJExsqvq1gz/RerM9oJ3C/B8hn7of9uO9ZeBq9tDngC8KckO4E7aCzhDX/0nk9xUVYckeS3D8xXgE1U1yTCuZwP/G/hoDf/mEYYW97tbF8pdwHFV9eMMX899FHBFkjsZ+sffkeQ9DOf/dW1/Rn03yYUMnw+9vM1baP9HnQZ8LMM/Pr+cpc/hv2H4XOa7y93x3jjaoqQuJPk4wz+5Pne1a1ktdrlIWteS7JPkauBHu3KYgy10SeqGLXRJ6oSBLkmdMNAlqRMGuiR1wkCXpE4Y6JLUif8PvXqyODL/E0cAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "specific_offer_word_number = X_sim.sum(axis=1)\n",
    "sns.distplot(specific_offer_word_number, kde=False, norm_hist=False, bins=range(np.max(X_sim.sum(axis=1))), \n",
    "             axlabel=\"Word count number for the custom vocabulary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word counter looks fine and will be saved as well as the bag of words array for the job offers just a moment later in the all in one function. Let's first define the necessary directory and file paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "SKILL_WORD_COUNTER_DIR = './skill_word_counter'\n",
    "os.makedirs(SKILL_WORD_COUNTER_DIR, exist_ok=True)\n",
    "SKILL_WORD_COUNTER_PATH = os.path.join(SKILL_WORD_COUNTER_DIR, 'skill_word_counter.pkl')\n",
    "X_SIM_PATH = os.path.join(SKILL_WORD_COUNTER_DIR, 'X_sim.pkl')   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>c) All in one function</h4>\n",
    "<a id=\"2.1.c\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_or_update_df_union():\n",
    "    \"\"\"Create or update df union from each single job board database : from data import&cleaning to feature engineering.\n",
    "    Also create/update the word counting sparse matrix for job_offers (used for Dice similarity)\"\"\"\n",
    "    \n",
    "    #First import all the jb board database\n",
    "    DB_SAVING_DIR = './job_db'\n",
    "    DB_APEC_NEW_SAVING_PATH = os.path.join(DB_SAVING_DIR, 'apec_new.xlsx')\n",
    "    DB_INDEED_NEW_SAVING_PATH = os.path.join(DB_SAVING_DIR, 'indeed_new.xlsx')\n",
    "    DB_UNION_SAVING_PATH = os.path.join(DB_SAVING_DIR, 'union_database.xlsx') #union of individual databases\n",
    "\n",
    "    dateparse = lambda x: pd.datetime.strptime(x, '%d/%m/%Y') #to parse date format when importing the dataframe\n",
    "    df={} # initiate the dict for the job database\n",
    "    update = os.path.isfile(DB_UNION_SAVING_PATH)\n",
    "    if update:\n",
    "        file_name_pat='\\*_new.xlsx' #only *_new.xlsx files (preprocessing task has been previously executed on an old db)\n",
    "    else:\n",
    "        file_name_pat='\\*_db.xlsx' #only *_db.xlsx files (preprocessing task has never been executed before)\n",
    "    \n",
    "    for filepath in glob.glob(DB_SAVING_DIR + file_name_pat):\n",
    "        name_origin = re.search(r'\\w+(?=_(db|new)\\.xlsx)', filepath).group()\n",
    "        df[name_origin] = pd.read_excel(filepath, parse_dates=['pub_date'], date_parser=dateparse)\n",
    "        df[name_origin]['origin'] = name_origin\n",
    "        print(\"Raw data - {}: {} job offers\".format(name_origin, len(df[name_origin])))\n",
    "    \n",
    "    #Proceed to some data cleaning\n",
    "    mask={} #initiate the dict for boolean mask filtering\n",
    "    mask['apec'] = df['apec']['job_title'].map(lambda x: re.match(r'.*\\b(data|données?|bi|business intelligence)\\b.*',\n",
    "                                                                  str(x), flags=re.IGNORECASE)).notnull()\n",
    "    mask['indeed'] = df['indeed']['pos_type'].map(lambda x: re.match(r'.*\\b(stage|alternance|apprentissage)\\b.*',\n",
    "                                                                     str(x), flags=re.IGNORECASE)).isnull()\n",
    "    for key in df.keys():\n",
    "        df[key] = df[key][mask[key]]\n",
    "        print(\"After filtering - {}: {} job offers\".format(key, len(df[key])))    \n",
    "      \n",
    "    df['apec']['pos_descr'] = df['apec']['comp_info'] + '\\n' + df['apec']['pos_descr'] + '\\n' + df['apec']['profil']\n",
    "    \n",
    "    # Feature engineering\n",
    "    ## City and Department\n",
    "    df['apec']['city'], df['apec']['dept'] = zip(*df['apec']['location'].map(get_city_and_dept_apec))\n",
    "    df['indeed']['city'], df['indeed']['dept'] = zip(*df['indeed']['location'].map(get_city_and_dept_indeed))\n",
    "    \n",
    "    df_union = pd.concat([df[key] for key in df.keys()], ignore_index=True, sort=False) #aggregate all the databases\n",
    "    \n",
    "    ## Region\n",
    "    DEPT_PATH = './dpt_reg_fr.csv'\n",
    "    DEPT_DL_URL = 'https://raw.githubusercontent.com/GuillaumeHarel/portfolio-projects/master/'\\\n",
    "                  '%231_Webscraping_%26_Bokeh/Auxiliary_files/Dept_Region_France.csv'\n",
    "    urllib.request.urlretrieve(DEPT_DL_URL, DEPT_PATH)\n",
    "    df_dept_reg = pd.read_csv(DEPT_PATH, dtype={'regionCode': 'Int32'})\n",
    "    df_union = pd.merge(df_union, df_dept_reg[['departmentCode', 'departmentName', 'regionName']],\n",
    "                        how='left', left_on='dept', right_on='departmentCode')\n",
    "    df_union.drop('departmentCode', 1, inplace=True)\n",
    "    \n",
    "    ##Average Salary\n",
    "    df_union['avg_sal'] = df_union['salary'].map(get_salary)\n",
    "    \n",
    "    ##Job categories\n",
    "    df_union['job_cat'] = df_union['job_title'].map(get_cat_dummies)\n",
    "    df_union[job_cat_labels] = pd.DataFrame(df_union['job_cat'].values.tolist(), index=df_union.index)\n",
    "    \n",
    "    ##Similarity\n",
    "    df_union['sim'] = np.NaN\n",
    "    \n",
    "    ##index_0\n",
    "    if update:\n",
    "        df_union['index_0'] = df_union.index\n",
    "    \n",
    "    #Compute word counting sparse matrix for job offers (used later for Dice similarity)\n",
    "    tok_pattern = '\\w+\\-?\\w*'\n",
    "    word_counter = CountVectorizer(strip_accents=None, lowercase=True,\n",
    "                                   analyzer='word', token_pattern=tok_pattern, ngram_range=(1, 2),\n",
    "                                   vocabulary=overall_skill_l, binary=True)\n",
    "    X_sim = word_counter.fit_transform(df_union['pos_descr'])\n",
    "    \n",
    "    SKILL_WORD_COUNTER_DIR = './skill_word_counter'\n",
    "    os.makedirs(SKILL_WORD_COUNTER_DIR, exist_ok=True)\n",
    "    SKILL_WORD_COUNTER_PATH = os.path.join(SKILL_WORD_COUNTER_DIR, 'skill_word_counter.pkl')\n",
    "    X_SIM_PATH = os.path.join(SKILL_WORD_COUNTER_DIR, 'X_sim.pkl')\n",
    "    \n",
    "    if update:\n",
    "        with open(X_SIM_PATH, 'rb') as input:\n",
    "            X_sim_db = pickle.load(input)\n",
    "        X_sim_full = vstack([X_sim_db, X_sim])\n",
    "        with open(X_SIM_PATH, 'wb') as output:\n",
    "            pickle.dump(X_sim_full, output)\n",
    "        with open(SKILL_WORD_COUNTER_PATH, 'wb') as output:\n",
    "            pickle.dump(word_counter, output)\n",
    "        print('\\nWord counter transformer and word counting sparse matrix have been updated and saved')\n",
    "        \n",
    "    else:    \n",
    "        with open(X_SIM_PATH, 'wb') as output:\n",
    "            pickle.dump(X_sim, output)\n",
    "        with open(SKILL_WORD_COUNTER_PATH, 'wb') as output:\n",
    "            pickle.dump(word_counter, output)\n",
    "        print('\\nWord counter transformer and word counting sparse matrix have been created and saved')\n",
    "    \n",
    "    \n",
    "    #Eventually save or update (with index kept)\n",
    "    if update:\n",
    "        df_union_full = update_excel_file(DB_UNION_SAVING_PATH, df_union, keep_index=True)\n",
    "        for filepath in glob.glob(DB_SAVING_DIR + file_name_pat):\n",
    "            os.remove(filepath) #avoid adding same new file twice\n",
    "        print('\\nDone: the Union DB has been updated and contains {} offers'.format(len(df_union_full)))\n",
    "    else:\n",
    "        with pd.ExcelWriter(DB_UNION_SAVING_PATH, options={'strings_to_urls': False}) as writer:\n",
    "            df_union.to_excel(writer, index=True, index_label=\"index_0\")\n",
    "        print('\\nDone: the Union DB has been created and contains {} offers'.format(len(df_union)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data - apec: 18 job offers\n",
      "Raw data - indeed: 94 job offers\n",
      "After filtering - apec: 7 job offers\n",
      "After filtering - indeed: 90 job offers\n",
      "\n",
      "Word counter transformer and word counting sparse matrix have been updated and saved\n",
      "\n",
      "Done: the Union DB has been updated and contains 5271 offers\n"
     ]
    }
   ],
   "source": [
    "create_or_update_df_union()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data preprocessing is complete. It's time to build the Bokeh App for the custom aggregated job board.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2) Custom aggregated job board using Bokeh</h3>\n",
    "<a id=\"2.2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to have a nice interactive and synchronized dashboard for the job database, it is necessary to run a Bokeh server to handle our Bokeh App. This will keep the Bokeh model objects in python and in the browser in synchronization with each other, allowing to trigger callbacks to update data when control widgets are manipulated and respond to queries using the full power of python. It will also enable to include some nice custom CSS styling for the dashboard display in browser !\n",
    "<br>\n",
    "The different files required to properly run the Bokeh server and build our dashboard App will be presented in the current chapter section of the Notebook. \n",
    "<br>\n",
    "**However, the Bokeh server must be run via the Windows terminal and all the necessary files must be placed in specific fold/subfolders of the current project directory as described in the Bokeh App architecture subchapter below**.\n",
    "<br> \n",
    "It is not possible to run the Bokeh dashboard App for the aggregated job board directly from the Notebook.\n",
    "<br> \n",
    "<br> \n",
    "For furter information, please read Bokeh documentation [[9]](#ref9) (especially the following user guide section : \"Running a Bokeh Server\", \"Adding Interactions\")\n",
    "A very complete and didactical blog article to get hands on Bokeh is also available here: [[10]](#ref10), [[11]](#ref11), [[12]](#ref12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>a) Bokeh App architecture</h4>\n",
    "<a id=\"2.2.a\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It was necessary to use the \"directory format\" for the Bokeh App in order to include some custom CSS styling.\n",
    "The name of my directory is web_scraping and it respects the following general architecture:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://i.imgur.com/gp9n23l.png' width=\"1000\" height=\"200\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*main.py* contains the python code for the interactive dashboard App while the template subdirectory contains an *index.html* Jinja template file and *styles.css* file for custom css styling.\n",
    "<br>\n",
    "These three files are reproduced in the coming sub-chapter and are available in my GitHub repository.\n",
    "<br>\n",
    "<br>\n",
    "To run the Bokeh server with the directory format Bokeh App, in Windows terminal :\n",
    "* Activate the proper python environment;\n",
    "* Change directory to the parent directory of the working directory;\n",
    "* Run the following command:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bokeh serve --show web_scraping\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>b) Bokeh App</h4>\n",
    "<a id=\"2.2.b\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**main.py** (*#not to be executed in the Notebook*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import os\n",
    "import base64\n",
    "import pickle\n",
    "\n",
    "from bokeh.models.widgets import HTMLTemplateFormatter, StringFormatter, DateFormatter\n",
    "from bokeh.models.widgets import DataTable, TableColumn, RadioButtonGroup, TextInput, TextAreaInput, Dropdown,\\\n",
    "    RangeSlider, CheckboxGroup, Div, FileInput\n",
    "from bokeh.models import ColumnDataSource, Panel, Tabs\n",
    "from bokeh.io import curdoc\n",
    "from bokeh.layouts import gridplot, column\n",
    "\n",
    "#manage the directories and paths\n",
    "PROJECT_DIR = './web_scraping'\n",
    "DB_SAVING_DIR = os.path.join(PROJECT_DIR, 'job_db')\n",
    "UNION_DB_PATH = os.path.join(DB_SAVING_DIR, 'union_database.xlsx')\n",
    "SKILL_WORD_COUNTER_DIR = os.path.join(PROJECT_DIR, 'skill_word_counter')\n",
    "WORD_COUNTER_PATH = os.path.join(SKILL_WORD_COUNTER_DIR, 'skill_word_counter.pkl')\n",
    "JOB_OFFER_SPR_MATRIX_PATH = os.path.join(SKILL_WORD_COUNTER_DIR, 'X_sim.pkl')\n",
    "\n",
    "#import df_union\n",
    "df_union = pd.read_excel(UNION_DB_PATH)\n",
    "\n",
    "#A few \"metrics\" and  labels for later\n",
    "site_source = ['Apec', 'Indeed']\n",
    "origin_source = ['All sites'] + site_source\n",
    "total_job_numbers = [len(df_union)] + [len(df_union[df_union['origin'] == origin.lower()]) for origin in site_source]\n",
    "min_salary = [(np.min(df_union['avg_sal'])//5-1)*5] + \\\n",
    "             [(np.min(df_union.loc[df_union['origin'] == origin.lower(), 'avg_sal'])//5-1)*5 for origin in site_source]\n",
    "\n",
    "max_salary = [(np.max(df_union['avg_sal'])//5+1)*5] + \\\n",
    "             [(np.max(df_union.loc[df_union['origin'] == origin.lower(), 'avg_sal'])//5+1)*5 for origin in site_source]\n",
    "\n",
    "location_choice = [\"France\", \"Auvergne-Rhône-Alpes\", \"Rhône\", \"Lyon\"]\n",
    "location_scale_col = [None, 'regionName', 'departmentName', 'city'] #usefull later for RadioGroupbutton\n",
    "\n",
    "job_cat_labels = [\"Data scientist\",\n",
    "                  \"Data analyst & BI\",\n",
    "                  \"Big Data (engineer, dev, archi)\",\n",
    "                  \"IT Project Manager (data related)\",\n",
    "                  \"Data Manager/Officer\",\n",
    "                  \"Unclassified\"]\n",
    "\n",
    "#column to keep to display in Bokeh App\n",
    "ord_kept_columnns = ['index_0', 'pub_date', 'job_title', 'company', 'location', 'pos_type',\n",
    "                     'nb_pos', 'req_exp', 'salary', 'act_area', 'url', 'sim']\n",
    "\n",
    "#names of the kept columns to display in Bokeh App (datatable)\n",
    "ord_names = ['#', 'Date', 'Title', 'Company', 'Location', 'Type',\n",
    "             'Nb', 'Experience', 'Salary', 'Activity area', 'URL', 'Similarity']\n",
    "\n",
    "#respective length of each column in Bokeh datatable (in px)\n",
    "ord_kept_columnns_len = [30, 60, 120, 100, 110, 30,\n",
    "                         20, 80, 80, 130, 35, 50]\n",
    "\n",
    "\n",
    "#Define all column format for the Bokeh datatable\n",
    "date_formatter = DateFormatter(format='%d/%m/%Y')\n",
    "##URL format to display origin (e.g. apec, indeed) instead of URL link\n",
    "url_formatter = HTMLTemplateFormatter(template='<a href=\"<%= url %>\"><%= origin %></a>')\n",
    "##Add a \"hover\" tool for datatable to display long text using a HTMLTemplateFormatter\n",
    "template_long_text = \"\"\"<span href=\"#\" data-toggle=\"tooltip\" title=\"<%= value %>\"><%= value %></span>\"\"\"\n",
    "text_tooltip = HTMLTemplateFormatter(template=template_long_text)\n",
    "##One more HTLMTemplateFormatter with underscore js to customise font color of Dice similarity according to its value\n",
    "template_sim = \"\"\"\n",
    "            <div style=\"color: <%= \n",
    "                    (function colorfromint(){\n",
    "                        if(sim>0.4){return('green')}\n",
    "                        else if (sim>0.2){return('orange')}\n",
    "                        else if (sim>=0) {return('red')}\n",
    "                        else {return('black')}\n",
    "                        }()) %>;\"> \n",
    "                <%= value %>\n",
    "                </font>\n",
    "            </div>\n",
    "            \"\"\"\n",
    "template_sim_formatter = HTMLTemplateFormatter(template=template_sim)\n",
    "\n",
    "index_formatter = StringFormatter(font_style='italic', text_align='right', text_color=(142, 142, 161))\n",
    "\n",
    "formatters = [index_formatter] + [date_formatter] + [text_tooltip]*(len(ord_names)-4) + \\\n",
    "             [url_formatter] + [template_sim_formatter]\n",
    "\n",
    "\n",
    "\n",
    "#TableColumn object necessary to build the Bokeh datatable\n",
    "Columns = [TableColumn(field=Ci, title=Ti, width=Wi, formatter=Fi) for Ci, Ti, Wi, Fi in zip(\n",
    "    ord_kept_columnns, ord_names, ord_kept_columnns_len, formatters)]\n",
    "\n",
    "\n",
    "#custom template for a Div object that will be included in the Bokeh App to display the current number of job offers\n",
    "template_div = (\"\"\"\n",
    "      <div class='content'>\n",
    "       <div class='name'> {site_name} </div>\n",
    "        <span class='number'>{number}<small>/{total}</small> </span>\n",
    "      </div>\n",
    "      \"\"\")\n",
    "\n",
    "\n",
    "#initiate all the dict that will contain widget objects.\n",
    "# One panel per job board origin and one independent database for each panel!\n",
    "cv_input = {}\n",
    "location_select = {}\n",
    "div_job_number = {}\n",
    "select_1 = {}\n",
    "select_2 = {}\n",
    "select_3 = {}\n",
    "\n",
    "df_source = {}\n",
    "data_table_panel = {}\n",
    "source = {}\n",
    "table_row_1 = {}\n",
    "table_row_2 = {}\n",
    "table_row_3 = {}\n",
    "table_row_4 = {}\n",
    "table_bloc = {}\n",
    "grid = {}\n",
    "tab = {}\n",
    "\n",
    "#age max for job offer age\n",
    "age_max = \"100000\"\n",
    "\n",
    "#build all widget objects and aggregate them in a grid plot for each panel (according to job board origin).\n",
    "for idx, origin in enumerate(origin_source):\n",
    "    cv_input[origin] = FileInput(accept='.txt')\n",
    "    location_select[origin] = RadioButtonGroup(labels=location_choice, active=0,\n",
    "                                               css_classes=['custom_group_button_bokeh'])\n",
    "    text = template_div.format(site_name=origin,\n",
    "                               number=total_job_numbers[idx],\n",
    "                               total=total_job_numbers[idx])\n",
    "    div_job_number[origin] = Div(text=text, height=50)\n",
    "    select_1[origin] = Dropdown(value=age_max, label='Publication date', css_classes=['custom_button_bokeh'],\n",
    "                                menu=[(\"All\", age_max),\n",
    "                                      (\"Less than 1 day\", \"1\"),\n",
    "                                      (\"Less than 3 days\", \"3\"),\n",
    "                                      (\"Less than 7 days\", \"7\"),\n",
    "                                      (\"Less than 14 days\", \"14\"),\n",
    "                                      (\"Less than 30 days\", \"30\")\n",
    "                                      ]\n",
    "                                )\n",
    "    # WARNING for Dropdown button, use value param. to set default value (and not default_value param. !!!)\n",
    "\n",
    "    select_2[origin] = CheckboxGroup(labels=job_cat_labels, active=list(range(len(job_cat_labels))))\n",
    "\n",
    "    select_3[origin] = RangeSlider(title=\"Salary(k€)\", start=min_salary[idx], end=max_salary[idx], step=5,\n",
    "                                   value=(min_salary[idx], max_salary[idx]))\n",
    "\n",
    "    if origin == 'All sites':\n",
    "        df_source[origin] = df_union\n",
    "    else:\n",
    "        df_source[origin] = df_union[df_union['origin'] == origin.lower()]\n",
    "    source[origin] = ColumnDataSource(df_source[origin])\n",
    "    data_table_panel[origin] = DataTable(columns=Columns, source=source[origin],\n",
    "                                         reorderable=True, fit_columns=True, index_position=None,\n",
    "                                         width=1000, height=260, row_height=23,\n",
    "                                         css_classes=[\"my-table\"])\n",
    "    table_row_1[origin] = TextInput(value='', title=\"Job title\")\n",
    "    table_row_2[origin] = TextInput(value='', title=\"Company\")\n",
    "    table_row_3[origin] = TextInput(value='', title=\"Location\")\n",
    "    table_row_4[origin] = TextInput(value='', title=\"Recruitment responsible\")\n",
    "    table_bloc[origin] = TextAreaInput(value='', title=\"Job description\", cols=1000, max_length=5000, rows=11)\n",
    "    grid[origin] = gridplot([[cv_input[origin], location_select[origin]],\n",
    "                             [column(select_1[origin],\n",
    "                                     select_2[origin],\n",
    "                                     select_3[origin],\n",
    "                                     div_job_number[origin]),\n",
    "                              data_table_panel[origin]],\n",
    "                             [column(table_row_1[origin],\n",
    "                                     table_row_2[origin],\n",
    "                                     table_row_3[origin],\n",
    "                                     table_row_4[origin]),\n",
    "                              table_bloc[origin]]])\n",
    "\n",
    "    tab[origin] = Panel(child=grid[origin], title=origin)\n",
    "\n",
    "tabs = Tabs(tabs=[tab[origin] for origin in origin_source])\n",
    "\n",
    "\n",
    "def function_source(attr, old, new):\n",
    "    \"\"\"Display information in appropriate table_row/bloc Widgets on row selection in the datatable\"\"\"\n",
    "    active_panel = origin_source[tabs.active]\n",
    "    try:\n",
    "        selected_index = source[active_panel].selected.indices[0]\n",
    "        table_row_1[active_panel].value = str(source[active_panel].data[\"job_title\"][selected_index])\n",
    "        table_row_2[active_panel].value = str(source[active_panel].data[\"company\"][selected_index])\n",
    "        table_row_3[active_panel].value = str(source[active_panel].data[\"location\"][selected_index])\n",
    "        table_row_4[active_panel].value = str(source[active_panel].data[\"recruit_resp\"][selected_index])\n",
    "        table_bloc[active_panel].value = str(source[active_panel].data[\"pos_descr\"][selected_index])\n",
    "    except IndexError:\n",
    "        pass\n",
    "\n",
    "\n",
    "def make_dataset(offer_age, min_sal, max_sal, job_cat, loc):\n",
    "    \"\"\"Make a subset of the full dataset according to filters defined by user via Bokeh widgets\"\"\"\n",
    "    data = df_source[origin_source[tabs.active]]\n",
    "    today = pd.Timestamp.today().floor(\"D\")\n",
    "    min_date = today - datetime.timedelta(days=int(offer_age))\n",
    "    date_mask = data['pub_date'].between(min_date, today)\n",
    "    salary_mask = ((data['avg_sal'] >= min_sal) & (data['avg_sal'] <= max_sal)) | (data['avg_sal'].isnull())\n",
    "    cat_mask = (data[job_cat] == 1).any(axis=1)\n",
    "    if loc == 0:\n",
    "        loc_mask = [True] * len(data)\n",
    "    elif loc < 3:\n",
    "        loc_mask = data[location_scale_col[loc]] == location_choice[loc]\n",
    "    else:\n",
    "        loc_mask = data[location_scale_col[loc]].map(lambda x: bool(re.match(location_choice[loc],\n",
    "                                                                             str(x), flags=re.IGNORECASE)))\n",
    "    sub_df = data[date_mask & salary_mask & cat_mask & loc_mask]\n",
    "    return sub_df\n",
    "\n",
    "\n",
    "def update_dpdown_label():\n",
    "    \"\"\"Updata dropdown button label according to user selection\"\"\"\n",
    "    active_panel = origin_source[tabs.active]\n",
    "    offer_age = select_1[active_panel].value\n",
    "    if offer_age == age_max:\n",
    "        new_label = \"Publication date : All\"\n",
    "    else:\n",
    "        new_label = \"Publication date : Less than {} day(s)\".format(offer_age)\n",
    "    select_1[active_panel].label = new_label\n",
    "\n",
    "\n",
    "def update_div_job_numbers():\n",
    "    \"\"\"Update job number display in the Div object according to current activated filters\"\"\"\n",
    "    new_site_name = origin_source[tabs.active]\n",
    "    new_number = len(source[origin_source[tabs.active]].data['index'])\n",
    "    total = total_job_numbers[tabs.active]\n",
    "    new_text = template_div.format(site_name=new_site_name,\n",
    "                           number=new_number,\n",
    "                           total=total)\n",
    "    div_job_number[origin_source[tabs.active]].update(text=new_text)\n",
    "\n",
    "\n",
    "def change_data_source():\n",
    "    \"\"\"Update the source datatable via the make_dataset function and filters provided by user via widgets\"\"\"\n",
    "    active_panel = origin_source[tabs.active]\n",
    "    offer_age = select_1[active_panel].value\n",
    "    min_sal = select_3[active_panel].value[0]\n",
    "    max_sal = select_3[active_panel].value[1]\n",
    "    job_cat = [select_2[active_panel].labels[i] for i in select_2[active_panel].active]\n",
    "    loc = location_select[active_panel].active\n",
    "    new_src = make_dataset(offer_age=offer_age, min_sal=min_sal, max_sal=max_sal, job_cat=job_cat, loc=loc)\n",
    "    source[active_panel].data.update(ColumnDataSource(new_src).data) #if passing directly the the new src in update, issue with index\n",
    "\n",
    "\n",
    "def compute_similarity(cv, word_counter_path=WORD_COUNTER_PATH, offer_spr_matrix_path=JOB_OFFER_SPR_MATRIX_PATH):\n",
    "    \"\"\" Transforms a resume into a word counting sparse matrix by a word counter (restricted vocabulary) and compute\n",
    "    Dice index metric with word counting sparse matrix for job offers.\"\"\"\n",
    "    with open(word_counter_path, 'rb') as input:\n",
    "        word_counter = pickle.load(input)\n",
    "    with open(offer_spr_matrix_path, 'rb') as input:\n",
    "        X_sim = pickle.load(input)\n",
    "    Y = word_counter.transform([cv]) #word_counter iterates over raw text document, not raw text\n",
    "    Z = X_sim.multiply(Y) #element-wise multiplication for sparse matrix\n",
    "    sim_mat = 2 * Z.sum(axis=1)/(X_sim.sum(axis=1)+Y.sum())\n",
    "    sim_arr = np.around(np.squeeze(np.asarray(sim_mat)), decimals=2) #convert matrix object into array\n",
    "    D_sim = pd.Series(sim_arr, index=df_union.index)\n",
    "    return D_sim\n",
    "\n",
    "\n",
    "def update_similarity(attr, old, new):\n",
    "    \"\"\"Compute similarity between job offers and a resume provided by the user via the InputFile widget\"\"\"\n",
    "    active_panel = origin_source[tabs.active]\n",
    "    cv_64_enc_str = cv_input[active_panel].value\n",
    "    # Encoding the Base64 encoded string into bytes\n",
    "    cv_64_enc_b = cv_64_enc_str.encode('utf-8')\n",
    "    # Decoding the Base64 bytes\n",
    "    cv_enc_b = base64.b64decode(cv_64_enc_b)\n",
    "    # Decoding the bytes to string\n",
    "    cv_str = cv_enc_b.decode('utf-8')\n",
    "    D_sim = compute_similarity(cv=cv_str)\n",
    "    if active_panel != 'All sites':\n",
    "        mask_origin = df_source['All sites']['origin'] == active_panel.lower()\n",
    "        D_sim = D_sim[mask_origin]\n",
    "    df_source[active_panel]['sim'] = D_sim\n",
    "    change_data_source()\n",
    "\n",
    "#triggers the functions according to widget modifications by user\n",
    "for origin in origin_source:\n",
    "    cv_input[origin].on_change(\"value\", update_similarity)\n",
    "    select_1[origin].on_change(\"value\", lambda attr, old, new: change_data_source())\n",
    "    select_1[origin].on_change(\"value\", lambda attr, old, new: update_dpdown_label())\n",
    "    select_3[origin].on_change(\"value\", lambda attr, old, new: change_data_source())\n",
    "    select_2[origin].on_change(\"active\", lambda attr, old, new: change_data_source())\n",
    "    location_select[origin].on_change(\"active\", lambda attr, old, new: change_data_source())\n",
    "    source[origin].selected.on_change('indices', function_source)\n",
    "    source[origin].on_change('data', lambda attr, old, new: update_div_job_numbers())\n",
    "\n",
    "curdoc().add_root(tabs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**index.html**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "    <head>\n",
    "      <meta charset=\"utf-8\">\n",
    "      {{ bokeh_css }}\n",
    "      {{ bokeh_js }}\n",
    "      <style type=\"text/css\">{% include 'styles.css' %}</style>\n",
    "    </head>\n",
    "    <body>\n",
    "      {{ plot_div|indent(8) }}\n",
    "      {{ plot_script|indent(8) }}\n",
    "    </body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**styles.css**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ".content{\n",
    "    color: #2b6980;      \n",
    "    width: 100px;\n",
    "    height: 30px;\n",
    "\ttext-align:center;\n",
    "\tdisplay:inline-block;\n",
    "\tbackground-color: #dbe7eb;\n",
    "\tfont-weight: bold;\n",
    "\tposition: absolute;\n",
    "\tleft: 200px;\n",
    "}\n",
    "\n",
    ".name{\n",
    "    font-size: 6pt;      \n",
    "}\n",
    "\n",
    ".number{\n",
    "    font-size: 10pt;\n",
    "}\n",
    "\n",
    "\n",
    ".custom_button_bokeh button.bk.bk-btn.bk-btn-default {\n",
    "\tcolor: black;\n",
    "\tfont-size: 9pt;\n",
    "\tbackground-color: #ff841278;\n",
    "\tborder-color: #999;\n",
    "\tpadding : 6px 32px;\n",
    "}\n",
    ".custom_button_bokeh div.bk.bk-menu.bk-below {\n",
    "\tposition: absolute;\n",
    "\tleft: 0;\n",
    "\twidth: 100%;\n",
    "\tz-index: 100;\n",
    "\tcursor: pointer;\n",
    "\tfont-size: 12px;\n",
    "\tbackground-color: #fff;\n",
    "\tborder: 1px solid #ccc;\n",
    "\tborder-radius: 4px;\n",
    "\tbox-shadow: 0 6px 12px rgba(0,0,0,0.175);\n",
    "\tbackground-color: antiquewhite;\n",
    "}\n",
    "\n",
    ".custom_group_button_bokeh div.bk.bk-btn.bk-btn-default {\n",
    "\tcolor: black;\n",
    "\tfont-size: 9pt;\n",
    "\tbackground-color: #b4cdd57a;\n",
    "\tborder-color: #999;\n",
    "\tpadding : 6px 32px;\n",
    "}\n",
    "\n",
    ".custom_group_button_bokeh div.bk.bk-btn.bk-btn-default.bk-active {\n",
    "\tcolor: black;\n",
    "\tfont-size: 9pt;\n",
    "\tbackground-color: #a6cedb7a;\n",
    "\tborder-color: #999;\n",
    "\tpadding : 6px 32px;\n",
    "\tfont-weight : bold;\n",
    "}\n",
    "\n",
    ".bk-root .bk-tabs-header .bk-tab.bk-active {\n",
    "    color: #4d4d4d;\n",
    "    background-color: #ffd26059;\n",
    "    border-color: #e6e6e6;\n",
    "    font-weight: bold;\n",
    "}\n",
    "\n",
    ".my-table .slick-header-column:nth-child(1) .slick-column-name{\n",
    "    float:right !important;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**And the final result for the interactive Bokeh dashboard in action:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Bokeh_Gif_URL](https://s3.gifyu.com/images/bokeh.gif \"Bokeh dashboard sample\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>III) Exploratory data analysis (on going...)</h2>\n",
    "<a id=\"3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<h2><u>Useful links</u></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] https://selenium-python.readthedocs.io/index.html#\n",
    "<a id=\"ref1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[2] https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
    "<a id=\"ref2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[3] http://jonathansoma.com/lede/foundations-2018/classes/selenium/selenium-windows-install/\n",
    "<a id=\"ref3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[4] https://medium.com/ymedialabs-innovation/web-scraping-using-beautiful-soup-and-selenium-for-dynamic-page-2f8ad15efe25\n",
    "<a id=\"ref4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[5] http://sametmax.com/parser-du-html-avec-beautifulsoup/\n",
    "<a id=\"ref5\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[6] http://blogs.quovantis.com/how-to-write-awesome-xpaths-for-test-automation-in-selenium/\n",
    "<a id=\"ref6\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[7] https://www.pluralsight.com/guides/web-scraping-with-selenium\n",
    "<a id=\"ref2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[8] https://hal.archives-ouvertes.fr/hal-00874280/document\n",
    "<a id=\"ref8\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[9] https://bokeh.pydata.org/en/latest/docs/user_guide.html\n",
    "<a id=\"ref9\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[10] https://towardsdatascience.com/data-visualization-with-bokeh-in-python-part-one-getting-started-a11655a467d4\n",
    "<a id=\"ref10\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[11] https://towardsdatascience.com/data-visualization-with-bokeh-in-python-part-ii-interactions-a4cf994e2512\n",
    "<a id=\"ref11\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[12] https://towardsdatascience.com/data-visualization-with-bokeh-in-python-part-iii-a-complete-dashboard-dc6a86aa6e23\n",
    "<a id=\"ref12\"></a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
